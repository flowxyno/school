Arithmetic with R

All the normal operators apply with the addition of 

    Exponentation ^ = Raises the power of the number on the left by the number on the right

    Modulo %% = returns the remainder of the division of the number to the left by the number on the right

#######################################################################################################################
Variable assignment: 

    my_var <- 4

#######################################################################################################################
Basic Data Types in R:

    Numerics = Decimal Values like 4.5
    Intergers = whole numbers like 4
    Logical = TRUE/FALSE                        # Note that these are always in all CAPS
    characters = text / string

########################################################################################################################
Checking the Data Type: 

    class() = will check what data type a variable Intergers

########################################################################################################################
Create a Vector:

    A vector is a 1D array that can hold numeric, character, or logical data

    c() = is the command to create a vector, place the elements between the () separated by comas

    example: 
        numeric_vector = c(1,2,3)
        character_vector = c("a", "b", "c")
        boolean_vector = c(TRUE, FALSE, TRUE)

###########################################################################################################################
Naming a Vector: 

    names() = the command to name a vector

    example: names each element in roulette_vector with a corisponding day of the week
        roulette_vector <- c(-24, -50, 100, -350, 10)
        names(roulette_vector) <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") # you can also assign the days of the week to a variable and use it

#############################################################################################################################
Calculate Total Winnings:

    sum() = calculates the sum of all elements in a vector

    example:
        total_poker <- sum(poker_vector)

#############################################################################################################################
Selecting Elements in a Vector:

    example:
    # It seems that R indexes start from 1 not 0
    my_vector[1]

    example: 
    # selects the first and fifth elements
    poker_vector[c(1,5)]

    example: 
    # selects elements 2-4 including 4
    poker_vector[c(2:4)]

    example: 
    # if you have named your elements you can select them by name
    poker_vector[c("Monday", "Tuesday")]

##########################################################################################################################
Calculating The average of values

    mean() = calculates the average of the the values given to it

    example:
    poker_start <- poker_vector[c("Monday", "Tuesday", "Wednesday")
    mean(poker_start)]

##########################################################################################################################
Comparison operators

    < = less than
    > = Greater than
    <= = Less than or equal to 
    >= = Greater than or equal to 
    == = equal to each other
    != = Not equal to each other

    example:
    selection_vector <- poker_vector > 0    # creates a boolean array
    poker_winning_days <- poker_vector[selection_vector]    # This will only select elements that are true and assign the values

###########################################################################################################################
Advanced Selection:

    example:
    selection_vector <- roulette_vector > 0
    roulette_winning_days <- roulette_vector[selection_vector]

###########################################################################################################################
Matrix:

    matrix() = collection of elements of the same data type arranged in a fixed number of rows and columns which is a 2D array

    example:
    # 1:9 is shortcut for making a vector with content 1-9
    # byrow = TRUE means the martix will be filled by row if you want to fill by column then set to FALSE
    # nrow = 3 sets the row count to 3 
    matrix(1:9, byrow = True, nrow = 3)

Analyzing Matrices:

###########################################################################################################################
Factors:

    The term factor refers to a statistical data type used to store categorical variables. The difference between a categorical variable and a continuous variable is that a categorical variable can belong to a limited number of categories. A continuous variable, on the other hand, can correspond to an infinite number of values.

It is important that R knows whether it is dealing with a continuous or a categorical variable, as the statistical models you will develop in the future treat both types differently. (You will see later why this is the case.)

A good example of a categorical variable is sex. In many circumstances you can limit the sex categories to "Male" or "Female".
 (Sometimes you may need different categories. For example, you may need to consider chromosomal variation, hermaphroditic 
 animals, or different cultural norms, but you will always have a finite number of categories.)

 To create factors in R, you make use of the function factor(). First thing that you have to do is create a vector that 
 contains all the observations that belong to a limited number of categories. For example, sex_vector contains the sex of 5 
 different individuals:

sex_vector <- c("Male","Female","Female","Male","Male")
It is clear that there are two categories, or in R-terms 'factor levels', at work here: "Male" and "Female".

The function factor() will encode the vector as a factor:

factor_sex_vector <- factor(sex_vector)

What's a factor and why would you use it? (3)
There are two types of categorical variables: a nominal categorical variable and an ordinal categorical variable.

A nominal variable is a categorical variable without an implied order. This means that it is impossible to say that 
'one is worth more than the other'. For example, think of the categorical variable animals_vector with the categories 
"Elephant", "Giraffe", "Donkey" and "Horse". Here, it is impossible to say that one stands above or below the other. 
(Note that some of you might disagree ;-) ).

In contrast, ordinal variables do have a natural ordering. Consider for example the categorical variable temperature_vector 
with the categories: "Low", "Medium" and "High". Here it is obvious that "Medium" stands above "Low", and "High" stands 
above "Medium".

Factor levels
When you first get a dataset, you will often notice that it contains factors with specific factor levels. However, 
sometimes you will want to change the names of these levels for clarity or other reasons. R allows you to do this with the 
function levels():

levels(factor_vector) <- c("name1", "name2",...)

A good illustration is the raw data that is provided to you by a survey. A common question for every questionnaire is the 
sex of the respondent. Here, for simplicity, just two categories were recorded, "M" and "F". (You usually need more categories
 for survey data; either way, you use a factor to store the categorical data.)

survey_vector <- c("M", "F", "F", "M", "M")

Recording the sex with the abbreviations "M" and "F" can be convenient if you are collecting data with pen and paper, 
but it can introduce confusion when analyzing the data. At that point, you will often want to change the factor levels to 
"Male" and "Female" instead of "M" and "F" for clarity.

Watch out: the order with which you assign the levels is important. If you type levels(factor_survey_vector), you'll see 
that it outputs [1] "F" "M". If you don't specify the levels of the factor when creating the vector, R will automatically 
assign them alphabetically. To correctly map "F" to "Female" and "M" to "Male", the levels should be set to 
c("Female", "Male"), in this order.

Summarizing a factor
After finishing this course, one of your favorite functions in R will be summary(). This will give you a quick overview 
of the contents of a variable:

summary(my_var)

Going back to our survey, you would like to know how many "Male" responses you have in your study, and how many 
"Female" responses. The summary() function gives you the answer to this question.

You might wonder what happens when you try to compare elements of a factor. In factor_survey_vector you have a factor
 with two levels: "Male" and "Female". But how does R value these relative to each other?

 example:
    # Read the code in the editor and submit the answer to test if male is greater than (>) female.

    # Build factor_survey_vector with clean levels
    survey_vector <- c("M", "F", "F", "M", "M")
    factor_survey_vector <- factor(survey_vector)
    levels(factor_survey_vector) <- c("Female", "Male")

    # Male
    male <- factor_survey_vector[1]

    # Female
    female <- factor_survey_vector[2]

    # Battle of the sexes: Male 'larger' than female?
    male > female

Ordered Factors: 

Since "Male" and "Female" are unordered (or nominal) factor levels, R returns a warning message, telling you that the 
greater than operator is not meaningful. As seen before, R attaches an equal value to the levels for such factors.

But this is not always the case! Sometimes you will also deal with factors that do have a natural ordering between its 
categories. If this is the case, we have to make sure that we pass this information to R…

Let us say that you are leading a research team of five data analysts and that you want to evaluate their performance. 
To do this, you track their speed, evaluate each analyst as "slow", "medium" or "fast", and save the results in 
speed_vector.

speed_vector should be converted to an ordinal factor since its categories have a natural ordering. By default, the 
function factor() transforms speed_vector into an unordered factor. To create an ordered factor, you have to add two 
additional arguments: ordered and levels.

factor(some_vector,
       ordered = TRUE,
       levels = c("lev1", "lev2" ...))

By setting the argument ordered to TRUE in the function factor(), you indicate that the factor is ordered. With the
 argument levels you give the values of the factor in the correct order.

    example: 
    # Create speed_vector
    speed_vector <- c("medium", "slow", "slow", "medium", "fast")

    # Convert speed_vector to ordered factor vector
    factor_speed_vector <- factor(speed_vector, ordered = TRUE, c("slow", "medium", "fast"))

    # Print factor_speed_vector
    factor_speed_vector
    summary(factor_speed_vector)

Comparing ordered factors
Having a bad day at work, 'data analyst number two' enters your office and starts complaining that 'data analyst number five' 
is slowing down the entire project. Since you know that 'data analyst number two' has the reputation of being a smarty-pants, 
you first decide to check if his statement is true.

The fact that factor_speed_vector is now ordered enables us to compare different elements (the data analysts in this case). 
You can simply do this by using the well-known operators.

    example:
    # Create factor_speed_vector
    speed_vector <- c("medium", "slow", "slow", "medium", "fast")
    factor_speed_vector <- factor(speed_vector, ordered = TRUE, levels = c("slow", "medium", "fast"))

    # Factor value for second data analyst
    da2 <- factor_speed_vector[2]

    # Factor value for fifth data analyst
    da5 <- factor_speed_vector[5]

    # Is data analyst 2 faster than data analyst 5?
    da2 > da5

###########################################################################################################################
Dataframes:

What's a data frame?
You may remember from the chapter about matrices that all the elements that you put in a matrix should be of the same type. 
Back then, your dataset on Star Wars only contained numeric elements.

When doing a market research survey, however, you often have questions such as:

'Are you married?' or 'yes/no' questions (logical)
'How old are you?' (numeric)
'What is your opinion on this product?' or other 'open-ended' questions (character)
…
The output, namely the respondents' answers to the questions formulated above, is a dataset of different data types. You will 
often find yourself working with datasets that contain different data types instead of only one.

A data frame has the variables of a dataset as columns and the observations as rows. This will be a familiar concept for 
those coming from different statistical software packages such as SAS or SPSS.

Quick, have a look at your dataset
Wow, that is a lot of cars!

Working with large datasets is not uncommon in data analysis. When you work with (extremely) large datasets and data frames, 
your first task as a data analyst is to develop a clear understanding of its structure and main elements. Therefore, it is 
often useful to show only a small part of the entire dataset.

So how to do this in R? Well, the function head() enables you to show the first observations of a data frame. Similarly, the 
function tail() prints out the last observations in your dataset.

Both head() and tail() print a top line called the 'header', which contains the names of the different variables in your 
dataset.

Have a look at the structure
Another method that is often used to get a rapid overview of your data is the function str(). The function str() shows you 
the structure of your dataset. For a data frame it tells you:

The total number of observations (e.g. 32 car types)
The total number of variables (e.g. 11 car features)
A full list of the variables names (e.g. mpg, cyl … )
The data type of each variable (e.g. num)
The first observations
Applying the str() function will often be the first thing that you do when receiving a new dataset or data frame. It is a
great way to get more insight in your dataset before diving into the real analysis.

Creating a data frame
Since using built-in datasets is not even half the fun of creating your own datasets, the rest of this chapter is based on 
your personally developed dataset. Put your jet pack on because it is time for some space exploration!

As a first goal, you want to construct a data frame that describes the main characteristics of eight planets in our solar 
system. According to your good friend Buzz, the main features of a planet are:

The type of planet (Terrestrial or Gas Giant).
The planet's diameter relative to the diameter of the Earth.
The planet's rotation across the sun relative to that of the Earth.
If the planet has rings or not (TRUE or FALSE).
After doing some high-quality research on Wikipedia, you feel confident enough to create the necessary vectors: name, type, 
diameter, rotation and rings; these vectors have already been coded up in the editor. The first element in each of these 
vectors correspond to the first observation.

You construct a data frame with the data.frame() function. As arguments, you pass the vectors from before: they will become 
the different columns of your data frame. Because every column has the same length, the vectors you pass should also have the 
same length. But don't forget that it is possible (and likely) that they contain different types of data.

    Example:
    # Definition of vectors
    name <- c("Mercury", "Venus", "Earth", 
            "Mars", "Jupiter", "Saturn", 
            "Uranus", "Neptune")
    type <- c("Terrestrial planet", 
            "Terrestrial planet", 
            "Terrestrial planet", 
            "Terrestrial planet", "Gas giant", 
            "Gas giant", "Gas giant", "Gas giant")
    diameter <- c(0.382, 0.949, 1, 0.532, 
                11.209, 9.449, 4.007, 3.883)
    rotation <- c(58.64, -243.02, 1, 1.03, 
                0.41, 0.43, -0.72, 0.67)
    rings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)

    # Create a data frame from the vectors
    planets_df <- data.frame(name, type, diameter, rotation, rings)

Selection of data frame elements
Similar to vectors and matrices, you select elements from a data frame with the help of square brackets [ ]. By using a 
comma, you can indicate what to select from the rows and the columns respectively. For example:

my_df[1,2] selects the value at the first row and second column in my_df.
my_df[1:3,2:4] selects rows 1, 2, 3 and columns 2, 3, 4 in my_df.

Sometimes you want to select all elements of a row or column. For example, my_df[1, ] selects all elements of the first row. 
Let us now apply this technique on planets_df!

    Example:
    # The planets_df data frame from the previous exercise is pre-loaded

    # Print out diameter of Mercury (row 1, column 3)
    planets_df[1,3]

    # Print out data for Mars (entire fourth row)
    planets_df[4,]

Selection of data frame elements (2)
Instead of using numerics to select elements of a data frame, you can also use the variable names to select columns of a 
data frame.

Suppose you want to select the first three elements of the type column. One way to do this is

planets_df[1:3,2]

A possible disadvantage of this approach is that you have to know (or look up) the column number of type, which gets hard if 
you have a lot of variables. It is often easier to just make use of the variable name:

planets_df[1:3,"type"]

    Example:
    # The planets_df data frame from the previous exercise is pre-loaded

    # Select first 5 values of diameter column
    planets_df[1:5, "diameter"]

Only planets with rings
You will often want to select an entire column, namely one specific variable from a data frame. If you want to select all 
elements of the variable diameter, for example, both of these will do the trick:

planets_df[,3]
planets_df[,"diameter"]

However, there is a short-cut. If your columns have names, you can use the $ sign:

planets_df$diameter

    Example:
    # planets_df is pre-loaded in your workspace

    # Select the rings variable from planets_df
    rings_vector <- planets_df$rings
    
    # Print out rings_vector
    rings_vector

Only planets with rings (2)
You probably remember from high school that some planets in our solar system have rings and others do not. Unfortunately you 
can not recall their names. Could R help you out?

If you type rings_vector in the console, you get:

[1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE

This means that the first four observations (or planets) do not have a ring (FALSE), but the other four do (TRUE). However, 
you do not get a nice overview of the names of these planets, their diameter, etc. Let's try to use rings_vector to select 
the data for the four planets with rings.

    Example:
    # planets_df and rings_vector are pre-loaded in your workspace

    # Adapt the code to select all columns for planets with rings
    planets_df[rings_vector, ]

Only planets with rings but shorter
So what exactly did you learn in the previous exercises? You selected a subset from a data frame (planets_df) based on 
whether or not a certain condition was true (rings or no rings), and you managed to pull out all relevant data. Pretty 
awesome! By now, NASA is probably already flirting with your CV ;-).

Now, let us move up one level and use the function subset(). You should see the subset() function as a short-cut to do 
exactly the same as what you did in the previous exercises.

subset(my_df, subset = some_condition)

The first argument of subset() specifies the dataset for which you want a subset. By adding the second argument, you give R 
the necessary information and conditions to select the correct subset.

The code below will give the exact same result as you got in the previous exercise, but this time, you didn't need the 
rings_vector!

subset(planets_df, subset = rings)

    Example:
    # planets_df is pre-loaded in your workspace

    # Select planets with diameter < 1
    subset(planets_df, subset = diameter < 1)

###########################################################################################################################
Sorting: 

Making and creating rankings is one of mankind's favorite affairs. These rankings can be useful (best universities in the 
world), entertaining (most influential movie stars) or pointless (best 007 look-a-like).

In data analysis you can sort your data according to a certain variable in the dataset. In R, this is done with the help of 
the function order().

order() is a function that gives you the ranked position of each element when it is applied on a variable, such as a vector 
for example:

a <- c(100, 10, 1000)
order(a)
[1] 2 1 3

10, which is the second element in a, is the smallest element, so 2 comes first in the output of order(a). 100, which is the 
first element in a is the second smallest element, so 1 comes second in the output of order(a).

This means we can use the output of order(a) to reshuffle a:

a[order(a)]
[1]   10  100 1000

Sorting your data frame
Alright, now that you understand the order() function, let us do something useful with it. You would like to rearrange your 
data frame such that it starts with the smallest planet and ends with the largest one. A sort on the diameter column.

    Example: 
    # planets_df is pre-loaded in your workspace

    # Use order() to create positions
    positions <-  order(planets_df$diameter)

    # Use positions to sort planets_df
    planets_df[positions,]

########################################################################################################################
Lists:

Lists, why would you need them?
Congratulations! At this point in the course you are already familiar with:

Vectors (one dimensional array): can hold numeric, character or logical values. The elements in a vector all have the same 
data type.
Matrices (two dimensional array): can hold numeric, character or logical values. The elements in a matrix all have the same 
data type.
Data frames (two-dimensional objects): can hold numeric, character or logical values. Within a column all elements have the 
same data type, but different columns can be of different data type.

Pretty sweet for an R newbie, right? ;-)

Lists, why would you need them? (2)
A list in R is similar to your to-do list at work or school: the different items on that list most likely differ in length, 
characteristic, and type of activity that has to be done.

A list in R allows you to gather a variety of objects under one name (that is, the name of the list) in an ordered way. These 
objects can be matrices, vectors, data frames, even other lists, etc. It is not even required that these objects are related 
to each other in any way.

You could say that a list is some kind super data type: you can store practically any piece of information in it!

Creating a list
Let us create our first list! To construct a list you use the function list():

my_list <- list(comp1, comp2 ...)

The arguments to the list function are the list components. Remember, these components can be matrices, vectors, other lists, …

    Example:
    # Vector with numerics from 1 up to 10
    my_vector <- 1:10 

    # Matrix with numerics from 1 up to 9
    my_matrix <- matrix(1:9, ncol = 3)

    # First 10 elements of the built-in data frame mtcars
    my_df <- mtcars[1:10,]

    # Construct list with these different elements:
    my_list <- list(my_vector, my_matrix, my_df)

Creating a named list
Well done, you're on a roll!

Just like on your to-do list, you want to avoid not knowing or remembering what the components of your list stand for. 
That is why you should give names to them:

my_list <- list(name1 = your_comp1, 
                name2 = your_comp2)

This creates a list with components that are named name1, name2, and so on. If you want to name your lists after you've 
created them, you can use the names() function as you did with vectors. The following commands are fully equivalent to the 
assignment above:

my_list <- list(your_comp1, your_comp2)
names(my_list) <- c("name1", "name2")

    Example: 
    # Vector with numerics from 1 up to 10
    my_vector <- 1:10 

    # Matrix with numerics from 1 up to 9
    my_matrix <- matrix(1:9, ncol = 3)

    # First 10 elements of the built-in data frame mtcars
    my_df <- mtcars[1:10,]

    # Adapt list() call to give the components names
    my_list <- list("vec" = my_vector, "mat" = my_matrix, "df" =my_df)


    # Print out my_list
    my_list

Creating a named list (2)
Being a huge movie fan (remember your job at LucasFilms), you decide to start storing information on good movies with the 
help of lists.

Start by creating a list for the movie "The Shining". We have already created the variables mov, act and rev in your R 
workspace. Feel free to check them out in the console.

    Example:
    # The variables mov, act and rev are available

    # Finish the code to build shining_list
    shining_list <- list(moviename = mov, actors = act, reviews = rev)

Selecting elements from a list
Your list will often be built out of numerous elements and components. Therefore, getting a single element, multiple 
elements, or a component out of it is not always straightforward.

One way to select a component is using the numbered position of that component. For example, to "grab" the first component 
of shining_list you type

shining_list[[1]]

A quick way to check this out is typing it in the console. Important to remember: to select elements from vectors, you use 
single square brackets: [ ]. Don't mix them up!

You can also refer to the names of the components, with [[ ]] or with the $ sign. Both will select the data frame 
representing the reviews:

shining_list[["reviews"]]
shining_list$reviews
Besides selecting components, you often need to select specific elements out of these components. For example, with 
shining_list[[2]][1] you select from the second component, actors (shining_list[[2]]), the first element ([1]). When 
you type this in the console, you will see the answer is Jack Nicholson.

    Example:
    # shining_list is already pre-loaded in the workspace

    # Print out the vector representing the actors
    shining_list$actors

    # Print the second element of the vector representing the actors
    shining_list[[2]][2]

Creating a new list for another movie
You found reviews of another, more recent, Jack Nicholson movie: The Departed!

Scores	        Comments
4.6	            I would watch it again
5	            Amazing!
4.8	            I liked it
5	            One of the best movies
4.2	            Fascinating plot

It would be useful to collect together all the pieces of information about the movie, like the title, actors, and reviews 
into a single variable. Since these pieces of data are different shapes, it is natural to combine them in a list variable.

movie_title, containing the title of the movie, and movie_actors, containing the names of some of the actors in the movie, 
are available in your workspace.

    Example:
    # Use the table from the exercise to define the comments and scores vectors
    scores <- c(4.6, 5, 4.8, 5, 4.2)
    comments <- c("I would watch it again", "Amazing!", "I liked it", "One of the best movies", "Fascinating plot")

    # Save the average of the scores vector as avg_review
    avg_review <- mean(scores)

    # Combine scores and comments into the reviews_df data frame
    reviews_df <- data.frame(scores, comments)

    # Create and print out a list, called departed_list
    departed_list <- list(movie_title, movie_actors, reviews_df, avg_review)
    departed_list

##########################################################################################################################
Functions:

Before even thinking of using an R function, you should clarify which arguments it expects. All the relevant details such 
as a description, usage, and arguments can be found in the documentation. To consult the documentation on the sample() 
function, for example, you can use one of following R commands:

help(sample)

?sample

If you execute these commands, you'll be redirected to www.rdocumentation.org.

A quick hack to see the arguments of the sample() function is the args() function. Try it out in the console:

args(sample)
In the next exercises, you'll be learning how to use the mean() function with increasing complexity. The first thing you'll 
have to do is get acquainted with the mean() function.

    Example:
    # Consult the documentation on the mean() function
    ?mean

    # Inspect the arguments of the mean() function
    args(mean)

Use a function
The documentation on the mean() function gives us quite some information:

The mean() function computes the arithmetic mean.
The most general method takes multiple arguments: x and ....
The x argument should be a vector containing numeric, logical or time-related information.
Remember that R can match arguments both by position and by name. Can you still remember the difference? You'll find out 
in this exercise!

Once more, you'll be working with the view counts of your social network profiles for the past 7 days. These are stored 
in the linkedin and facebook vectors and have already been created for you.

    Example:
    # The linkedin and facebook vectors have already been created for you
    linkedin <- c(16, 9, 13, 5, 2, 17, 14)
    facebook <- c(17, 7, 5, 16, 8, 13, 14)

    # Calculate average number of views
    avg_li <- mean(linkedin)
    avg_fb <- mean(facebook)

    # Inspect avg_li and avg_fb
    print(avg_li)
    print(avg_fb)

Use a function (2)
Check the documentation on the mean() function again:

?mean
The Usage section of the documentation includes two versions of the mean() function. The first usage,

mean(x, ...)
is the most general usage of the mean function. The 'Default S3 method', however, is:

mean(x, trim = 0, na.rm = FALSE, ...)
The ... is called the ellipsis. It is a way for R to pass arguments along without the function having to name them explicitly. 
The ellipsis will be treated in more detail in future courses.

For the remainder of this exercise, just work with the second usage of the mean function. Notice that both trim and na.rm 
have default values. This makes them optional arguments.

    Example:
    # The linkedin and facebook vectors have already been created for you
    linkedin <- c(16, 9, 13, 5, 2, 17, 14)
    facebook <- c(17, 7, 5, 16, 8, 13, 14)

    # Calculate the mean of the sum
    avg_sum <- mean(linkedin + facebook)

    # Calculate the trimmed mean of the sum
    avg_sum_trimmed <- mean(linkedin + facebook, trim = 0.2)

    # Inspect both new variables
    print(avg_sum)
    print(avg_sum_trimmed)

Use a function (3)
In the video, Filip guided you through the example of specifying arguments of the sd() function. The sd() function has an 
optional argument, na.rm that specified whether or not to remove missing values from the input vector before calculating 
the standard deviation.

If you've had a good look at the documentation, you'll know by now that the mean() function also has this argument, na.rm, 
and it does the exact same thing. By default, it is set to FALSE, as the Usage of the default S3 method shows:

mean(x, trim = 0, na.rm = FALSE, ...)

Let's see what happens if your vectors linkedin and facebook contain missing values (NA).

    Example:
    # The linkedin and facebook vectors have already been created for you
    linkedin <- c(16, 9, 13, 5, NA, 17, 14)
    facebook <- c(17, NA, 5, 16, 8, 13, 14)

    # Basic average of linkedin
    mean(linkedin)

    # Advanced average of linkedin
    mean(linkedin, na.rm = TRUE)

Functions inside functions
You already know that R functions return objects that you can then use somewhere else. This makes it easy to use functions 
inside functions, as you've seen before:

speed <- 31
print(paste("Your speed is", speed))

Notice that both the print() and paste() functions use the ellipsis - ... - as an argument. Can you figure out how they're 
used?

    Example:
    # The linkedin and facebook vectors have already been created for you
    linkedin <- c(16, 9, 13, 5, NA, 17, 14)
    facebook <- c(17, NA, 5, 16, 8, 13, 14)

    # Calculate the mean absolute deviation
    mean(abs(linkedin - facebook), na.rm = TRUE)

Required, or optional?
By now, you will probably have a good understanding of the difference between required and optional arguments. Let's refresh 
this difference by having one last look at the mean() function:

mean(x, trim = 0, na.rm = FALSE, ...)

x is required; if you do not specify it, R will throw an error. trim and na.rm are optional arguments: they have a default value which is used if the arguments are not explicitly specified.

Which of the following statements about the read.table() function are true?

header, sep and quote are all optional arguments.
row.names and fileEncoding don't have default values.
read.table("myfile.txt", "-", TRUE) will throw an error.
read.table("myfile.txt", sep = "-", header = TRUE) will throw an error.

##########################################################################################################################
Writing Functions: 

Syntax:
my_function <- function(art1, arg2) {
    body of the function
}

Write your own function
Wow, things are getting serious… you're about to write your own function! Before you have a go at it, have a look at the 
following function template:

my_fun <- function(arg1, arg2) {
  body
}

Notice that this recipe uses the assignment operator (<-) just as if you were assigning a vector to a variable for example. 
This is not a coincidence. Creating a function in R basically is the assignment of a function object to a variable! In the 
recipe above, you're creating a new R variable my_fun, that becomes available in the workspace as soon as you execute the 
definition. From then on, you can use the my_fun as a function.

    Example:
    # Create a function pow_two()
    pow_two <- function(a) {
        a * a
    }


    # Use the function
    pow_two(12)

    # Create a function sum_abs()
    sum_abs <- function(a,b){
        abs(a) + abs(b)
    }


    # Use the function
    sum_abs(-2, 3)

Write your own function (2)
There are situations in which your function does not require an input. Let's say you want to write a function that gives us 
the random outcome of throwing a fair die:

throw_die <- function() {
  number <- sample(1:6, size = 1)
  number
}

throw_die()

Up to you to code a function that doesn't take any arguments!

    Example:
    # Define the function hello()
    hello <- function(){
        print("Hi there!")
        return(TRUE)
    }




    # Call the function hello()
    hello()

Write your own function (3)
Do you still remember the difference between an argument with and without default values? The usage section in the sd() 
documentation shows the following information:

sd(x, na.rm = FALSE)

This tells us that x has to be defined for the sd() function to be called correctly, however, na.rm already has a default 
value. Not specifying this argument won't cause an error.

You can define default argument values in your own R functions as well. You can use the following recipe to do so:

my_fun <- function(arg1, arg2 = val2) {
  body
}

The editor on the right already includes an extended version of the pow_two() function from before. Can you finish it?

    Example: 
    # Finish the pow_two() function
    pow_two <- function(x, print_info = TRUE) {
    y <- x ^ 2
    if(print_info == TRUE){
        print(paste(x, "to the power two equals", y))
    }
    return(y)
    }

Function scoping
An issue that Filip did not discuss in the video is function scoping. It implies that variables that are defined inside a function are not accessible outside that function. Try running the following code and see if you understand the results:

pow_two <- function(x) {
  y <- x ^ 2
  return(y)
}
pow_two(4)
y
x

y was defined inside the pow_two() function and therefore it is not accessible outside of that function. This is also true 
for the function's arguments of course - x in this case.

Which statement is correct about the following chunk of code? The function two_dice() is already available in the workspace.

two_dice <- function() {
  possibilities <- 1:6
  dice1 <- sample(possibilities, size = 1)
  dice2 <- sample(possibilities, size = 1)
  dice1 + dice2
}

R passes arguments by value
The title gives it away already: R passes arguments by value. What does this mean? Simply put, it means that an R function 
cannot change the variable that you input to that function. Let's look at a simple example (try it in the console):

triple <- function(x) {
  x <- 3*x
  x
}
a <- 5
triple(a)
a

Inside the triple() function, the argument x gets overwritten with its value times three. Afterwards this new x is returned. 
If you call this function with a variable a set equal to 5, you obtain 15. But did the value of a change? If R were to pass a 
to triple() by reference, the override of the x inside the function would ripple through to the variable a, outside the 
function. However, R passes by value, so the R objects you pass to a function can never change unless you do an explicit 
assignment. a remains equal to 5, even after calling triple(a).

Can you tell which one of the following statements is false about the following piece of code?

increment <- function(x, inc = 1) {
  x <- x + inc
  x
}
count <- 5
a <- increment(count, 2)
b <- increment(count)
count <- increment(count, 2)

R you functional?
Now that you've acquired some skills in defining functions with different types of arguments and return values, you should 
try to create more advanced functions. As you've noticed in the previous exercises, it's perfectly possible to add 
control-flow constructs, loops and even other functions to your function body.

Remember our social media example? The vectors linkedin and facebook are already defined in the workspace so you can get your 
hands dirty straight away. As a first step, you will be writing a function that can interpret a single value of this vector. 
In the next exercise, you will write another function that can handle an entire vector at once.

    Example:
    # The linkedin and facebook vectors have already been created for you

    # Define the interpret function
    interpret <- function(num_views) {
        if (num_views > 15) {
            print("You're popular!")
            return(num_views)
        } else {
            print("Try to be more visible!")
            return(0)
        }
    }

    # Call the interpret function twice
    interpret(linkedin[1])
    interpret(facebook[2])

R you functional? (2)
A possible implementation of the interpret() function has been provided for you. In this exercise you'll be writing another 
function that will use the interpret() function to interpret all the data from your daily profile views inside a vector. 
Furthermore, your function will return the sum of views on popular days, if asked for. A for loop is ideal for iterating 
over all the vector elements. The ability to return the sum of views on popular days is something you can code through a 
function argument with a default value.

    Example:
    # The linkedin and facebook vectors have already been created for you
    linkedin <- c(16, 9, 13, 5, 2, 17, 14)
    facebook <- c(17, 7, 5, 16, 8, 13, 14)

    # The interpret() can be used inside interpret_all()
    interpret <- function(num_views) {
        if (num_views > 15) {
            print("You're popular!")
            return(num_views)
        } else {
            print("Try to be more visible!")
            return(0)
        }
    }

    # Define the interpret_all() function
    # views: vector with data to interpret
    # return_sum: return total number of views on popular days?
    interpret_all <- function(views, return_sum = TRUE) {
        count <- 0

        for (v in views) {
            count = count + interpret(v)
        }

        if (return_sum == TRUE) {
            return(count)
        } else {
            return(NULL)
        }
    }

    # Call the interpret_all() function on both linkedin and facebook
    interpret_all(linkedin)
    interpret_all(facebook)

##########################################################################################################################
R Packages:

Load an R Package
There are basically two extremely important functions when it comes down to R packages:

install.packages(), which as you can expect, installs a given package.
library() which loads packages, i.e. attaches them to the search list on your R workspace.
To install packages, you need administrator privileges. This means that install.packages() will thus not work in the 
DataCamp interface. However, almost all CRAN packages are installed on our servers. You can load them with library().

In this exercise, you'll be learning how to load the ggplot2 package, a powerful package for data visualization. You'll 
use it to create a plot of two variables of the mtcars data frame. The data has already been prepared for you in the 
workspace.

Before starting, execute the following commands in the console:

search(), to look at the currently attached packages and
qplot(mtcars$wt, mtcars$hp), to build a plot of two variables of the mtcars data frame.
An error should occur, because you haven't loaded the ggplot2 package yet!

    Example:
    # Load the ggplot2 package
    library(ggplot2)

    # Retry the qplot() function
    qplot(mtcars$wt, mtcars$hp)

    # Check out the currently attached packages again
    search()

Different ways to load a package
The library() and require() functions are not very picky when it comes down to argument types: both library(rjson) and 
library("rjson") work perfectly fine for loading a package.

Have a look at some more code chunks that (attempt to) load one or more packages:

# Chunk 1
library(data.table)
require(rjson)

# Chunk 2
library("data.table")
require(rjson)

# Chunk 3
library(data.table)
require(rjson, character.only = TRUE)

# Chunk 4
library(c("data.table", "rjson"))
Select the option that lists all of the chunks that do not generate an error. The console is yours to experiment in.

####################################################################################################################
Lapply:

Before you go about solving the exercises below, have a look at the documentation of the lapply() function. The Usage 
section shows the following expression:

lapply(X, FUN, ...)

To put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. If FUN 
requires additional arguments, you pass them after you've specified X and FUN (...). The output of lapply() is a list, 
the same length as X, where each element is the result of applying FUN on the corresponding element of X.

Now that you are truly brushing up on your data science skills, let's revisit some of the most relevant figures in 
data science history. We've compiled a vector of famous mathematicians/statisticians and the year they were born. Up 
to you to extract some information!

    Example: 
    # The vector pioneers has already been created for you
    pioneers <- c("GAUSS:1777", "BAYES:1702", "PASCAL:1623", "PEARSON:1857")

    # Split names from birth year
    split_math <- strsplit(pioneers, split = ":")

    # Convert to lowercase strings: split_low
    split_low <- lapply(split_math, tolower)

    # Take a look at the structure of split_low
    str(split_low)

Use lapply with your own function
As Filip explained in the instructional video, you can use lapply() on your own functions as well. You just need to code a new function and 
make sure it is available in the workspace. After that, you can use the function inside lapply() just as you did with 
base R functions.

In the previous exercise you already used lapply() once to convert the information about your favorite pioneering 
statisticians to a list of vectors composed of two character strings. Let's write some code to select the names and 
the birth years separately.

The sample code already includes code that defined select_first(), that takes a vector as input and returns the first 
element of this vector.

    Example: 
    # Code from previous exercise:
    pioneers <- c("GAUSS:1777", "BAYES:1702", "PASCAL:1623", "PEARSON:1857")
    split <- strsplit(pioneers, split = ":")
    split_low <- lapply(split, tolower)

    # Write function select_first()
    select_first <- function(x) {
        x[1]
    }

    # Apply select_first() over split_low: names
    names <- lapply(split_low, select_first)

    # Write function select_second()
    select_second <- function(x) {
        x[2]
    }



    # Apply select_second() over split_low: years
    years <- lapply(split_low, select_second)

lapply and anonymous functions
Writing your own functions and then using them inside lapply() is quite an accomplishment! But defining functions to 
use them only once is kind of overkill, isn't it? That's why you can use so-called anonymous functions in R.

Previously, you learned that functions in R are objects in their own right. This means that they aren't automatically 
bound to a name. When you create a function, you can use the assignment operator to give the function a name. It's 
perfectly possible, however, to not give the function a name. This is called an anonymous function:

# Named function
triple <- function(x) { 3 * x }

# Anonymous function with same implementation
function(x) { 3 * x }

# Use anonymous function inside lapply()
lapply(list(1,2,3), function(x) { 3 * x })

split_low is defined for you.

    Example:
    # split_low has been created for you
    split_low

    # Transform: use anonymous function inside lapply
    names <- lapply(split_low, function(x) {x[1]})

    # Transform: use anonymous function inside lapply
    years <- lapply(split_low, function(x) {x[2]})

Use lapply with additional arguments
In the video, the triple() function was transformed to the multiply() function to allow for a more generic approach. 
lapply() provides a way to handle functions that require more than one argument, such as the multiply() function:

multiply <- function(x, factor) {
  x * factor
}

lapply(list(1,2,3), multiply, factor = 3)
On the right we've included a generic version of the select functions that you've coded earlier: select_el(). It 
takes a vector as its first argument, and an index as its second argument. It returns the vector's element at the 
specified index.

    Example:
    # Definition of split_low
    pioneers <- c("GAUSS:1777", "BAYES:1702", "PASCAL:1623", "PEARSON:1857")
    split <- strsplit(pioneers, split = ":")
    split_low <- lapply(split, tolower)

    # Generic select function
    select_el <- function(x, index) {
        x[index]
    }

    # Use lapply() twice on split_low: names and years
    names <- lapply(split_low, select_el, 1)
    years <- lapply(split_low, select_el, 2)

Apply functions that return NULL
In all of the previous exercises, it was assumed that the functions that were applied over vectors and lists actually 
returned a meaningful result. For example, the tolower() function simply returns the strings with the characters in 
lowercase. This won't always be the case. Suppose you want to display the structure of every element of a list. You 
could use the str() function for this, which returns NULL:

lapply(list(1, "a", TRUE), str)
This call actually returns a list, the same size as the input list, containing all NULL values. On the other hand 
calling

str(TRUE)
on its own prints only the structure of the logical to the console, not NULL. That's because str() uses invisible() 
behind the scenes, which returns an invisible copy of the return value, NULL in this case. This prevents it from being 
printed when the result of str() is not assigned.

What will the following code chunk return (split_low is already available in the workspace)? Try to reason about the 
result before simply executing it in the console!

lapply(split_low, function(x) {
  if (nchar(x[1]) > 5) {
    return(NULL)
  } else {
    return(x[2])
  }
})

###################################################################################################################
Sapply:

How to use sapply
You can use sapply() similar to how you used lapply(). The first argument of sapply() is the list or vector X over 
which you want to apply a function, FUN. Potential additional arguments to this function are specified afterwards 
(...):

sapply(X, FUN, ...)
In the next couple of exercises, you'll be working with the variable temp, that contains temperature measurements for 
7 days. temp is a list of length 7, where each element is a vector of length 5, representing 5 measurements on a given 
day. This variable has already been defined in the workspace: type str(temp) to see its structure.

    Example:
    # temp has already been defined in the workspace

    # Use lapply() to find each day's minimum temperature
    lapply(temp, min)

    # Use sapply() to find each day's minimum temperature
    sapply(temp, min)

    # Use lapply() to find each day's maximum temperature
    lapply(temp, max)

    # Use sapply() to find each day's maximum temperature
    sapply(temp, max)

sapply with your own function
Like lapply(), sapply() allows you to use self-defined functions and apply them over a vector or a list:

sapply(X, FUN, ...)

Here, FUN can be one of R's built-in functions, but it can also be a function you wrote. This self-written function 
can be defined before hand, or can be inserted directly as an anonymous function.

    Example:
    # temp is already defined in the workspace

    # Finish function definition of extremes_avg
    extremes_avg <- function(x) {
        ( min(x) + max(x) ) / 2
    }

    # Apply extremes_avg() over temp using sapply()
    sapply(temp, extremes_avg)

    # Apply extremes_avg() over temp using lapply()
    lapply(temp, extremes_avg)

sapply with function returning vector
In the previous exercises, you've seen how sapply() simplifies the list that lapply() would return by turning it into 
a vector. But what if the function you're applying over a list or a vector returns a vector of length greater than 1? 
If you don't remember from the video, don't waste more time in the valley of ignorance and head over to the 
instructions!

    Example: 
    # temp is already available in the workspace

    # Create a function that returns min and max of a vector: extremes
    extremes <- function(x) {
        c(min = min(x), max = max(x))
    }

    # Apply extremes() over temp with sapply()
    sapply(temp, extremes)

    # Apply extremes() over temp with lapply()
    lapply(temp, extremes)

sapply can't simplify, now what?
It seems like we've hit the jackpot with sapply(). On all of the examples so far, sapply() was able to nicely simplify 
the rather bulky output of lapply(). But, as with life, there are things you can't simplify. How does sapply() react?

We already created a function, below_zero(), that takes a vector of numerical values and returns a vector that only 
contains the values that are strictly below zero.

    Example:
    # temp is already prepared for you in the workspace

    # Definition of below_zero()
    below_zero <- function(x) {
        return(x[x < 0])
    }

    # Apply below_zero over temp using sapply(): freezing_s
    freezing_s <- sapply(temp, below_zero)

    # Apply below_zero over temp using lapply(): freezing_l
    freezing_l <- lapply(temp, below_zero)

    # Are freezing_s and freezing_l identical?
    identical(freezing_s, freezing_l)

sapply with functions that return NULL
You already have some apply tricks under your sleeve, but you're surely hungry for some more, aren't you? In this 
exercise, you'll see how sapply() reacts when it is used to apply a function that returns NULL over a vector or a 
list.

A function print_info(), that takes a vector and prints the average of this vector, has already been created for you. 
It uses the cat() function.

    Example:
    # temp is already available in the workspace

    # Definition of print_info()
    print_info <- function(x) {
        cat("The average temperature is", mean(x), "\n")
    }

    # Apply print_info() over temp using sapply()
    sapply(temp, print_info)

    # Apply print_info() over temp using lapply()
    lapply(temp, print_info)

Reverse engineering sapply

sapply(list(runif (10), runif (10)), 
       function(x) c(min = min(x), mean = mean(x), max = max(x)))

Without going straight to the console to run the code, try to reason through which of the following statements are 
correct and why.

(1) sapply() can't simplify the result that lapply() would return, and thus returns a list of vectors.
(2) This code generates a matrix with 3 rows and 2 columns.
(3) The function that is used inside sapply() is anonymous.
(4) The resulting data structure does not contain any names.

Select the option that lists all correct statements.

###################################################################################################################
Vapply:

Use vapply
Before you get your hands dirty with the third and last apply function that you'll learn about in this intermediate R 
course, let's take a look at its syntax. The function is called vapply(), and it has the following syntax:

vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)

Over the elements inside X, the function FUN is applied. The FUN.VALUE argument expects a template for the return 
argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, 
if possible.

For the next set of exercises, you'll be working on the temp list again, that contains 7 numerical vectors of length 5. 
We also coded a function basics() that takes a vector, and returns a named vector of length 3, containing the minimum, 
mean and maximum value of the vector respectively.

    Example:
    # temp is already available in the workspace

    # Definition of basics()
    basics <- function(x) {
        c(min = min(x), mean = mean(x), max = max(x))
    }

    # Apply basics() over temp using vapply()
    vapply(temp, basics, numeric(3))

Use vapply (2)
So far you've seen that vapply() mimics the behavior of sapply() if everything goes according to plan. But what if it 
doesn't?

In the video, Filip showed you that there are cases where the structure of the output of the function you want to 
apply, FUN, does not correspond to the template you specify in FUN.VALUE. In that case, vapply() will throw an error 
that informs you about the misalignment between expected and actual output.

    Example: 
    # Incorrect code
    # temp is already available in the workspace

    # Definition of the basics() function
    basics <- function(x) {
        c(min = min(x), mean = mean(x), median = median(x), max = max(x))
    }

    # Fix the error:
    vapply(temp, basics, numeric(3))

    # result
    # temp is already available in the workspace
    # Definition of the basics() function
    basics <- function(x) {
        c(min = min(x), mean = mean(x), median = median(x), max = max(x))
    }
    # Fix the error:
    vapply(temp, basics, numeric(3))
    Error: values must be length 3,
    but FUN(X[[1]]) result is length 

    # Fixed Code
    # temp is already available in the workspace

    # Definition of the basics() function
    basics <- function(x) {
        c(min = min(x), mean = mean(x), median = median(x), max = max(x))
    }

    # Fix the error:
    vapply(temp, basics, numeric(4))

From sapply to vapply
As highlighted before, vapply() can be considered a more robust version of sapply(), because you explicitly restrict 
the output of the function you want to apply. Converting your sapply() expressions in your own R scripts to vapply() 
expressions is therefore a good practice (and also a breeze!).

    Example:
    # temp is already defined in the workspace

    # Convert to vapply() expression
    # sapply(temp, max)
    vapply(temp, max, numeric(1))

    # Convert to vapply() expression
    # sapply(temp, function(x, y) { mean(x) > y }, y = 5)
    vapply(temp, function(x, y) { mean(x) > y }, logical(1), y = 5)

####################################################################################################################
Useful Functions:

Mathematical utilities
Have another look at some useful math functions that R features:

abs(): Calculate the absolute value.
sum(): Calculate the sum of all the values in a data structure.
mean(): Calculate the arithmetic mean.
round(): Round the values to 0 decimal places by default. Try out ?round in the console for variations of round() and 
ways to change the number of digits to round to.
As a data scientist in training, you've estimated a regression model on the sales data for the past six months. After 
evaluating your model, you see that the training error of your model is quite regular, showing both positive and 
negative values. A vector errors containing the error values has been pre-defined for you.

    Example:
    # The errors vector has already been defined for you
    errors <- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3)

    # Sum of absolute rounded values of errors
    print(sum(abs(round(errors))))

We went ahead and pre-loaded some code for you, but there's still an error. Can you trace it and fix it?

In times of despair, help with functions such as sum() and rev() are a single command away; simply execute the 
code ?sum and ?rev.

    Example:
    # Don't edit these two lines
    vec1 <- c(1.5, 2.5, 8.4, 3.7, 6.3)
    vec2 <- rev(vec1)

    # Fix the error
    mean(c(abs(vec1), abs(vec2)))


Data Utilities
R features a bunch of functions to juggle around with data structures::

seq(): Generate sequences, by specifying the from, to, and by arguments.
rep(): Replicate elements of vectors and lists.
sort(): Sort a vector in ascending order. Works on numerics, but also on character strings and logicals.
rev(): Reverse the elements in a data structures for which reversal is defined.
str(): Display the structure of any R object.
append(): Merge vectors or lists.
is.*(): Check for the class of an R object.
as.*(): Convert an R object from one class to another.
unlist(): Flatten (possibly embedded) lists to produce a vector.
Remember the social media profile views data? Your LinkedIn and Facebook view counts for the last seven days have 
been pre-defined as lists.

    Example:
    # The linkedin and facebook lists have already been created for you
    linkedin <- list(16, 9, 13, 5, 2, 17, 14)
    facebook <- list(17, 7, 5, 16, 8, 13, 14)

    # Convert linkedin and facebook to a vector: li_vec and fb_vec
    li_vec <- unlist(linkedin)
    fb_vec <- unlist(facebook)

    # Append fb_vec to li_vec: social_vec
    social_vec <- append(li_vec, fb_vec)

    # Sort social_vec
    print(sort(social_vec, decreasing = TRUE))

Find the error (2)
Just as before, let's switch roles. It's up to you to see what unforgivable mistakes we've made. Go fix them!

    Example: 
    # Incorrect code:
    # Fix me
    seq(rep(1, 7, by = 2), times = 7)

    #Corrected Code:
    # Fix me
    rep(seq(1, 7, by = 2), times = 7)

Beat Gauss using R
There is a popular story about young Gauss. As a pupil, he had a lazy teacher who wanted to keep the classroom busy 
by having them add up the numbers 1 to 100. Gauss came up with an answer almost instantaneously, 5050. On the spot, 
he had developed a formula for calculating the sum of an arithmetic series. There are more general formulas for 
calculating the sum of an arithmetic series with different starting values and increments. Instead of deriving such 
a formula, why not use R to calculate the sum of a sequence?

    Example:
    # Create first sequence: seq1
    seq1 <- seq(1, 500, by = 3)

    # Create second sequence: seq2
    seq2 <- seq(1200, 900, by = -7)

    # Calculate total sum of the sequences
    print(sum(seq1) + sum(seq2))

#####################################################################################################################
Regular Expressions: 

grepl & grep
In their most basic form, regular expressions can be used to see whether a pattern exists inside a character string or 
a vector of character strings. For this purpose, you can use:

grepl(), which returns TRUE when a pattern is found in the corresponding character string.

grep(), which returns a vector of indices of the character strings that contains the pattern.

Both functions need a pattern and an x argument, where pattern is the regular expression you want to match for, and 
the x argument is the character vector from which matches should be sought.

In this and the following exercises, you'll be querying and manipulating a character vector of email addresses! The 
vector emails has been pre-defined so you can begin with the instructions straight away!

    Example:
    # The emails vector has already been defined for you
    emails <- c("john.doe@ivyleague.edu", "education@world.gov", "dalai.lama@peace.org",
                "invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")

    # Use grepl() to match for "edu"
    grepl("edu", emails)

    # Use grep() to match for "edu", save result to hits
    hits <- grep("edu", emails)

    # Subset emails using hits
    emails[hits]


grepl & grep (2)
You can use the caret, ^, and the dollar sign, $ to match the content located in the start and end of a string, 
respectively. This could take us one step closer to a correct pattern for matching only the ".edu" email addresses 
from our list of emails. But there's more that can be added to make the pattern more robust:

@, because a valid email must contain an at-sign.

.*, which matches any character (.) zero or more times (*). Both the dot and the asterisk are metacharacters. You can 
use them to match any character between the at-sign and the ".edu" portion of an email address.

\\.edu$, to match the ".edu" part of the email at the end of the string. The \\ part escapes the dot: it tells R that 
you want to use the . as an actual character.

    Example:
    # The emails vector has already been defined for you
    emails <- c("john.doe@ivyleague.edu", "education@world.gov", "dalai.lama@peace.org",
                "invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")

    # Use grepl() to match for .edu addresses more robustly
    print(grepl("@.*\\.edu$", emails))

    # Use grep() to match for .edu addresses more robustly, save result to hits
    hits <- grep("@.*\\.edu$", emails)

    # Subset emails using hits
    emails[hits]

sub & gsub
While grep() and grepl() were used to simply check whether a regular expression could be matched with a character 
vector, sub() and gsub() take it one step further: you can specify a replacement argument. If inside the character 
vector x, the regular expression pattern is found, the matching element(s) will be replaced with replacement. sub() 
only replaces the first match, whereas gsub() replaces all matches.

Suppose that emails vector you've been working with is an excerpt of DataCamp's email database. Why not offer the 
owners of the .edu email addresses a new email address on the datacamp.edu domain? This could be quite a powerful 
marketing stunt: Online education is taking over traditional learning institutions! Convert your email and be a part 
of the new generation!

    Example:
    # The emails vector has already been defined for you
    emails <- c("john.doe@ivyleague.edu", "education@world.gov", "global@peace.org",
                "invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")

    # Use sub() to convert the email domains to datacamp.edu
    print(sub("@.*\\.edu$", "@datacamp.edu", emails))

sub & gsub (2)
Regular expressions are a typical concept that you'll learn by doing and by seeing other examples. Before you rack 
your brains over the regular expression in this exercise, have a look at the new things that will be used:

.*: A usual suspect! It can be read as "any character that is matched zero or more times".
\\s: Match a space. The "s" is normally a character, escaping it (\\) makes it a metacharacter.
[0-9]+: Match the numbers 0 to 9, at least once (+).
([0-9]+): The parentheses are used to make parts of the matching string available to define the replacement. The \\1 
in the replacement argument of sub() gets set to the string that is captured by the regular expression [0-9]+.

awards <- c("Won 1 Oscar.",
  "Won 1 Oscar. Another 9 wins & 24 nominations.",
  "1 win and 2 nominations.",
  "2 wins & 3 nominations.",
  "Nominated for 2 Golden Globes. 1 more win & 2 nominations.",
  "4 wins & 1 nomination.")

sub(".*\\s([0-9]+)\\snomination.*$", "\\1", awards)

What does this code chunk return? awards is already defined in the workspace so you can start playing in the console 
straight away.

#####################################################################################################################
Times & Dates:

Right here, right now
In R, dates are represented by Date objects, while times are represented by POSIXct objects. Under the hood, however, 
these dates and times are simple numerical values. Date objects store the number of days since the 1st of January in 
1970. POSIXct objects on the other hand, store the number of seconds since the 1st of January in 1970.

The 1st of January in 1970 is the common origin for representing times and dates in a wide range of programming 
languages. There is no particular reason for this; it is a simple convention. Of course, it's also possible to create 
dates and times before 1970; the corresponding numerical values are simply negative in this case.

    Example: 
    # Get the current date: today
    today <- Sys.Date()

    # See what today looks like under the hood
    unclass(today)

    # Get the current time: now
    now <- Sys.time()

    # See what now looks like under the hood
    unclass(now)

Create and format dates
To create a Date object from a simple character string in R, you can use the as.Date() function. The character string 
has to obey a format that can be defined using a set of symbols (the examples correspond to 13 January, 1982):

%Y: 4-digit year (1982)
%y: 2-digit year (82)
%m: 2-digit month (01)
%d: 2-digit day of the month (13)
%A: weekday (Wednesday)
%a: abbreviated weekday (Wed)
%B: month (January)
%b: abbreviated month (Jan)

The following R commands will all create the same Date object for the 13th day in January of 1982:

as.Date("1982-01-13")
as.Date("Jan-13-82", format = "%b-%d-%y")
as.Date("13 January, 1982", format = "%d %B, %Y")
Notice that the first line here did not need a format argument, because by default R matches your character string to 
the formats "%Y-%m-%d" or "%Y/%m/%d".

In addition to creating dates, you can also convert dates to character strings that use a different date notation. For 
this, you use the format() function. Try the following lines of code:

today <- Sys.Date()
format(Sys.Date(), format = "%d %B, %Y")
format(Sys.Date(), format = "Today is a %A!")

    Example:
    # Definition of character strings representing dates
    str1 <- "May 23, '96"
    str2 <- "2012-03-15"
    str3 <- "30/January/2006"

    # Convert the strings to dates: date1, date2, date3
    date1 <- as.Date(str1, format = "%b %d, '%y")
    date2 <- as.Date(str2)
    date3 <- as.Date(str3, format = "%d/%B/%Y")

    # Convert dates to formatted strings
    format(date1, "%A")
    format(date2, "%d")
    format(date3, "%b %Y")

Create and format times
Similar to working with dates, you can use as.POSIXct() to convert from a character string to a POSIXct object, and 
format() to convert from a POSIXct object to a character string. Again, you have a wide variety of symbols:

%H: hours as a decimal number (00-23)
%I: hours as a decimal number (01-12)
%M: minutes as a decimal number
%S: seconds as a decimal number
%T: shorthand notation for the typical format %H:%M:%S
%p: AM/PM indicator

For a full list of conversion symbols, consult the strptime documentation in the console:

?strptime

Again,as.POSIXct() uses a default format to match character strings. In this case, it's %Y-%m-%d %H:%M:%S. In this 
exercise, abstraction is made of different time zones.

    Example:
    # Definition of character strings representing times
    str1 <- "May 23, '96 hours:23 minutes:01 seconds:45"
    str2 <- "2012-3-12 14:23:08"

    # Convert the strings to POSIXct objects: time1, time2
    time1 <- as.POSIXct(str1, format = "%B %d, '%y hours:%H minutes:%M seconds:%S")
    time2 <- as.POSIXct(str2)

    # Convert times to formatted strings
    format(time1, "%M")
    format(time2, "%I:%M %p")

Calculations with Dates
Both Date and POSIXct R objects are represented by simple numerical values under the hood. This makes calculation with 
time and date objects very straightforward: R performs the calculations using the underlying numerical values, and 
then converts the result back to human-readable time information again.

You can increment and decrement Date objects, or do actual calculations with them:

today <- Sys.Date()
today + 1
today - 1

as.Date("2015-03-12") - as.Date("2015-02-27")
To control your eating habits, you decided to write down the dates of the last five days that you ate pizza. In the 
workspace, these dates are defined as five Date objects, day1 to day5. A vector pizza containing these 5 Date objects 
has been pre-defined for you.

    Example:
    # day1, day2, day3, day4 and day5 are already available in the workspace

    # Difference between last and first pizza day
    print(as.Date(day5) - as.Date(day1))

    # Create vector pizza
    pizza <- c(day1, day2, day3, day4, day5)

    # Create differences between consecutive pizza days: day_diff
    day_diff <- diff(pizza)

    # Average period between two consecutive pizza days
    print(mean(day_diff, na.rm = TRUE))

Calculations with Times
Calculations using POSIXct objects are completely analogous to those using Date objects. Try to experiment with this 
code to increase or decrease POSIXct objects:

now <- Sys.time()
now + 3600          # add an hour
now - 3600 * 24     # subtract a day

Adding or subtracting time objects is also straightforward:

birth <- as.POSIXct("1879-03-14 14:37:23")
death <- as.POSIXct("1955-04-18 03:47:12")
einstein <- death - birth
einstein

You're developing a website that requires users to log in and out. You want to know what is the total and average 
amount of time a particular user spends on your website. This user has logged in 5 times and logged out 5 times as 
well. These times are gathered in the vectors login and logout, which are already defined in the workspace.

    Example:
    # login and logout are already defined in the workspace
    # Calculate the difference between login and logout: time_online
    time_online <- logout - login

    # Inspect the variable time_online
    print(time_online)

    # Calculate the total time online
    print(sum(time_online, na.rm = TRUE))

    # Calculate the average time online
    print(mean(time_online, na.rm = TRUE))

Time is of the essence
The dates when a season begins and ends can vary depending on who you ask. People in Australia will tell you that 
spring starts on September 1st. The Irish people in the Northern hemisphere will swear that spring starts on February 
1st, with the celebration of St. Brigid's Day. Then there's also the difference between astronomical and meteorological 
seasons: while astronomers are used to equinoxes and solstices, meteorologists divide the year into 4 fixed seasons 
that are each three months long. (source: www.timeanddate.com)

A vector astro, which contains character strings representing the dates on which the 4 astronomical seasons start, 
has been defined on your workspace. Similarly, a vector meteo has already been created for you, with the meteorological 
beginnings of a season.

    Examples:
    # Convert astro to vector of Date objects: astro_dates
    astro_dates <- as.Date(astro, format = "%d-%b-%Y")

    # Convert meteo to vector of Date objects: meteo_dates
    meteo_dates <- as.Date(meteo, format = "%B %d, %y")

    # Calculate the maximum absolute difference between astro_dates and meteo_dates
    print(max(abs(astro_dates - meteo_dates)))

#######################################################################################################################################
Why you should use functions:

Calling functions
One way to make your code more readable is to be careful about the order you pass arguments when you call functions, and whether you pass 
the arguments by position or by name.

gold_medals, a numeric vector of the number of gold medals won by each country in the 2016 Summer Olympics, is provided.

For convenience, the arguments of median() and rank() are displayed using args(). Setting rank()'s na.last argument to "keep" means "keep 
the rank of NA values as NA".

Best practice for calling functions is to include them in the order shown by args(), and to only name rare arguments.

    Example:
    # Look at the gold medals data
    gold_medals

    # Note the arguments to median()
    args(median)

    # Rewrite this function call, following best practices
    median(gold_medals, na.rm = TRUE)

    Example:
    # Note the arguments to rank()
    args(rank)

    # Rewrite this function call, following best practices
    rank(-gold_medals, na.last = "keep",ties.method = "min")

########################################################################################################################################
Converting scripts into functions:

Syntax: 
my_fun <- function(arg1, arg2) {
    # Do something
}

Your first function: tossing a coin
Time to write your first function! It's a really good idea when writing functions to start simple. You can always make a function more 
complicated later if it's really necessary, so let's not worry about arguments for now.

    Example 1:
    coin_sides <- c("head", "tail")

    # Sample from coin_sides once
    sample(coin_sides, 1)

    -------------------------------------
    # Write a template for your function, toss_coin()
    toss_coin <- function()
    # (Leave the contents of the body for later)
    # Add punctuation to finish the body
    {}

    -------------------------------------
    # Your script, from a previous step
    coin_sides <- c("head", "tail")
    sample(coin_sides, 1)

    # Paste your script into the function body
    toss_coin <- function() {
        coin_sides <- c("head", "tail")
        sample(coin_sides, 1)
    }

    --------------------------------------
    # Your functions, from previous steps
    toss_coin <- function() {
        coin_sides <- c("head", "tail")
        sample(coin_sides, 1)
    }

    # Call your function
    toss_coin()

Inputs to functions
Most functions require some sort of input to determine what to compute. The inputs to functions are called arguments. You specify them 
inside the parentheses after the word "function."

As mentioned in the video, the following exercises assume that you are using sample() to do random sampling.

    Example 2:
    coin_sides <- c("head", "tail")
    n_flips <- 10

    # Sample from coin_sides n_flips times with replacement
    sample(coin_sides, n_flips, replace = TRUE)

    ------------------------------------------
    # Update the function to return n coin tosses
    toss_coin <- function(n_flips) {
        coin_sides <- c("head", "tail")
        sample(coin_sides, n_flips, replace = TRUE)
    }

    # Generate 10 coin tosses
    toss_coin(10)

Multiple inputs to functions
If a function should have more than one argument, list them in the function signature, separated by commas.

To solve this exercise, you need to know how to specify sampling weights to sample(). Set the prob argument to a numeric vector with the 
same length as x. Each value of prob is the probability of sampling the corresponding element of x, so their values add up to one. In 
the following example, each sample has a 20% chance of "bat", a 30% chance of "cat" and a 50% chance of "rat".

sample(c("bat", "cat", "rat"), 10, replace = TRUE, prob = c(0.2, 0.3, 0.5))

    Example: 
    coin_sides <- c("head", "tail")
    n_flips <- 10
    p_head <- 0.8

    # Define a vector of weights
    weights <- c(p_head, 1 - p_head)

    # Update so that heads are sampled with prob p_head
    sample(coin_sides, n_flips, replace = TRUE, prob = c(weights))

    -------------------------------------------------------------
    # Update the function so heads have probability p_head
    toss_coin <- function(n_flips, p_head) {
        coin_sides <- c("head", "tail")
        # Define a vector of weights
        weights <- c(p_head, 1 - p_head)
        # Modify the sampling to be weighted
        sample(coin_sides, n_flips, replace = TRUE, prob = weights)
    }

    # Generate 10 coin tosses
    toss_coin(10, 0.80)

#######################################################################################################################################
Y kant I reed ur code?

Renaming GLM
R's generalized linear regression function, glm(), suffers the same usability problems as lm(): its name is an acronym, and its formula 
and data arguments are in the wrong order.

To solve this exercise, you need to know two things about generalized linear regression:

glm() formulas are specified like lm() formulas: response is on the left, and explanatory variables are added on the right.
To model count data, set glm()'s family argument to poisson, making it a Poisson regression.
Here you'll use data on the number of yearly visits to Snake River at Jackson Hole, Wyoming, snake_river_visits.

    Example:
    # Run a generalized linear regression 
    glm(
        # Model no. of visits vs. gender, income, travel
        n_visits ~ gender + income + travel, 
        # Use the snake_river_visits dataset
        data = snake_river_visits, 
        # Make it a Poisson regression
        family = poisson
    )

    --------------------------------------------------------------------------
    # Write a function to run a Poisson regression
    run_poisson_regression <- function(data, formula) {
        glm(formula, family = poisson, data)
    }

    ---------------------------------------------------------------------------
    # From previous step
        run_poisson_regression <- function(data, formula) {
        glm(formula, data, family = poisson)
    }

    # Re-run the Poisson regression, using your function
    model <- snake_river_visits %>%
        run_poisson_regression(n_visits ~ gender + income + travel)

    # Run this to see the predictions
    snake_river_explanatory %>%
        mutate(predicted_n_visits = predict(model, ., type = "response"))%>%
        arrange(desc(predicted_n_visits))

########################################################################################################################################
Default Arguments:

Numeric defaults
cut_by_quantile() converts a numeric vector into a categorical variable where quantiles define the cut points. This is a useful function, 
but at the moment you have to specify five arguments to make it work. This is too much thinking and typing.

By specifying default arguments, you can make it easier to use. Let's start with n, which specifies how many categories to cut x into.

A numeric vector of the number of visits to Snake River is provided as n_visits.

    Example 1:
    # Set the default for n to 5
    cut_by_quantile <- function(x, n= 5, na.rm, labels, interval_type) {
        probs <- seq(0, 1, length.out = n + 1)
        qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
        right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
        cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
    }

    # Remove the n argument from the call
    cut_by_quantile(
        n_visits,  
        na.rm = FALSE, 
        labels = c("very low", "low", "medium", "high", "very high"),
        interval_type = "(lo, hi]"
    )

    Example 2:
    # Set the default for na.rm to FALSE
    cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels, interval_type) {
        probs <- seq(0, 1, length.out = n + 1)
        qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
        right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
        cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
    }

    # Remove the na.rm argument from the call
    cut_by_quantile(
        n_visits, 
        labels = c("very low", "low", "medium", "high", "very high"),
        interval_type = "(lo, hi]"
    )

NULL defaults
The cut() function used by cut_by_quantile() can automatically provide sensible labels for each category. The code to generate these 
labels is pretty complicated, so rather than appearing in the function signature directly, its labels argument defaults to NULL, and 
the calculation details are shown on the ?cut help page.

    Example: 
    # Set the default for labels to NULL
    cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, interval_type) {
        probs <- seq(0, 1, length.out = n + 1)
        qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
        right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
        cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
    }

    # Remove the labels argument from the call
    cut_by_quantile(
        n_visits,
        interval_type = "(lo, hi]"
    )

Categorical defaults
When cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom (in mathematical terminology, "open on the left, closed on the right", or (lo, hi]). Or you can choose the opposite ("closed on the left, open on the right", or [lo, hi)). cut_by_quantile() should allow these two choices.

The pattern for categorical defaults is:

function(cat_arg = c("choice1", "choice2")) {
  cat_arg <- match.arg(cat_arg)
}

Free hint: In the console, type head(rank) to see the start of rank()'s definition, and look at the ties.method argument.

    Example:
    # Set the categories for interval_type to "(lo, hi]" and "[lo, hi)"
    cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, 
                                interval_type = c("(lo, hi]", "[lo, hi)")) {
        # Match the interval_type argument
        interval_type <- "(lo, hi]"
        probs <- seq(0, 1, length.out = n + 1)
        qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
        right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
        cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
    }

    # Remove the interval_type argument from the call
    cut_by_quantile(n_visits)

#######################################################################################################################################
Passing arguments between functions:

Harmonic mean
The harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. That is


The harmonic mean is often used to average ratio data. You'll be using it on the price/earnings ratio of stocks in the Standard and Poor's 500 index, provided as std_and_poor500. Price/earnings ratio is a measure of how expensive a stock is.

The dplyr package is loaded.

    Example: 
    # Look at the Standard and Poor 500 data
    glimpse(std_and_poor500)

    # Write a function to calculate the reciprocal
    get_reciprocal <- function(x) {
        return(1/x)
    }

    -----------------------------------------------------------------
    # From previous step
    get_reciprocal <- function(x) {
        1 / x
    }

    # Write a function to calculate the harmonic mean
    calc_harmonic_mean <- function(x) {
        x %>%
            get_reciprocal %>%
            mean() %>%
            get_reciprocal
    }

    ------------------------------------------------------------------
    # From previous steps
    get_reciprocal <- function(x) {
        1 / x
    }
    calc_harmonic_mean <- function(x) {
        x %>%
            get_reciprocal() %>%
            mean() %>%
            get_reciprocal()
    }

    std_and_poor500 %>% 
        # Group by sector
        group_by(sector) %>% 
        # Summarize, calculating harmonic mean of P/E ratio
        summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))

Dealing with missing values
In the last exercise, many sectors had an NA value for the harmonic mean. It would be useful for your function to be able to remove 
missing values before calculating.

Rather than writing your own code for this, you can outsource this functionality to mean().

The dplyr package is loaded.

    Example:
    # Add an na.rm arg with a default, and pass it to mean()
    calc_harmonic_mean <- function(x, na.rm = FALSE) {
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
    }

    ---------------------------------------------------------------------------
    # From previous step
    calc_harmonic_mean <- function(x, na.rm = FALSE) {
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
    }

    std_and_poor500 %>% 
        # Group by sector
        group_by(sector) %>% 
        # Summarize, calculating harmonic mean of P/E ratio
        summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))

Passing arguments with ...
Rather than explicitly giving calc_harmonic_mean() and na.rm argument, you can use ... to simply "pass other arguments" to mean().

The dplyr package is loaded.

    Example:
    # Swap na.rm arg for ... in signature and body
    calc_harmonic_mean <- function(x, ...) {
    x %>%
        get_reciprocal() %>%
        mean(...) %>%
        get_reciprocal()
    }

    --------------------------------------------------------------------------------
    calc_harmonic_mean <- function(x, ...) {
    x %>%
        get_reciprocal() %>%
        mean(...) %>%
        get_reciprocal()
    }

    std_and_poor500 %>% 
        # Group by sector
        group_by(sector) %>% 
        # Summarize, calculating harmonic mean of P/E ratio
        summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))

#######################################################################################################################################
Checking Arguments:

Throwing errors with bad arguments
If a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are

Throw the error message as soon as you realize there is a problem (typically at the start of the function).

Make the error message easily understandable.

You can use the assert_*() functions from assertive to check inputs and throw errors when they fail.

    Example:
    calc_harmonic_mean <- function(x, na.rm = FALSE) {
    # Assert that x is numeric
    assert_is_numeric(x)
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
    }

    # See what happens when you pass it strings
    calc_harmonic_mean(std_and_poor500$sector)

Custom error logic
Sometimes the assert_*() functions in assertive don't give the most informative error message. For example, the assertions that check if 
a number is in a numeric range will tell the user that a value is out of range, but the won't say why that's a problem. In that case, you 
can use the is_*() functions in conjunction with messages, warnings, or errors to define custom feedback.

The harmonic mean only makes sense when x has all positive values. (Try calculating the harmonic mean of one and minus one to see why.) 
Make sure your users know this!

    Example:
    calc_harmonic_mean <- function(x, na.rm = FALSE) {
    assert_is_numeric(x)
    # Check if any values of x are non-positive
    if(any(is_non_positive(x), na.rm = TRUE)) {
        # Throw an error
        stop("x contains non-positive values, so the harmonic mean makes no sense.")
    }
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
    }

    # See what happens when you pass it negative numbers
    calc_harmonic_mean(std_and_poor500$pe_ratio - 20)

Fixing function arguments
The harmonic mean function is almost complete. However, you still need to provide some checks on the na.rm argument. This time, rather 
than throwing errors when the input is in an incorrect form, you are going to try to fix it.

na.rm should be a logical vector with one element (that is, TRUE, or FALSE).

The assertive package is loaded for you.

    Example:
    # Update the function definition to fix the na.rm argument
    calc_harmonic_mean <- function(x, na.rm = FALSE) {
        assert_is_numeric(x)
        if(any(is_non_positive(x), na.rm = TRUE)) {
            stop("x contains non-positive values, so the harmonic mean makes no sense.")
    }
    # Use the first value of na.rm, and coerce to logical
    na.rm <- coerce_to(use_first(na.rm), "logical")
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
    }

    # See what happens when you pass it malformed na.rm
    calc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)

#####################################################################################################################################
Returning Values from Functions:

Returning early
Sometimes, you don't need to run through the whole body of a function to get the answer. In that case you can return early from that 
function using return().

To check if x is divisible by n, you can use is_divisible_by(x, n) from assertive.

Alternatively, use the modulo operator, %%. x %% n gives the remainder when dividing x by n, so x %% n == 0 determines whether x is 
divisible by n. Try 1:10 %% 3 == 0 in the console.

To solve this exercise, you need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn't a century 
(like 1904 but not 1900 or 1905).

assertive is loaded.

    Example:
    is_leap_year <- function(year) {
    # If year is div. by 400 return TRUE
    if(year %% 400 == 0) {
        return(TRUE)
    }
    # If year is div. by 100 return FALSE
    if(year %% 100 == 0) {
        return(FALSE)
    }  
    # If year is div. by 4 return TRUE
    if(year %% 4 == 0) {
        return(TRUE)
    }
    
    
    # Otherwise return FALSE
    return(FALSE)
    }

Returning invisibly
When the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a 
return value to be printed as well. In that case, the value should be invisibly returned.

The base R plot function returns NULL, since its main purpose is to draw a plot. This isn't helpful if you want to use it in piped code: 
instead it should invisibly return the plot data to be piped on to the next step.

Recall that plot() has a formula interface: instead of giving it vectors for x and y, you can specify a formula describing which columns 
of a data frame go on the x and y axes, and a data argument for the data frame. Note that just like lm(), the arguments are the wrong way 
round because the detail argument, formula, comes before the data argument.

plot(y ~ x, data = data)

    Example:
    # Using cars, draw a scatter plot of dist vs. speed
    plt_dist_vs_speed <- plot(dist ~ speed, data = cars)

    # Oh no! The plot object is NULL
    plt_dist_vs_speed

    -------------------------------------------------------------------------
    # Define a pipeable plot fn with data and formula args
    pipeable_plot <- function(data, formula) {
        # Call plot() with the formula interface
        plot(formula, data = data)
        # Invisibly return the input dataset
        invisible(data)
    }

    # Draw the scatter plot of dist vs. speed again
    plt_dist_vs_speed <- cars %>% 
        pipeable_plot(speed ~ dist)

    # Now the plot object has a value
    plt_dist_vs_speed

####################################################################################################################################
Returning Multiple Values from Functions:

Returning many things
Functions can only return one value. If you want to return multiple things, then you can store them all in a list.

If users want to have the list items as separate variables, they can assign each list element to its own variable using zeallot's 
multi-assignment operator, %<-%.

glance(), tidy(), and augment() each take the model object as their only argument.

The Poisson regression model of Snake River visits is available as model. broom and zeallot are loaded.

    Example: 
    # Look at the structure of model (it's a mess!)
    str(model)

    # Use broom tools to get a list of 3 data frames
    list(
        # Get model-level values
        model = glance(model),
        # Get coefficient-level values
        coefficients = tidy(model),
        # Get observation-level values
        observations = augment(model)
    )

    ------------------------------------------------------------------------------
    # Wrap this code into a function, groom_model
    groom_model <- function(model)
    list(
        model = glance(model),
        coefficients = tidy(model),
        observations = augment(model)
    )
    groom_model(model)

    --------------------------------------------------------------------------------
    # From previous step
    groom_model <- function(model) {
    list(
        model = glance(model),
        coefficients = tidy(model),
        observations = augment(model)
    )
    }

    # Call groom_model on model, assigning to 3 variables
    c(mdl, cff, obs) %<-% groom_model(model)

    # See these individual variables
    mdl; cff; obs

Returning metadata
Sometimes you want to return multiple things from a function, but you want the result to have a particular class (for example, a data 
frame or a numeric vector), so returning a list isn't appropriate. This is common when you have a result plus metadata about the result. 
(Metadata is "data about the data". For example, it could be the file a dataset was loaded from, or the username of the person who 
created the variable, or the number of iterations for an algorithm to converge.)

In that case, you can store the metadata in attributes. Recall the syntax for assigning attributes is as follows.

attr(object, "attribute_name") <- attribute_value

    Example:
    pipeable_plot <- function(data, formula) {
        plot(formula, data)
        # Add a "formula" attribute to data
        attr(data, "formula") <- formula
        invisible(data)
    }

    # From previous exercise
    plt_dist_vs_speed <- cars %>% 
        pipeable_plot(dist ~ speed)

    # Examine the structure of the result
    str(plt_dist_vs_speed)

#######################################################################################################################################
Environments:

Creating and exploring environments
Environments are used to store other variables. Mostly, you can think of them as lists, but there's an important extra property that is 
relevant to writing functions. Every environment has a parent environment (except the empty environment, at the root of the environment 
tree). This determines which variables R know about at different places in your code.

Facts about the Republic of South Africa are contained in capitals, national_parks, and population.

    Example:
    # Add capitals, national_parks, & population to a named list
    rsa_lst <- list(
        capitals = capitals,
        national_parks = national_parks,
        population = population
    )

    # List the structure of each element of rsa_lst
    ls.str(rsa_lst)

    --------------------------------------------------------------------------
    # From previous step
    rsa_lst <- list(
        capitals = capitals,
        national_parks = national_parks,
        population = population
    )

    # Convert the list to an environment
    rsa_env <- list2env(rsa_lst)

    # List the structure of each variable
    ls.str(rsa_env)

    ---------------------------------------------------------------------------
    # From previous steps
    rsa_lst <- list(
        capitals = capitals,
        national_parks = national_parks,
        population = population
    )
    rsa_env <- list2env(rsa_lst)

    # Find the parent environment of rsa_env
    parent <- parent.env(rsa_env)

    # Print its name
    environmentName(parent)

Do variables exist?
If R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and 
so on until it finds it.

rsa_env has been modified so it includes capitals and national_parks, but not population.

    Example:
    # Compare the contents of the global environment and rsa_env
    ls.str(globalenv())
    ls.str(rsa_env)

    # Does population exist in rsa_env?
    exists("population", envir = rsa_env)

    # Does population exist in rsa_env, ignoring inheritance?
    exists("population", envir = rsa_env, inherits = FALSE)

########################################################################################################################################
Scope and Precedence:

Watch the video!!

########################################################################################################################################
Grain yields and unit conversion:

Converting areas to metric 1
In this chapter, you'll be working with grain yield data from the United States Department of Agriculture, National Agricultural 
Statistics Service. Unfortunately, they report all areas in acres. So, the first thing you need to do is write some utility functions 
to convert areas in acres to areas in hectares.

To solve this exercise, you need to know the following:

There are 4840 square yards in an acre.
There are 36 inches in a yard and one inch is 0.0254 meters.
There are 10000 square meters in a hectare.

    Example:
    # Write a function to convert acres to sq. yards
    acres_to_sq_yards <- function(acres) {
        acres * 4840
    }

    ----------------------------------------------------------------------------------
    # Write a function to convert yards to meters
    yards_to_meters <- function(yards){
        yards * 0.9144
    }

    ----------------------------------------------------------------------------------
    # Write a function to convert sq. meters to hectares
    sq_meters_to_hectares <- function(sq_meters) {
        sq_meters / 10000
    }

Converting areas to metric 2
You're almost there with creating a function to convert acres to hectares. You need another utility function to deal with getting from 
square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function. 
Finally, in the next exercise you'll be calculating area conversions in the denominator of a ratio, so you'll need a harmonic 
acre-to-hectare conversion function.

Free hints: magrittr's raise_to_power() will be useful here. The last step is similar to Chapter 2's Harmonic Mean.

The three utility functions from the last exercise (acres_to_sq_yards(), yards_to_meters(), and sq_meters_to_hectares()) are available, 
as is your get_reciprocal() from Chapter 2. magrittr is loaded.

    Example:
    # Write a function to convert sq. yards to sq. meters
    sq_yards_to_sq_meters <- function(sq_yards) {
    sq_yards %>%
        # Take the square root
        magrittr::raise_to_power(0.5) %>%
        # Convert yards to meters
        yards_to_meters() %>%
        # Square it
        magrittr::raise_to_power(2)
    }

    ----------------------------------------------------------------------------------
    # Load the function from the previous step
    load_step2()

    # Write a function to convert acres to hectares
    acres_to_hectares <- function(acres) {
        acres %>%
            # Convert acres to sq yards
            acres_to_sq_yards() %>%
            # Convert sq yards to sq meters
            sq_yards_to_sq_meters() %>%
            # Convert sq meters to hectares
            sq_meters_to_hectares()
    }

    ------------------------------------------------------------------------------------
    # Load the functions from the previous steps
    load_step3()

    # Define a harmonic acres to hectares function
    harmonic_acres_to_hectares <- function(acres) {
    acres %>% 
        # Get the reciprocal
        get_reciprocal() %>%
        # Convert acres to hectares
        acres_to_hectares() %>% 
        # Get the reciprocal again
        get_reciprocal()
    }

Converting yields to metric
The yields in the NASS corn data are also given in US units, namely bushels per acre. You'll need to write some more utility functions 
to convert this unit to the metric unit of kg per hectare.

Bushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for 
each grain! To solve this exercise, you need to know these facts.

One pound (lb) is 0.45359237 kilograms (kg).
One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat.
magrittr is loaded.

    Example:
    # Write a function to convert lb to kg
    lbs_to_kgs <- function(lbs) {
        lbs * 0.45359237
    }

    -------------------------------------------------------------------
    # Write a function to convert bushels to lbs
    bushels_to_lbs <- function(bushels, crop) {
    # Define a lookup table of scale factors
    c(barley = 48, corn = 56, wheat = 60) %>%
        # Extract the value for the crop
        extract(crop) %>%
        # Multiply by the no. of bushels
        multiply_by(bushels)
    }

    --------------------------------------------------------------------
    # Load fns defined in previous steps
    load_step3()

    # Write a function to convert bushels to kg
    bushels_to_kgs <- function(bushels, crop) {
    bushels %>%
        # Convert bushels to lbs for this crop
        bushels_to_lbs(crop = crop) %>%
        # Convert lbs to kgs
        lbs_to_kgs()
    }

    ---------------------------------------------------------------------
    # Load fns defined in previous steps
    load_step4()

    # Write a function to convert bushels/acre to kg/ha
    bushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, crop = c("barley", "corn", "wheat")) {
    # Match the crop argument
    crop <- match.arg(crop)
    bushels_per_acre %>%
        # Convert bushels to kgs for this crop
        bushels_to_kgs(crop = crop) %>%
        # Convert harmonic acres to ha
        harmonic_acres_to_hectares()
    }

Applying the unit conversion
Now that you've written some functions, it's time to apply them! The NASS corn dataset is available, and you can fortify it (jargon 
for "adding new columns") with metrics areas and yields.

This fortification process can also be turned into a function, so you'll define a function for this, and test it on the NASS wheat 
dataset.

    Example:
    # View the corn dataset
    glimpse(corn)

    corn %>%
    # Add some columns
    mutate(
        # Convert farmed area from acres to ha
        farmed_area_ha = acres_to_hectares(farmed_area_acres),
        # Convert yield from bushels/acre to kg/ha
        yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(
        yield_bushels_per_acre,
        crop = "corn"
        )
    )

    --------------------------------------------------------------------------
    # Wrap this code into a function
    fortify_with_metric_units <- function(data,crop){
    data %>%
        mutate(
        farmed_area_ha = acres_to_hectares(farmed_area_acres),
        yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(
            yield_bushels_per_acre, 
            crop = crop
        )
        )
    }

    # Try it on the wheat dataset
    fortify_with_metric_units(wheat, crop = "wheat")

######################################################################################################################################
Visualizing grain yields:

Plotting yields over time
Now that the units have been dealt with, it's time to explore the datasets. An obvious question to ask about each crop is, "how do the 
yields change over time in each US state?" Let's draw a line plot to find out!

ggplot2 is loaded, and corn and wheat datasets are available with metric units.

    Example:
    # Using corn, plot yield (kg/ha) vs. year
    ggplot(corn, aes(x = year, y = yield_kg_per_ha)) +
        # Add a line layer, grouped by state
        geom_line(aes(group = state)) +
        # Add a smooth trend layer
        geom_smooth()

    ------------------------------------------------------------------------------
    # Wrap this plotting code into a function
    plot_yield_vs_year <- function(data) {
    ggplot(data, aes(year, yield_kg_per_ha)) +
        geom_line(aes(group = state)) +
        geom_smooth()
    }

    # Test it on the wheat dataset
    plot_yield_vs_year(wheat)

A nation divided
The USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states 
separately, we can use the USA Census Regions to get 9 groups.

The "Corn Belt", where most US corn is grown is in the "West North Central" and "East North Central" regions. The "Wheat Belt" is in 
the "West South Central" region.

dplyr is loaded, the corn and wheat datasets are available, as is usa_census_regions.

    Example:
    # Inner join the corn dataset to usa_census_regions by state
    corn %>%
        inner_join(usa_census_regions, by = "state")

    ----------------------------------------------------------------------------------
    # Wrap this code into a function
    fortify_with_census_region <- function(data) {
    data %>%
        inner_join(usa_census_regions, by = "state")
    }

    # Try it on the wheat dataset
    fortify_with_census_region(wheat)

Plotting yields over time by region
So far, you have used a function to plot yields over time for each crop, and you've added a census_region column to the crop datasets. 
Now you are ready to look at how the yields change over time in each region of the USA.

ggplot2 is loaded. corn and wheat have been fortified with census regions. plot_yield_vs_year() is available.

    Example:
    # Plot yield vs. year for the corn dataset
    plot_yield_vs_year(corn) +
        # Facet, wrapped by census region
        facet_wrap(vars(census_region))

    ----------------------------------------------------------------------------------------
    # Wrap this code into a function
    plot_yield_vs_year_by_region <- function(data) {
    plot_yield_vs_year(data) +
        facet_wrap(vars(census_region))
    }

    # Try it on the wheat dataset
    plot_yield_vs_year_by_region(wheat)

#######################################################################################################################################
Modeling Grain Yields:

Running a model
The smooth trend line you saw in the plots of yield over time use a generalized additive model (GAM) to determine where the line should 
lie. This sort of model is ideal for fitting nonlinear curves. So we can make predictions about future yields, let's explicitly run the 
model. The syntax for running this GAM takes the following form.

gam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset)
Here, s() means "make the variable smooth", where smooth very roughly means nonlinear.

mgcv and dplyr are loaded; the corn and wheat datasets are available.

    Example:
    # Run a generalized additive model of yield vs. smoothed year and census region
    gam(yield_kg_per_ha ~ s(year) + census_region, data = corn)

    ---------------------------------------------------------------------------------------------
    # Wrap the model code into a function
    run_gam_yield_vs_year_by_region <- function(data) {
        gam(yield_kg_per_ha ~ s(year) + census_region, data = data)
    }

    # Try it on the wheat dataset
    run_gam_yield_vs_year_by_region(wheat)

Making yield predictions
The fun part of modeling is using the models to make predictions. You can do this using a call to predict(), in the following form.

predict(model, cases_to_predict, type = "response")
mgcv and dplyr are loaded; GAMs of the corn and wheat datasets are available as corn_model and wheat_model. A character vector of census 
regions is stored as census_regions.

    Example:
    # Make predictions in 2050  
    predict_this <- data.frame(
        year = 2050,
        census_region = census_regions
    ) 

    # Predict the yield
    pred_yield_kg_per_ha <- predict(corn_model, predict_this, type = "response")

    predict_this %>%
        # Add the prediction as a column of predict_this 
        mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)

    --------------------------------------------------------------------------------------------------
    # Wrap this prediction code into a function
    predict_yields <- function(model, year) {
    predict_this <- data.frame(
        year = year,
        census_region = census_regions
    ) 
    pred_yield_kg_per_ha <- predict(model, predict_this, type = "response")
    predict_this %>%
        mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)
    }

    # Try it on the wheat dataset
    predict_yields(wheat_model, year = 2050)

Do it all over again
Hopefully, by now, you've realized that the real benefit to writing functions is that you can reuse your code easily. Now you are going 
to rerun the whole analysis from this chapter on a new crop, barley. Since all the infrastructure is in place, that's less effort than 
it sounds!

Barley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana.

dplyr and ggplot2, and mgcv are loaded; fortify_with_metric_units(), fortify_with_census_region(), plot_yield_vs_year_by_region(), 
run_gam_yield_vs_year_by_region(), and predict_yields() are available.

    Example:
    fortified_barley <- barley %>% 
        # Fortify with metric units
        fortify_with_metric_units() %>%
        # Fortify with census regions
        fortify_with_census_region()

    # See the result
    glimpse(fortified_barley)

    -------------------------------------------------------------------------------------------
    # From previous step
    fortified_barley <- barley %>% 
        fortify_with_metric_units() %>%
        fortify_with_census_region()

    # Plot yield vs. year by region
    plot_yield_vs_year_by_region(fortified_barley)

    -------------------------------------------------------------------------------------------
    # From previous step
    fortified_barley <- barley %>% 
        fortify_with_metric_units() %>%
        fortify_with_census_region()

    fortified_barley %>% 
        # Run a GAM of yield vs. year by region
        run_gam_yield_vs_year_by_region()  %>% 
        # Make predictions of yields in 2050
        predict_yields(year=2050)
























#########################################################################################################
# Comparison Operators

<       Strictly less than
<=      Less than or equal
>       Strictly Greater than
>=      Greater than or equal
==      equal
!=      Not equal

#########################################################################################################
# Boolean Operators
 
or 
not

The and Operator

True and True = True
False and True = False
True and False = False
False and False = False

example: 
    x = 12 
    x > 5 and x < 15
    # True    # True

The or Operator

True or True = True
False or True = True
True or False = True
False or False = False

example: 
    y = 5
    y < 7 or y > 13


The not Operator

not True = False
not False = True

NumPy uses: 
    logical_and()
    logical_or()
    logical_not()

example:
    np.logical_and(bmi > 21, bmi < 22) # to get just the boolean answers
    bmi[np.logical_and(bmi > 21, bmi < 22)] # to get the actual bmi answers

################################################################################################
if, elif, and else: 

if statement

if condition :
    expression

    example: 
        z = 4
        if z % 2 == 0 :           # % 2 will return 0 if z is even 
            print("z is even")

If / else statement

if condition : 
    expression
    else :
    expression

    example:
        z = 5 
        if z% 2 == 0 :
            print("z is even)
        else :
            print("z is odd)

The elif statement

if condition:
    expression
elif condition:
    expression
elif condition:
    expression

    example:
        z = 3 
        if z % 2 == 0 :
            print("z is divisible by 2")
        elif z % 3 == 0 :
            print("z is divisible by 3")
        else :
            Print("z is neither divisible by 2 nor by 3")

####################################################################################################
Filtering Pandas DataFrames:

Example: 
        import pandas as pd
        brics = pd.read_csb("path/to/prics.csv, index_col = 0)

    # You want to keep the countries that have an area of greater than 8 million km
    # You want a pandas series not a pandas dataframe
    
    # Step 1: Get the column
        brics["area"]
    
    # Step 2: Compare
    # Gives you a series containing booleans stored in variable is_huge
        is_huge = brics["area"] > 8

    # Step 3: Subset the DataFrame
        brics[is_huge]

Using boolean Operators

Example:
    # Only want to display countries with greater than 8 but less than 10 million km
        import numpy as np

    # Step 1: build the code for the series
        np.logical_and(brics["area"] > 8, brics["area"] < 10)

    # Step 2: Wrap in brics brackets []
         brics[np.logical_and(brics["area"] > 8, brics["area"] < 10)]

#########################################################################################################
While Loop:

    while condition :
        expression

    example:
        error = 50.0

        while error > 1:
            error = error / 4
            print(error)
    
###########################################################################################################
For loop:

    # For each variable in the sequence, execute the expression
    for var in seq :
        expression

    example:
        fam = [1.73, 1.68, 1.71, 1.89]
        for height in fam :
            print(height)

    example: printing the index number for each
        fam = [1.73, 1.68, 1.71, 1.89]
        for index, height in enumerate(fam) :
            print("index " + str(index) + ": " + str(height))

    example: interating a string and captilizing the prints
        for c in "family" :
            print(c.capitalize())

#############################################################################################################
Loop Data Structures part 1: 

Dictionaries: 

    example:
        for key, value in world.items() :
            print(key + " -- " + str(value))


NumPy Arrays:

    example: 1D array
        np_height = np.array([1.73, 1.68.1.71, 1.89, 1.79])
        np_weight = np.array([1.73, 1.68.1.71, 1.89, 1.79])
        bmi = np_weight / np_height ** 2

        for val in bmi :
            print (val)

    example: 2D array
        np_height = np.array([1.73, 1.68.1.71, 1.89, 1.79])
        np_weight = np.array([1.73, 1.68.1.71, 1.89, 1.79])
        meas = np.array([np_height, np_weight])

        for val in np.nditer(meas) :
            print(val)

#################################################################################################################
Loop Data Structures part 2:

Pandas Data Structures

    example: Prints Labels then rows
    import pandas as pd
    brics = pd.read_csv("brics.csv", index_col = 0)

    for lab, row in brics.iterrows(): 
        print(lab)
        print(row)

    example: Selective print
    for lab, row in brics.iterrows(): 
        print(lab + ": " + row["capital"])

    example: Add column with for loop
    for lab, row in brics.iterrows() :
        #Creating series on every iteration
        brics.loc[lab, "name_length"] = len(row["country"])

    example: Add column without for loop
    brics["name_length"] = brics["country"].apply(len)
    print(brics)

##################################################################################################################
Random Numbers: 

example: 
    import numpy as np
    np.random.rand()    # Pseudo-random number generator

example: coin toss
    np.random.seed(123)    # seed can be anything
    coin = np.random.randint(0, 2)  # randomly generate 0 or 1

    if coin == 0 :
        print("heads")
    else : 
        print("tails")

##################################################################################################################
Random Walk:

# Keeps track of the history of specific random events

example: Heads or tails (not a random walk)
    import numpy as np 
    np.random.seed(123)
    outcomes = []
    for x in range(10) :          # this will keep track of 10 events
        coin = np.random.randint(0,2) 
        if coin == 0 :
            outcomes.append("heads")
        else : 
            outcomes.append("tails")
    print(outcomes)

example: Heads or tails (Random Walk)
    import numpy as np
    np.random.seed(123)
    tails = [0]
    for x in range(10) :
        coin = np.random.randint(0,2)
        tails.append(tails[x] + coin)
    print(tails)

#####################################################################################################################
Distribution:

example: Heads or tails (Random Walk)
    import numpy as np
    np.random.seed(123)
    tails = [0]
    for x in range(10) :
        coin = np.random.randint(0,2)
        tails.append(tails[x] + coin)
    print(tails)

example: Distribution of 100 flips
    import numpy as np
    import matplotlib.pyplot as plt
    np.random.seed(123)
    final_tails = []
    for x in range(100)
        tails = [0]
        for x in range(10) :
            coin = np.random.randint(0,2)
            tails.append(tails[x] + coin)
        final_tails.append(tails[-1])
    plt.hist(final_tails, bins = 10)
    plt.show()

########################################################################################################################################
User-Defined functions:

Parameters
    # No Arguments
    def fun1():
        expression

    # One Argument
    def fun2(arg1):
        expression

    # Multiple Arguments
    def fun3 (arg1, arg2)
        expression

    Examples: 
    # Define the function shout
    def shout():
        """Print a string with three exclamation marks"""
        # Concatenate the strings: shout_word
        shout_word = 'congratulations' + '!!!'

        # Print shout_word
        print(shout_word)

    # Call shout
    shout()

    ----------------------------------------------------------------------------------
    # Define shout with the parameter, word
    def shout(word):
        """Print a string with three exclamation marks"""
        # Concatenate the strings: shout_word
        shout_word = word + '!!!'

        # Print shout_word
        print(shout_word)

    # Call shout with the string 'congratulations'
    shout('congratulations')

    ----------------------------------------------------------------------------------
    # Define shout with the parameter, word
    def shout(word):
        """Return a string with three exclamation marks"""
        # Concatenate the strings: shout_word
        shout_word = word + '!!!'

        # Replace print with return
        return(shout_word)

    # Pass 'congratulations' to shout: yell
    yell = shout('congratulations')

    # Print yell
    print(yell)

#####################################################################################################################################
Multiple Parameters and Return Values:

    Examples: Multiple parameters
    # Define shout with parameters word1 and word2
    def shout(word1, word2):
        """Concatenate strings with three exclamation marks"""
        # Concatenate word1 with '!!!': shout1
        shout1 = word1 + '!!!'
        
        # Concatenate word2 with '!!!': shout2
        shout2 = word2 + '!!!'
        
        # Concatenate shout1 with shout2: new_shout
        new_shout = shout1 + shout2

        # Return new_shout
        return new_shout

    # Pass 'congratulations' and 'you' to shout(): yell
    yell = shout('congratulations', 'you')

    # Print yell
    print(yell)

    -------------------------------------------------------------------------------
    Example: Tuples
    # Unpack nums into num1, num2, and num3
    num1 = nums[0]
    num2 = nums[1]
    num3 = nums[2]

    # Construct even_nums
    even_nums = (2, num2, num3)

    ---------------------------------------------------------------------------------
    Example: Returning multiple values
    # Define shout_all with parameters word1 and word2
    def shout_all(word1, word2):
        
        # Concatenate word1 with '!!!': shout1
        shout1 = word1 + '!!!'
        
        # Concatenate word2 with '!!!': shout2
        shout2 = word2 + '!!!'
        
        # Construct a tuple with shout1 and shout2: shout_words
        shout_words = (shout1, shout2)

        # Return shout_words
        return shout_words

    # Pass 'congratulations' and 'you' to shout_all(): yell1, yell2
    yell1, yell2 = shout_all('congratulations', 'you')

    # Print yell1 and yell2
    print(yell1)
    print(yell2)

#######################################################################################################################################
Bringing it all together:

    Examples:
    Bringing it all together (1)
    You've got your first taste of writing your own functions in the previous exercises. You've learned how to add parameters to your own function definitions, return a value or multiple values with tuples, and how to call the functions you've defined.

    In this and the following exercise, you will bring together all these concepts and apply them to a simple data science problem. You will load a dataset and develop functionalities to extract simple insights from the data.

    For this exercise, your goal is to recall how to load a dataset into a DataFrame. The dataset contains Twitter data and you will iterate over entries in a column to build a dictionary in which the keys are the names of languages and the values are the number of tweets in the given language. The file tweets.csv is available in your current directory.

    Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).

    Instructions
    
    Import the pandas package with the alias pd.
    Import the file 'tweets.csv' using the pandas function read_csv(). Assign the resulting DataFrame to df.
    Complete the for loop by iterating over col, the 'lang' column in the DataFrame df.
    Complete the bodies of the if-else statements in the for loop: if the key is in the dictionary langs_count, add 1 to the value corresponding to this key in the dictionary, else add the key to langs_count and set the corresponding value to 1. Use the loop variable entry in your code.

    # Import pandas
    import pandas as pd

    # Import Twitter data as DataFrame: df
    df = pd.read_csv('tweets.csv')

    # Initialize an empty dictionary: langs_count
    langs_count = {}

    # Extract column from DataFrame: col
    col = df['lang']

    # Iterate over lang column in DataFrame
    for entry in col:

        # If the language is in langs_count, add 1 
        if entry in langs_count.keys():
            langs_count[entry] += 1
        # Else add the language to langs_count, set the value to 1
        else:
            langs_count[entry] = 1

    # Print the populated dictionary
    print(langs_count)

    Bringing it all together (2)
    Great job! You've now defined the functionality for iterating over entries in a column and building a dictionary with keys the names of languages and values the number of tweets in the given language.

    In this exercise, you will define a function with the functionality you developed in the previous exercise, return the resulting dictionary from within the function, and call the function with the appropriate arguments.

    For your convenience, the pandas package has been imported as pd and the 'tweets.csv' file has been imported into the tweets_df variable.

    Instructions
    
    Define the function count_entries(), which has two parameters. The first parameter is df for the DataFrame and the second is col_name for the column name.
    Complete the bodies of the if-else statements in the for loop: if the key is in the dictionary langs_count, add 1 to its current value, else add the key to langs_count and set its value to 1. Use the loop variable entry in your code.
    Return the langs_count dictionary from inside the count_entries() function.
    Call the count_entries() function by passing to it tweets_df and the name of the column, 'lang'. Assign the result of the call to the variable result.

    # Define count_entries()
    def count_entries(df, col_name):
        """Return a dictionary with counts of 
        occurrences as value for each key."""

        # Initialize an empty dictionary: langs_count
        langs_count = {}
        
        # Extract column from DataFrame: col
        col = df[col_name]
        
        # Iterate over lang column in DataFrame
        for entry in col:

            # If the language is in langs_count, add 1
            if entry in langs_count.keys():
                langs_count[entry] += 1
            # Else add the language to langs_count, set the value to 1
            else:
                langs_count[entry] = 1

        # Return the langs_count dictionary
        return langs_count

    # Call count_entries(): result
    result = count_entries(tweets_df, 'lang')

    # Print the result
    print(result)

########################################################################################################################################
Scope and User-defined Functions:

You need to use the keyword global in order to access variables outside of the function you are working in.

    Example: 
    The keyword global
    Let's work more on your mastery of scope. In this exercise, you will use the keyword global within a function to alter the value of a variable defined in the global scope.

    Instructions
    
    Use the keyword global to alter the object team in the global scope.
    Change the value of team in the global scope to the string "justice league". Assign the result to team.
    Hit the Submit button to see how executing your newly defined function change_team() changes the value of the name team!

    # Create a string: team
    team = "teen titans"

    # Define change_team()
    def change_team():
        """Change the value of the global variable team."""

        # Use team in global scope
        global team

        # Change the value of team in global: team
        team = "justice league"

    # Print team
    print(team)

    # Call change_team()
    change_team()

    # Print team
    print(team)

#####################################################################################################################################
Nested Functions:

    Example: Nested Functions 1
    Nested Functions I
    You've learned in the last video about nesting functions within functions. One reason why you'd like to do this is to avoid writing out the same computations within functions repeatedly. There's nothing new about defining nested functions: you simply define it as you would a regular function with def and embed it inside another function!

    In this exercise, inside a function three_shouts(), you will define a nested function inner() that concatenates a string object with !!!. three_shouts() then returns a tuple of three elements, each a string concatenated with !!! using inner(). Go for it!

    Instructions
    
    Complete the function header of the nested function with the function name inner() and a single parameter word.
    Complete the return value: each element of the tuple should be a call to inner(), passing in the parameters from three_shouts() as arguments to each call.

    # Define three_shouts
    def three_shouts(word1, word2, word3):
        """Returns a tuple of strings
        concatenated with '!!!'."""

        # Define inner
        def inner(word):
            """Returns a string concatenated with '!!!'."""
            return word + '!!!'

        # Return a tuple of strings
        return (inner(word1), inner(word2), inner(word3))

    # Call three_shouts() and print
    print(three_shouts('a', 'b', 'c'))

    -----------------------------------------------------------------------------------------
    Example: Nested functions 2
    Nested Functions II
    Great job, you've just nested a function within another function. One other pretty cool reason for nesting functions is the idea of a closure. This means that the nested or inner function remembers the state of its enclosing scope when called. Thus, anything defined locally in the enclosing scope is available to the inner function even when the outer function has finished execution.

    Let's move forward then! In this exercise, you will complete the definition of the inner function inner_echo() and then call echo() a couple of times, each with a different argument. Complete the exercise and see what the output will be!

    Instructions
    
    Complete the function header of the inner function with the function name inner_echo() and a single parameter word1.
    Complete the function echo() so that it returns inner_echo.
    We have called echo(), passing 2 as an argument, and assigned the resulting function to twice. Your job is to call echo(), passing 3 as an argument. Assign the resulting function to thrice.
    Hit Submit to call twice() and thrice() and print the results.

    # Define echo
    def echo(n):
        """Return the inner_echo function."""

        # Define inner_echo
        def inner_echo(word1):
            """Concatenate n copies of word1."""
            echo_word = word1 * n
            return echo_word

        # Return inner_echo
        return inner_echo

    # Call echo: twice
    twice = echo(2)

    # Call echo: thrice
    thrice = echo(3)

    # Call twice() and thrice() then print
    print(twice('hello'), thrice('hello'))

    Example: Keword non local and nested functions
    Let's once again work further on your mastery of scope! In this exercise, you will use the keyword nonlocal within a nested function to alter the value of a variable defined in the enclosing scope.

    Instructions
    
    Assign to echo_word the string word, concatenated with itself.
    Use the keyword nonlocal to alter the value of echo_word in the enclosing scope.
    Alter echo_word to echo_word concatenated with '!!!'.
    Call the function echo_shout(), passing it a single argument 'hello'.

    # Define echo_shout()
    def echo_shout(word):
        """Change the value of a nonlocal variable"""
        
        # Concatenate word with itself: echo_word
        echo_word = word*2
        
        # Print echo_word
        print(echo_word)
        
        # Define inner function shout()
        def shout():
            """Alter a variable in the enclosing scope"""    
            # Use echo_word in nonlocal scope
            nonlocal echo_word
            
            # Change echo_word to echo_word concatenated with '!!!'
            echo_word = echo_word + '!!!'
        
        # Call function shout()
        shout()
        
        # Print echo_word
        print(echo_word)

    # Call function echo_shout() with argument 'hello'
    echo_shout('hello')

#######################################################################################################################################
Default and flexible arguments: 

flexible arguments = *args #creates a tuple called args
flexible arguments = **kwargs # arguments preceeeded by identifiers this turns the arguments given into a dictionary called kwargs

    Example:
    Functions with one default argument
    In the previous chapter, you've learned to define functions with more than one parameter and then calling those functions by passing the required number of arguments. In the last video, Hugo built on this idea by showing you how to define functions with default arguments. You will practice that skill in this exercise by writing a function that uses a default argument and then calling the function a couple of times.

    Instructions
    
    Complete the function header with the function name shout_echo. It accepts an argument word1 and a default argument echo with default value 1, in that order.
    Use the * operator to concatenate echo copies of word1. Assign the result to echo_word.
    Call shout_echo() with just the string, "Hey". Assign the result to no_echo.
    Call shout_echo() with the string "Hey" and the value 5 for the default argument, echo. Assign the result to with_echo.

    # Define shout_echo
    def shout_echo(word1, echo=1):
        """Concatenate echo copies of word1 and three
        exclamation marks at the end of the string."""

        # Concatenate echo copies of word1 using *: echo_word
        echo_word = word1 * echo

        # Concatenate '!!!' to echo_word: shout_word
        shout_word = echo_word + '!!!'

        # Return shout_word
        return shout_word

    # Call shout_echo() with "Hey": no_echo
    no_echo = shout_echo("Hey")

    # Call shout_echo() with "Hey" and echo=5: with_echo
    with_echo = shout_echo("Hey", echo=5)

    # Print no_echo and with_echo
    print(no_echo)
    print(with_echo)

    Example:
    Functions with multiple default arguments
    You've now defined a function that uses a default argument - don't stop there just yet! You will now try your hand at defining a function with more than one default argument and then calling this function in various ways.

    After defining the function, you will call it by supplying values to all the default arguments of the function. Additionally, you will call the function by not passing a value to one of the default arguments - see how that changes the output of your function!

    Instructions
    100 XP
    Complete the function header with the function name shout_echo. It accepts an argument word1, a default argument echo with default value 1 and a default argument intense with default value False, in that order.
    In the body of the if statement, make the string object echo_word upper case by applying the method .upper() on it.
    Call shout_echo() with the string, "Hey", the value 5 for echo and the value True for intense. Assign the result to with_big_echo.
    Call shout_echo() with the string "Hey" and the value True for intense. Assign the result to big_no_echo.

    # Define shout_echo
    def shout_echo(word1, echo = 1, intense = False):
        """Concatenate echo copies of word1 and three
        exclamation marks at the end of the string."""

        # Concatenate echo copies of word1 using *: echo_word
        echo_word = word1 * echo

        # Make echo_word uppercase if intense is True
        if intense is True:
            # Make uppercase and concatenate '!!!': echo_word_new
            echo_word_new = echo_word.upper() + '!!!'
        else:
            # Concatenate '!!!' to echo_word: echo_word_new
            echo_word_new = echo_word + '!!!'

        # Return echo_word_new
        return echo_word_new

    # Call shout_echo() with "Hey", echo=5 and intense=True: with_big_echo
    with_big_echo = shout_echo("Hey", echo=5, intense=True)

    # Call shout_echo() with "Hey" and intense=True: big_no_echo
    big_no_echo = shout_echo("Hey", intense=True)

    # Print values
    print(with_big_echo)
    print(big_no_echo)

    Example:
    Functions with variable-length arguments (*args)
    Flexible arguments enable you to pass a variable number of arguments to a function. In this exercise, you will practice defining a function that accepts a variable number of string arguments.

    The function you will define is gibberish() which can accept a variable number of string values. Its return value is a single string composed of all the string arguments concatenated together in the order they were passed to the function call. You will call the function with a single string argument and see how the output changes with another call using more than one string argument. Recall from the previous video that, within the function definition, args is a tuple.

    Instructions

    Complete the function header with the function name gibberish. It accepts a single flexible argument *args.
    Initialize a variable hodgepodge to an empty string.
    Return the variable hodgepodge at the end of the function body.
    Call gibberish() with the single string, "luke". Assign the result to one_word.
    Hit the Submit button to call gibberish() with multiple arguments and to print the value to the Shell.

    # Define gibberish
    def gibberish(*args):
        """Concatenate strings in *args together."""

        # Initialize an empty string: hodgepodge
        hodgepodge = ""

        # Concatenate the strings in args
        for word in args:
            hodgepodge += word

        # Return hodgepodge
        return hodgepodge

    # Call gibberish() with one string: one_word
    one_word = gibberish("luke")

    # Call gibberish() with five strings: many_words
    many_words = gibberish("luke", "leia", "han", "obi", "darth")

    # Print one_word and many_words
    print(one_word)
    print(many_words)

    Example:
    Functions with variable-length keyword arguments (**kwargs)
    Let's push further on what you've learned about flexible arguments - you've used *args, you're now going to use **kwargs! What makes **kwargs different is that it allows you to pass a variable number of keyword arguments to functions. Recall from the previous video that, within the function definition, kwargs is a dictionary.

    To understand this idea better, you're going to use **kwargs in this exercise to define a function that accepts a variable number of keyword arguments. The function simulates a simple status report system that prints out the status of a character in a movie.

    Instructions
    
    Complete the function header with the function name report_status. It accepts a single flexible argument **kwargs.
    Iterate over the key-value pairs of kwargs to print out the keys and values, separated by a colon ':'.
    In the first call to report_status(), pass the following keyword-value pairs: name="luke", affiliation="jedi" and status="missing".
    In the second call to report_status(), pass the following keyword-value pairs: name="anakin", affiliation="sith lord" and status="deceased".

    Example:
    # Define report_status
    def report_status(**kwargs):
        """Print out the status of a movie character."""

        print("\nBEGIN: REPORT\n")

        # Iterate over the key-value pairs of kwargs
        for key, value in kwargs.items():
            # Print out the keys and values, separated by a colon ':'
            print(key + ": " + value)

        print("\nEND REPORT")

    # First call to report_status()
    report_status(name="luke", affiliation="jedi", status="missing")

    # Second call to report_status()
    report_status(name="anakin", affiliation="sith lord", status="deceased")

#########################################################################################################################################
Bringing it all together:

    Example:
    Bringing it all together (1)
    Recall the Bringing it all together exercise in the previous chapter where you did a simple Twitter analysis by developing a function that counts how many tweets are in certain languages. The output of your function was a dictionary that had the language as the keys and the counts of tweets in that language as the value.

    In this exercise, we will generalize the Twitter language analysis that you did in the previous chapter. You will do that by including a default argument that takes a column name.

    For your convenience, pandas has been imported as pd and the 'tweets.csv' file has been imported into the DataFrame tweets_df. Parts of the code from your previous work are also provided.

    Instructions
    100 XP
    Complete the function header by supplying the parameter for a DataFrame df and the parameter col_name with a default value of 'lang' for the DataFrame column name.
    Call count_entries() by passing the tweets_df DataFrame and the column name 'lang'. Assign the result to result1. Note that since 'lang' is the default value of the col_name parameter, you don't have to specify it here.
    Call count_entries() by passing the tweets_df DataFrame and the column name 'source'. Assign the result to result2.

    # Define count_entries()
    def count_entries(df, col_name = 'lang'):
        """Return a dictionary with counts of
        occurrences as value for each key."""

        # Initialize an empty dictionary: cols_count
        cols_count = {}

        # Extract column from DataFrame: col
        col = df[col_name]
        
        # Iterate over the column in DataFrame
        for entry in col:

            # If entry is in cols_count, add 1
            if entry in cols_count.keys():
                cols_count[entry] += 1

            # Else add the entry to cols_count, set the value to 1
            else:
                cols_count[entry] = 1

        # Return the cols_count dictionary
        return cols_count

    # Call count_entries(): result1
    result1 = count_entries(tweets_df)

    # Call count_entries(): result2
    result2 = count_entries(tweets_df, col_name = 'source')

    # Print result1 and result2
    print(result1)
    print(result2)

    Example:
    Bringing it all together (2)
    Wow, you've just generalized your Twitter language analysis that you did in the previous chapter to include a default argument for the column name. You're now going to generalize this function one step further by allowing the user to pass it a flexible argument, that is, in this case, as many column names as the user would like!

    Once again, for your convenience, pandas has been imported as pd and the 'tweets.csv' file has been imported into the DataFrame tweets_df. Parts of the code from your previous work are also provided.

    Instructions

    Complete the function header by supplying the parameter for the DataFrame df and the flexible argument *args.
    Complete the for loop within the function definition so that the loop occurs over the tuple args.
    Call count_entries() by passing the tweets_df DataFrame and the column name 'lang'. Assign the result to result1.
    Call count_entries() by passing the tweets_df DataFrame and the column names 'lang' and 'source'. Assign the result to result2.

    # Define count_entries()
    def count_entries(df, *args):
        """Return a dictionary with counts of
        occurrences as value for each key."""
        
        #Initialize an empty dictionary: cols_count
        cols_count = {}
        
        # Iterate over column names in args
        for col_name in args:
        
            # Extract column from DataFrame: col
            col = df[col_name]
        
            # Iterate over the column in DataFrame
            for entry in col:
        
                # If entry is in cols_count, add 1
                if entry in cols_count.keys():
                    cols_count[entry] += 1
        
                # Else add the entry to cols_count, set the value to 1
                else:
                    cols_count[entry] = 1

        # Return the cols_count dictionary
        return cols_count

    # Call count_entries(): result1
    result1 = count_entries(tweets_df, 'lang')

    # Call count_entries(): result2
    result2 = count_entries(tweets_df, 'lang', 'source')

    # Print result1 and result2
    print(result1)
    print(result2)

#####################################################################################################################################
Lambda Functions:

Syntax = raise_to_power = lambda x, y: x ** y

    Example:
    Writing a lambda function you already know
    Some function definitions are simple enough that they can be converted to a lambda function. By doing this, you write less lines of code, which is pretty awesome and will come in handy, especially when you're writing and maintaining big programs. In this exercise, you will use what you know about lambda functions to convert a function that does a simple task into a lambda function. Take a look at this function definition:

    def echo_word(word1, echo):
        """Concatenate echo copies of word1."""
        words = word1 * echo
        return words
    The function echo_word takes 2 parameters: a string value, word1 and an integer value, echo. It returns a string that is a concatenation of echo copies of word1. Your task is to convert this simple function into a lambda function.

    Instructions

    Define the lambda function echo_word using the variables word1 and echo. Replicate what the original function definition for echo_word() does above.
    Call echo_word() with the string argument 'hey' and the value 5, in that order. Assign the call to result.

    # Define echo_word as a lambda function: echo_word
    echo_word = (lambda word1, echo: word1 * echo)

    # Call echo_word: result
    result = echo_word('hey', 5)

    # Print result
    print(result)

    Example:
    Map() and lambda functions
    So far, you've used lambda functions to write short, simple functions as well as to redefine functions with simple functionality. The best use case for lambda functions, however, are for when you want these simple functionalities to be anonymously embedded within larger expressions. What that means is that the functionality is not stored in the environment, unlike a function defined with def. To understand this idea better, you will use a lambda function in the context of the map() function.

    Recall from the video that map() applies a function over an object, such as a list. Here, you can use lambda functions to define the function that map() will use to process the object. For example:

    nums = [2, 4, 6, 8, 10]

    result = map(lambda a: a ** 2, nums)
    You can see here that a lambda function, which raises a value a to the power of 2, is passed to map() alongside a list of numbers, nums. The map object that results from the call to map() is stored in result. You will now practice the use of lambda functions with map(). For this exercise, you will map the functionality of the add_bangs() function you defined in previous exercises over a list of strings.

    Instructions
    
    In the map() call, pass a lambda function that concatenates the string '!!!' to a string item; also pass the list of strings, spells. Assign the resulting map object to shout_spells.
    Convert shout_spells to a list and print out the list.

    # Create a list of strings: spells
    spells = ["protego", "accio", "expecto patronum", "legilimens"]

    # Use map() to apply a lambda function over spells: shout_spells
    shout_spells = map(lambda spell: spell + '!!!', spells)

    # Convert shout_spells to a list: shout_spells_list
    shout_spells_list = list(shout_spells)

    # Print the result
    print(shout_spells_list)

    Example:
    Filter() and lambda functions
    In the previous exercise, you used lambda functions to anonymously embed an operation within map(). You will practice this again in this exercise by using a lambda function with filter(), which may be new to you! The function filter() offers a way to filter out elements from a list that don't satisfy certain criteria.

    Your goal in this exercise is to use filter() to create, from an input list of strings, a new list that contains only strings that have more than 6 characters.

    Instructions
    100 XP
    In the filter() call, pass a lambda function and the list of strings, fellowship. The lambda function should check if the number of characters in a string member is greater than 6; use the len() function to do this. Assign the resulting filter object to result.
    Convert result to a list and print out the list.

    # Create a list of strings: fellowship
    fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']

    # Use filter() to apply a lambda function over fellowship: result
    result = filter(lambda name: len(name) > 6, fellowship)

    # Convert result to a list: result_list
    result_list = list(result)

    # Print result_list
    print(result_list)

    Example:
    Reduce() and lambda functions
    You're getting very good at using lambda functions! Here's one more function to add to your repertoire of skills. The reduce() function is useful for performing some computation on a list and, unlike map() and filter(), returns a single value as a result. To use reduce(), you must import it from the functools module.

    Remember gibberish() from a few exercises back?

    # Define gibberish
    def gibberish(*args):
        """Concatenate strings in *args together."""
        hodgepodge = ''
        for word in args:
            hodgepodge += word
        return hodgepodge
    gibberish() simply takes a list of strings as an argument and returns, as a single-value result, the concatenation of all of these strings. In this exercise, you will replicate this functionality by using reduce() and a lambda function that concatenates strings together.

    Instructions
    
    Import the reduce function from the functools module.
    In the reduce() call, pass a lambda function that takes two string arguments item1 and item2 and concatenates them; also pass the list of strings, stark. Assign the result to result. The first argument to reduce() should be the lambda function and the second argument is the list stark.

    # Import reduce from functools
    from functools import reduce

    # Create a list of strings: stark
    stark = ['robb', 'sansa', 'arya', 'brandon', 'rickon']

    # Use reduce() to apply a lambda function over stark: result
    result = reduce(lambda item1, item2: item1 + item2, stark)

    # Print the result
    print(result)

#####################################################################################################################################
Introduction to error handling:

    Example: 
    Error handling with try-except
    A good practice in writing your own functions is also anticipating the ways in which other people (or yourself, if you accidentally 
    misuse your own function) might use the function you defined.

    As in the previous exercise, you saw that the len() function is able to handle input arguments such as strings, lists, and tuples, 
    but not int type ones and raises an appropriate error and error message when it encounters invalid input arguments. One way of 
    doing this is through exception handling with the try-except block.

    In this exercise, you will define a function as well as use a try-except block for handling cases when incorrect input arguments 
    are passed to the function.

    Recall the shout_echo() function you defined in previous exercises; parts of the function definition are provided in the sample 
    code. Your goal is to complete the exception handling code in the function definition and provide an appropriate error message 
    when raising an error.

    Instructions
    
    Initialize the variables echo_word and shout_words to empty strings.
    Add the keywords try and except in the appropriate locations for the exception handling block.
    Use the * operator to concatenate echo copies of word1. Assign the result to echo_word.
    Concatenate the string '!!!' to echo_word. Assign the result to shout_words.

    # Define shout_echo
    def shout_echo(word1, echo=1):
        """Concatenate echo copies of word1 and three
        exclamation marks at the end of the string."""

        # Initialize empty strings: echo_word, shout_words
        echo_word = ""
        shout_words = ""
        

        # Add exception handling with try-except
        try:
            # Concatenate echo copies of word1 using *: echo_word
            echo_word = word1 * echo

            # Concatenate '!!!' to echo_word: shout_words
            shout_words = echo_word + '!!!'
        except:
            # Print error message
            print("word1 must be a string and echo must be an integer.")

        # Return shout_words
        return shout_words

    # Call shout_echo
    shout_echo("particle", echo="accelerator")

    ---------------------------------------------------------------------------------------------
    Error handling by raising an error
    Another way to raise an error is by using raise. In this exercise, you will add a raise statement to the shout_echo() function you 
    defined before to raise an error message when the value supplied by the user to the echo argument is less than 0.

    The call to shout_echo() uses valid argument values. To test and see how the raise statement works, simply change the value for the 
    echo argument to a negative value. Don't forget to change it back to valid values to move on to the next exercise!

    Instructions
    
    Complete the if statement by checking if the value of echo is less than 0.
    In the body of the if statement, add a raise statement that raises a ValueError with message 'echo must be greater than or equal to 
    0' when the value supplied by the user to echo is less than 0.

    # Define shout_echo
    def shout_echo(word1, echo=1):
        """Concatenate echo copies of word1 and three
        exclamation marks at the end of the string."""

        # Raise an error with raise
        if echo < 0:
            raise ValueError('echo must be greater than or equal to 0')

        # Concatenate echo copies of word1 using *: echo_word
        echo_word = word1 * echo

        # Concatenate '!!!' to echo_word: shout_word
        shout_word = echo_word + '!!!'

        # Return shout_word
        return shout_word

    # Call shout_echo
    shout_echo("particle", echo=5)

######################################################################################################################################
Bringing it all together:

    Example: 
    Bringing it all together (1)
    This is awesome! You have now learned how to write anonymous functions using lambda, how to pass lambda functions as arguments to 
    other functions such as map(), filter(), and reduce(), as well as how to write errors and output custom error messages within your 
    functions. You will now put together these learnings to good use by working with a Twitter dataset. Before practicing your new 
    error handling skills; in this exercise, you will write a lambda function and use filter() to select retweets, that is, tweets that
     begin with the string 'RT'.

    To help you accomplish this, the Twitter data has been imported into the DataFrame, tweets_df. Go for it!

    Instructions
    100 XP
    In the filter() call, pass a lambda function and the sequence of tweets as strings, tweets_df['text']. The lambda function should 
    check if the first 2 characters in a tweet x are 'RT'. Assign the resulting filter object to result. To get the first 2 characters 
    in a tweet x, use x[0:2]. To check equality, use a Boolean filter with ==.
    Convert result to a list and print out the list.

    # Select retweets from the Twitter DataFrame: result
    result = filter(lambda x: x[0:2] == 'RT', tweets_df['text'])

    # Create list from filter object result: res_list
    res_list = list(result)

    # Print all retweets in res_list
    for tweet in res_list:
        print(tweet)

    -----------------------------------------------------------------------------------------------------
    Example:
    Bringing it all together (2)
    Sometimes, we make mistakes when calling functions - even ones you made yourself. But don't fret! In this exercise, you will 
    improve on your previous work with the count_entries() function in the last chapter by adding a try-except block to it. This will 
    allow your function to provide a helpful message when the user calls your count_entries() function but provides a column name that 
    isn't in the DataFrame.

    Once again, for your convenience, pandas has been imported as pd and the 'tweets.csv' file has been imported into the DataFrame 
    tweets_df. Parts of the code from your previous work are also provided.

    Instructions
    
    Add a try block so that when the function is called with the correct arguments, it processes the DataFrame and returns a dictionary 
    of results.
    Add an except block so that when the function is called incorrectly, it displays the following error message: 'The DataFrame does not 
    have a ' + col_name + ' column.'.

    # Define count_entries()
    def count_entries(df, col_name='lang'):
        """Return a dictionary with counts of
        occurrences as value for each key."""

        # Initialize an empty dictionary: cols_count
        cols_count = {}

        # Add try block
        try:
            # Extract column from DataFrame: col
            col = df[col_name]
            
            # Iterate over the column in DataFrame
            for entry in col:
        
                # If entry is in cols_count, add 1
                if entry in cols_count.keys():
                    cols_count[entry] += 1
                # Else add the entry to cols_count, set the value to 1
                else:
                    cols_count[entry] = 1
        
            # Return the cols_count dictionary
            return cols_count

        # Add except block
        except:
            print('The DataFrame does not have a ' + col_name + ' column')

    # Call count_entries(): result1
    result1 = count_entries(tweets_df, 'lang')

    # Print result1
    print(result1)

    -----------------------------------------------------------------------------------
    Bringing it all together (3)
    In the previous exercise, you built on your function count_entries() to add a try-except block. This was so that users would get 
    helpful messages when calling your count_entries() function and providing a column name that isn't in the DataFrame. In this 
    exercise, you'll instead raise a ValueError in the case that the user provides a column name that isn't in the DataFrame.

    Once again, for your convenience, pandas has been imported as pd and the 'tweets.csv' file has been imported into the DataFrame 
    tweets_df. Parts of the code from your previous work are also provided.

    Instructions

    If col_name is not a column in the DataFrame df, raise a ValueError 'The DataFrame does not have a ' + col_name + ' column.'.
    Call your new function count_entries() to analyze the 'lang' column of tweets_df. Store the result in result1.
    Print result1. This has been done for you, so hit 'Submit Answer' to check out the result. In the next exercise, you'll see that 
    it raises the necessary ValueErrors.

    # Define count_entries()
    def count_entries(df, col_name='lang'):
        """Return a dictionary with counts of
        occurrences as value for each key."""
        
        # Raise a ValueError if col_name is NOT in DataFrame
        if col_name not in df.columns:
            raise ValueError('The DataFrame does not have a ' + col_name + ' column.')

        # Initialize an empty dictionary: cols_count
        cols_count = {}
        
        # Extract column from DataFrame: col
        col = df[col_name]
        
        # Iterate over the column in DataFrame
        for entry in col:

            # If entry is in cols_count, add 1
            if entry in cols_count.keys():
                cols_count[entry] += 1
                # Else add the entry to cols_count, set the value to 1
            else:
                cols_count[entry] = 1
            
            # Return the cols_count dictionary
        return cols_count

    # Call count_entries(): result1
    result1 = count_entries(tweets_df, 'lang')

    # Print result1
    print(result1)

#####################################################################################################################################
Python Data Science Toolbox Part 2:
Introduction to Iterators:

Iterating with a for Loop: 
    employees = ['Nick', 'Lore', 'Hugo']
    for employee in employees:
        print(employee)

    for letter in 'DataCamp':
        print(letter)

    for i in range (4):
        print(i)

Iterating over iterables: next()
    word = 'Da'
    it = iter(word)
    next(it)

    word = 'Data'
    it = iter(word)
    next(*it) # Prints all of the iterables in the word

Iterating over a dictionary:
pythonistas = {'hugo': 'bown-anderson', 'francis': 'castro'}
for key, value in pythonistas.items():
    print(key, value)

iterating over file connections:
file = open('file.txt')
it = iter(file)
print(next(it))

    Example:
    Iterating over iterables (1)
    Great, you're familiar with what iterables and iterators are! In this exercise, you will reinforce your knowledge about these by 
    iterating over and printing from iterables and iterators.

    You are provided with a list of strings flash. You will practice iterating over the list by using a for loop. You will also create 
    an iterator for the list and access the values from the iterator.

    Instructions
    
    Create a for loop to loop over flash and print the values in the list. Use person as the loop variable.
    Create an iterator for the list flash and assign the result to superhero.
    Print each of the items from superhero using next() 4 times.

    # Create a list of strings: flash
    flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']

    # Print each list item in flash using a for loop
    for person in flash:
        print(person)


    # Create an iterator for flash: superhero
    superhero = iter(flash)

    # Print each item from the iterator
    print(next(superhero))
    print(next(superhero))
    print(next(superhero))
    print(next(superhero))

    -----------------------------------------------------------------------------------------------
    Iterating over iterables (2)
    One of the things you learned about in this chapter is that not all iterables are actual lists. A couple of examples that we looked 
    at are strings and the use of the range() function. In this exercise, we will focus on the range() function.

    You can use range() in a for loop as if it's a list to be iterated over:

    for i in range(5):
        print(i)
    Recall that range() doesn't actually create the list; instead, it creates a range object with an iterator that produces the values 
    until it reaches the limit (in the example, until the value 4). If range() created the actual list, calling it with a value of 
    may not work, especially since a number as big as that may go over a regular computer's memory. The value 
    is actually what's called a Googol which is a 1 followed by a hundred 0s. That's a huge number!

    Your task for this exercise is to show that calling range() with 
    won't actually pre-create the list.

    Instructions
    
    Create an iterator object small_value over range(3) using the function iter().
    Using a for loop, iterate over range(3), printing the value for every iteration. Use num as the loop variable.
    Create an iterator object googol over range(10 ** 100).

    # Create an iterator for range(3): small_value
    small_value = iter(range(3))

    # Print the values in small_value
    print(next(small_value))
    print(next(small_value))
    print(next(small_value))

    # Loop over range(3) and print the values
    for i in range(3):
        print(i)


    # Create an iterator for range(10 ** 100): googol
    googol = iter(range(10**100))

    # Print the first 5 values from googol
    print(next(googol))
    print(next(googol))
    print(next(googol))
    print(next(googol))
    print(next(googol))

    -----------------------------------------------------------------------------
    Iterators as function arguments
    You've been using the iter() function to get an iterator object, as well as the next() function to retrieve the values one by one 
    from the iterator object.

    There are also functions that take iterators and iterables as arguments. For example, the list() and sum() functions return a list 
    and the sum of elements, respectively.

    In this exercise, you will use these functions by passing an iterable from range() and then printing the results of the function 
    calls.

    Instructions
    
    Create a range object that would produce the values from 10 to 20 using range(). Assign the result to values.
    Use the list() function to create a list of values from the range object values. Assign the result to values_list.
    Use the sum() function to get the sum of the values from 10 to 20 from the range object values. Assign the result to values_sum.

    # Create a range object: values
    values = range(10, 21)

    # Print the range object
    print(values)

    # Create a list of integers: values_list
    values_list = list(values)

    # Print values_list
    print(values_list)

    # Get the sum of values: values_sum
    values_sum = sum(values)

    # Print values_sum
    print(values_sum)

####################################################################################################################################
Playing with iterators:

    Examples:
    Using enumerate
    You're really getting the hang of using iterators, great job!

    You've just gained several new ideas on iterators from the last video and one of them is the enumerate() function. Recall that 
    enumerate() returns an enumerate object that produces a sequence of tuples, and each of the tuples is an index-value pair.

    In this exercise, you are given a list of strings mutants and you will practice using enumerate() on it by printing out a list of 
    tuples and unpacking the tuples using a for loop.

    Instructions

    Create a list of tuples from mutants and assign the result to mutant_list. Make sure you generate the tuples using enumerate() and 
    turn the result from it into a list using list().
    Complete the first for loop by unpacking the tuples generated by calling enumerate() on mutants. Use index1 for the index and 
    value1 for the value when unpacking the tuple.
    Complete the second for loop similarly as with the first, but this time change the starting index to start from 1 by passing it in as 
    an argument to the start parameter of enumerate(). Use index2 for the index and value2 for the value when unpacking the tuple.

    # Create a list of strings: mutants
    mutants = ['charles xavier', 
                'bobby drake', 
                'kurt wagner', 
                'max eisenhardt', 
                'kitty pryde']

    # Create a list of tuples: mutant_list
    mutant_list = list(enumerate(mutants))

    # Print the list of tuples
    print(mutant_list)

    # Unpack and print the tuple pairs
    for index1, value1 in enumerate(mutants):
        print(index1, value1)

    # Change the start index
    for index2, value2 in enumerate(mutants, start=1):
        print(index2, value2)

    --------------------------------------------------------------------------------------
    Using zip
    Another interesting function that you've learned is zip(), which takes any number of iterables and returns a zip object that is an 
    iterator of tuples. If you wanted to print the values of a zip object, you can convert it into a list and then print it. Printing 
    just a zip object will not return the values unless you unpack it first. In this exercise, you will explore this for yourself.

    Three lists of strings are pre-loaded: mutants, aliases, and powers. First, you will use list() and zip() on these lists to 
    generate a list of tuples. Then, you will create a zip object using zip(). Finally, you will unpack this zip object in a for loop 
    to print the values in each tuple. Observe the different output generated by printing the list of tuples, then the zip object, and 
    finally, the tuple values in the for loop.

    Instructions

    Using zip() with list(), create a list of tuples from the three lists mutants, aliases, and powers (in that order) and assign the 
    result to mutant_data.
    Using zip(), create a zip object called mutant_zip from the three lists mutants, aliases, and powers.
    Complete the for loop by unpacking the zip object you created and printing the tuple values. Use value1, value2, value3 for the 
    values from each of mutants, aliases, and powers, in that order.

    # Create a list of tuples: mutant_data
    mutant_data = list(zip(mutants, aliases, powers))

    # Print the list of tuples
    print(mutant_data)

    # Create a zip object using the three lists: mutant_zip
    mutant_zip = zip(mutants, aliases, powers)

    # Print the zip object
    print(mutant_zip)

    # Unpack the zip object and print the tuple values
    for value1, value2, value3 in mutant_zip:
        print(value1, value2, value3)

    ---------------------------------------------------------------------------------------
    Using * and zip to 'unzip'
    You know how to use zip() as well as how to print out values from a zip object. Excellent!

    Let's play around with zip() a little more. There is no unzip function for doing the reverse of what zip() does. We can, however, 
    reverse what has been zipped together by using zip() with a little help from *! * unpacks an iterable such as a list or a tuple 
    into positional arguments in a function call.

    In this exercise, you will use * in a call to zip() to unpack the tuples produced by zip().

    Two tuples of strings, mutants and powers have been pre-loaded.

    Instructions

    Create a zip object by using zip() on mutants and powers, in that order. Assign the result to z1.
    Print the tuples in z1 by unpacking them into positional arguments using the * operator in a print() call.
    Because the previous print() call would have exhausted the elements in z1, recreate the zip object you defined earlier and assign 
    the result again to z1.
    'Unzip' the tuples in z1 by unpacking them into positional arguments using the * operator in a zip() call. Assign the results to 
    result1 and result2, in that order.
    The last print() statements prints the output of comparing result1 to mutants and result2 to powers. Click Submit Answer to see if 
    the unpacked result1 and result2 are equivalent to mutants and powers, respectively.

    # Create a zip object from mutants and powers: z1
    z1 = zip(mutants, powers)

    # Print the tuples in z1 by unpacking with *
    print(*z1)

    # Re-create a zip object from mutants and powers: z1
    z1 = zip(mutants, powers)

    # 'Unzip' the tuples in z1 by unpacking with * and zip(): result1, result2
    result1, result2 = zip(*z1)

    # Check if unpacked tuples are equivalent to original tuples
    print(result1 == mutants)
    print(result2 == powers)

######################################################################################################################################
Using iterators to load large files into memory:

    Example:
    Processing large amounts of Twitter data
    Sometimes, the data we have to process reaches a size that is too much for a computer's memory to handle. This is a common problem 
    faced by data scientists. A solution to this is to process an entire data source chunk by chunk, instead of a single go all at once.

    In this exercise, you will do just that. You will process a large csv file of Twitter data in the same way that you processed 
    'tweets.csv' in Bringing it all together exercises of the prequel course, but this time, working on it in chunks of 10 entries at a 
    time.

    If you are interested in learning how to access Twitter data so you can work with it on your own system, refer to Part 2 of the 
    DataCamp course on Importing Data in Python.

    The pandas package has been imported as pd and the file 'tweets.csv' is in your current directory for your use.

    Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive 
    content (in this exercise, and any following exercises that also use real Twitter data).

    Instructions

    Initialize an empty dictionary counts_dict for storing the results of processing the Twitter data.
    Iterate over the 'tweets.csv' file by using a for loop. Use the loop variable chunk and iterate over the call to pd.read_csv() with 
    a chunksize of 10.
    In the inner loop, iterate over the column 'lang' in chunk by using a for loop. Use the loop variable entry.

    # Initialize an empty dictionary: counts_dict
    counts_dict = {}

    # Iterate over the file chunk by chunk
    for chunk in pd.read_csv("tweets.csv", chunksize=10):

        # Iterate over the column in DataFrame
        for entry in chunk['lang']:
            if entry in counts_dict.keys():
                counts_dict[entry] += 1
            else:
                counts_dict[entry] = 1

    # Print the populated dictionary
    print(counts_dict)

    ----------------------------------------------------------------------------------------
    Extracting information for large amounts of Twitter data
    Great job chunking out that file in the previous exercise. You now know how to deal with situations where you need to process a 
    very large file and that's a very useful skill to have!

    It's good to know how to process a file in smaller, more manageable chunks, but it can become very tedious having to write and 
    rewrite the same code for the same task each time. In this exercise, you will be making your code more reusable by putting your 
    work in the last exercise in a function definition.

    The pandas package has been imported as pd and the file 'tweets.csv' is in your current directory for your use.

    Instructions

    Define the function count_entries(), which has 3 parameters. The first parameter is csv_file for the filename, the second is c_size 
    for the chunk size, and the last is colname for the column name.
    Iterate over the file in csv_file file by using a for loop. Use the loop variable chunk and iterate over the call to pd.read_csv(), 
    passing c_size to chunksize.
    In the inner loop, iterate over the column given by colname in chunk by using a for loop. Use the loop variable entry.
    Call the count_entries() function by passing to it the filename 'tweets.csv', the size of chunks 10, and the name of the column to 
    count, 'lang'. Assign the result of the call to the variable result_counts.

    # Define count_entries()
    def count_entries(csv_file, c_size, colname):
        """Return a dictionary with counts of
        occurrences as value for each key."""
        
        # Initialize an empty dictionary: counts_dict
        counts_dict = {}

        # Iterate over the file chunk by chunk
        for chunk in pd.read_csv(csv_file, chunksize=c_size):

            # Iterate over the column in DataFrame
            for entry in chunk[colname]:
                if entry in counts_dict.keys():
                    counts_dict[entry] += 1
                else:
                    counts_dict[entry] = 1

        # Return counts_dict
        return counts_dict

    # Call count_entries(): result_counts
    result_counts = count_entries('tweets.csv', 10, 'lang')

    # Print result_counts
    print(result_counts)

#####################################################################################################################################
List Comprehensions:

List Comprehension on Lists:
nums = [12,8,21,3,16]
new_nums = [num +1 for num in nums]

# Same as: 
new_nums = []
for num in nums:
    new_nums.append(num + 1)
print(new_nums)

List Comprehension on range objects:
result = [num for num in range(11)]
print(result)

List Comprehension on nested loops:
pairs_2 = [(num1, num2) for num1 in range(0,2) for num2 in range(6,8)]

# Same as:
pairs_2 = []
for num1 in range(0,2):
    for num2 in range(6,8)
        pairs_2.append((num1, num2))
print(pairs_2)

    Example:
    Writing list comprehensions
    You now have all the knowledge necessary to begin writing list comprehensions! Your job in this exercise is to write a list 
    comprehension that produces a list of the squares of the numbers ranging from 0 to 9.

    Instructions

    Using the range of numbers from 0 to 9 as your iterable and i as your iterator variable, write a list comprehension that produces 
    a list of numbers consisting of the squared values of i.

    # Create list comprehension: squares
    squares = [i**2 for i in range(0,10)]

    ------------------------------------------------------------------------------------------
    Nested list comprehensions
    Great! At this point, you have a good grasp of the basic syntax of list comprehensions. Let's push your code-writing skills a 
    little further. In this exercise, you will be writing a list comprehension within another list comprehension, or nested list 
    comprehensions. It sounds a little tricky, but you can do it!

    Let's step aside for a while from strings. One of the ways in which lists can be used are in representing multi-dimension objects 
    such as matrices. Matrices can be represented as a list of lists in Python. For example a 5 x 5 matrix with values 0 to 4 in each 
    row can be written as:

    matrix = [[0, 1, 2, 3, 4],
            [0, 1, 2, 3, 4],
            [0, 1, 2, 3, 4],
            [0, 1, 2, 3, 4],
            [0, 1, 2, 3, 4]]
    Your task is to recreate this matrix by using nested listed comprehensions. Recall that you can create one of the rows of the 
    matrix with a single list comprehension. To create the list of lists, you simply have to supply the list comprehension as the 
    output expression of the overall list comprehension:

    [[output expression] for iterator variable in iterable]

    Note that here, the output expression is itself a list comprehension.

    Instructions

    In the inner list comprehension - that is, the output expression of the nested list comprehension - create a list of values from 
    0 to 4 using range(). Use col as the iterator variable.
    In the iterable part of your nested list comprehension, use range() to count 5 rows - that is, create a list of values from 0 to 4. 
    Use row as the iterator variable; note that you won't be needing this variable to create values in the list of lists.

    # Create a 5 x 5 matrix using a list of lists: matrix
    matrix = [[col for col in range(5)] for row in range(5)]

    # Print the matrix
    for row in matrix:
        print(row)

########################################################################################################################################
Advanced Comprehensions:

Conditionals in Comprehensions
[num ** 2 for num in range(10) if num % 2 == 0] # introduces conditions in the computation

[num ** 2 if num % 2 == 0 else 0 for num in range(10)] # Introduces conditions in the output expression

Dictionary comprehensions:
pos_neg = {num: -num for num in range(9)}
print(pos_neg)

    Example:
    Using conditionals in comprehensions (1)
    You've been using list comprehensions to build lists of values, sometimes using operations to create these values.

    An interesting mechanism in list comprehensions is that you can also create lists with values that meet only a certain condition. 
    One way of doing this is by using conditionals on iterator variables. In this exercise, you will do exactly that!

    Recall from the video that you can apply a conditional statement to test the iterator variable by adding an if statement in the 
    optional predicate expression part after the for statement in the comprehension:

    [ output expression for iterator variable in iterable if predicate expression ].

    You will use this recipe to write a list comprehension for this exercise. You are given a list of strings fellowship and, using a 
    list comprehension, you will create a list that only includes the members of fellowship that have 7 characters or more.

    Instructions

    Use member as the iterator variable in the list comprehension. For the conditional, use len() to evaluate the iterator variable. 
    Note that you only want strings with 7 characters or more.

    # Create a list of strings: fellowship
    fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

    # Create list comprehension: new_fellowship
    new_fellowship = [member for member in fellowship if len(member) >= 7]

    # Print the new list
    print(new_fellowship)

    ------------------------------------------------------------------------------------------
    Using conditionals in comprehensions (2)
    In the previous exercise, you used an if conditional statement in the predicate expression part of a list comprehension to evaluate 
    an iterator variable. In this exercise, you will use an if-else statement on the output expression of the list.

    You will work on the same list, fellowship and, using a list comprehension and an if-else conditional statement in the output 
    expression, create a list that keeps members of fellowship with 7 or more characters and replaces others with an empty string. Use 
    member as the iterator variable in the list comprehension.

    Instructions

    In the output expression, keep the string as-is if the number of characters is >= 7, else replace it with an empty string - that 
    is, '' or "".

    # Create a list of strings: fellowship
    fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

    # Create list comprehension: new_fellowship
    new_fellowship = [member if len(member) >= 7 else '' for member in fellowship ]

    # Print the new list
    print(new_fellowship)

    ----------------------------------------------------------------------------------------------
    Dict comprehensions
    Comprehensions aren't relegated merely to the world of lists. There are many other objects you can build using comprehensions, such 
    as dictionaries, pervasive objects in Data Science. You will create a dictionary using the comprehension syntax for this exercise. 
    In this case, the comprehension is called a dict comprehension.

    Recall that the main difference between a list comprehension and a dict comprehension is the use of curly braces {} instead of []. 
    Additionally, members of the dictionary are created using a colon :, as in <key> : <value>.

    You are given a list of strings fellowship and, using a dict comprehension, create a dictionary with the members of the list as the 
    keys and the length of each string as the corresponding values.

    Instructions

    Create a dict comprehension where the key is a string in fellowship and the value is the length of the string. Remember to use the 
    syntax <key> : <value> in the output expression part of the comprehension to create the members of the dictionary. Use member as 
    the iterator variable.

    # Create a list of strings: fellowship
    fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

    # Create dict comprehension: new_fellowship
    new_fellowship = {member: len(member) for member in fellowship}

    # Print the new dictionary
    print(new_fellowship)

#####################################################################################################################################
Generators vs. list comprehensions:

    Example:
    Write your own generator expressions
    You are familiar with what generators and generator expressions are, as well as its difference from list comprehensions. In this 
    exercise, you will practice building generator expressions on your own.

    Recall that generator expressions basically have the same syntax as list comprehensions, except that it uses parentheses () instead 
    of brackets []; this should make things feel familiar! Furthermore, if you have ever iterated over a dictionary with .items(), or 
    used the range() function, for example, you have already encountered and used generators before, without knowing it! When you use 
    these functions, Python creates generators for you behind the scenes.

    Now, you will start simple by creating a generator object that produces numeric values.

    Instructions

    Create a generator object that will produce values from 0 to 30. Assign the result to result and use num as the iterator variable 
    in the generator expression.
    Print the first 5 values by using next() appropriately in print().
    Print the rest of the values by using a for loop to iterate over the generator object.

    # Create generator object: result
    result = (num for num in range(31))

    # Print the first 5 values
    print(next(result))
    print(next(result))
    print(next(result))
    print(next(result))
    print(next(result))

    # Print the rest of the values
    for value in result:
        print(value)

    ----------------------------------------------------------------------------------------
    Changing the output in generator expressions
    Great! At this point, you already know how to write a basic generator expression. In this exercise, you will push this idea a 
    little further by adding to the output expression of a generator expression. Because generator expressions and list comprehensions 
    are so alike in syntax, this should be a familiar task for you!

    You are given a list of strings lannister and, using a generator expression, create a generator object that you will iterate over 
    to print its values.

    Instructions

    Write a generator expression that will generate the lengths of each string in lannister. Use person as the iterator variable. 
    Assign the result to lengths.
    Supply the correct iterable in the for loop for printing the values in the generator object.

    # Create a list of strings: lannister
    lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

    # Create a generator object: lengths
    lengths = (len(person) for person in lannister)

    # Iterate over and print the values in lengths
    for value in lengths:
        print(value)

    -----------------------------------------------------------------------------------------------
    Build a generator
    In previous exercises, you've dealt mainly with writing generator expressions, which uses comprehension syntax. Being able to use 
    comprehension syntax for generator expressions made your work so much easier!

    Now, recall from the video that not only are there generator expressions, there are generator functions as well. Generator 
    functions are functions that, like generator expressions, yield a series of values, instead of returning a single value. A generator function is defined as you do a regular function, but whenever it generates a value, it uses the keyword yield instead of return.

    In this exercise, you will create a generator function with a similar mechanism as the generator expression you defined in the 
    previous exercise:

    lengths = (len(person) for person in lannister)
    Instructions

    Complete the function header for the function get_lengths() that has a single parameter, input_list.
    In the for loop in the function definition, yield the length of the strings in input_list.
    Complete the iterable part of the for loop for printing the values generated by the get_lengths() generator function. Supply the 
    call to get_lengths(), passing in the list lannister.

    # Create a list of strings
    lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

    # Define generator function get_lengths
    def get_lengths(input_list):
        """Generator function that yields the
        length of the strings in input_list."""

        # Yield the length of a string
        for person in input_list:
            yield len(person)

    # Print the values generated by get_lengths()
    for value in get_lengths(lannister):
        print(value)

#####################################################################################################################################
Wrapping up comprehensions and generators:

List Comprehension Syntax:
[output expression for iterator variable in iterable]
[len(name) for name in names] # Example

Advances Comprehension Syntax:
[output expression + conditional on output for iterator variable in iterable + conditional on iterable]

    Example:
    List comprehensions for time-stamped data
    You will now make use of what you've learned from this chapter to solve a simple data extraction problem. You will also be 
    introduced to a data structure, the pandas Series, in this exercise. We won't elaborate on it much here, but what you should know 
    is that it is a data structure that you will be working with a lot of times when analyzing data from pandas DataFrames. You can 
    think of DataFrame columns as single-dimension arrays called Series.

    In this exercise, you will be using a list comprehension to extract the time from time-stamped Twitter data. The pandas package has 
    been imported as pd and the file 'tweets.csv' has been imported as the df DataFrame for your use.

    Instructions

    Extract the column 'created_at' from df and assign the result to tweet_time. Fun fact: the extracted column in tweet_time here is a 
    Series data structure!
    Create a list comprehension that extracts the time from each row in tweet_time. Each row is a string that represents a timestamp, 
    and you will access the 12th to 19th characters in the string to extract the time. Use entry as the iterator variable and assign 
    the result to tweet_clock_time. Remember that Python uses 0-based indexing!

    # Extract the created_at column from df: tweet_time
    tweet_time = df['created_at']

    # Extract the clock time: tweet_clock_time
    tweet_clock_time = [entry[11:19] for entry in tweet_time]

    # Print the extracted times
    print(tweet_clock_time)

    ---------------------------------------------------------------------------------------------
    Conditional list comprehensions for time-stamped data
    Great, you've successfully extracted the data of interest, the time, from a pandas DataFrame! Let's tweak your work further by 
    adding a conditional that further specifies which entries to select.

    In this exercise, you will be using a list comprehension to extract the time from time-stamped Twitter data. You will add a 
    conditional expression to the list comprehension so that you only select the times in which entry[17:19] is equal to '19'. The 
    pandas package has been imported as pd and the file 'tweets.csv' has been imported as the df DataFrame for your use.

    Instructions

    Extract the column 'created_at' from df and assign the result to tweet_time.
    Create a list comprehension that extracts the time from each row in tweet_time. Each row is a string that represents a timestamp, 
    and you will access the 12th to 19th characters in the string to extract the time. Use entry as the iterator variable and assign 
    the result to tweet_clock_time. Additionally, add a conditional expression that checks whether entry[17:19] is equal to '19'

    # Extract the created_at column from df: tweet_time
    tweet_time = df['created_at']

    # Extract the clock time: tweet_clock_time
    tweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == '19']

    # Print the extracted times
    print(tweet_clock_time)

#####################################################################################################################################
Welcome to the Case Study:

    Example:
    Dictionaries for data science
    For this exercise, you'll use what you've learned about the zip() function and combine two lists into a dictionary.

    These lists are actually extracted from a bigger dataset file of world development indicators from the World Bank. For pedagogical 
    purposes, we have pre-processed this dataset into the lists that you'll be working with.

    The first list feature_names contains header names of the dataset and the second list row_vals contains actual values of a row from 
    the dataset, corresponding to each of the header names.

    Instructions

    Create a zip object by calling zip() and passing to it feature_names and row_vals. Assign the result to zipped_lists.
    Create a dictionary from the zipped_lists zip object by calling dict() with zipped_lists. Assign the resulting dictionary to 
    rs_dict.

    # Zip lists: zipped_lists
    zipped_lists = zip(feature_names, row_vals)

    # Create a dictionary: rs_dict
    rs_dict = dict(zipped_lists)

    # Print the dictionary
    print(rs_dict)

    -------------------------------------------------------------------------------------
    Writing a function to help you
    Suppose you needed to repeat the same process done in the previous exercise to many, many rows of data. Rewriting your code again 
    and again could become very tedious, repetitive, and unmaintainable.

    In this exercise, you will create a function to house the code you wrote earlier to make things easier and much more concise. Why? 
    This way, you only need to call the function and supply the appropriate lists to create your dictionaries! Again, the lists 
    feature_names and row_vals are preloaded and these contain the header names of the dataset and actual values of a row from the 
    dataset, respectively.

    Instructions

    Define the function lists2dict() with two parameters: first is list1 and second is list2.
    Return the resulting dictionary rs_dict in lists2dict().
    Call the lists2dict() function with the arguments feature_names and row_vals. Assign the result of the function call to rs_fxn.

    # Define lists2dict()
    def lists2dict(list1, list2):
        """Return a dictionary where list1 provides
        the keys and list2 provides the values."""

        # Zip lists: zipped_lists
        zipped_lists = zip(list1, list2)

        # Create a dictionary: rs_dict
        rs_dict = dict(zipped_lists)

        # Return the dictionary
        return rs_dict

    # Call lists2dict: rs_fxn
    rs_fxn = lists2dict(feature_names, row_vals)

    # Print rs_fxn
    print(rs_fxn)

    ------------------------------------------------------------------------------------------
    Using a list comprehension
    This time, you're going to use the lists2dict() function you defined in the last exercise to turn a bunch of lists into a list of 
    dictionaries with the help of a list comprehension.

    The lists2dict() function has already been preloaded, together with a couple of lists, feature_names and row_lists. feature_names 
    contains the header names of the World Bank dataset and row_lists is a list of lists, where each sublist is a list of actual values 
    of a row from the dataset.

    Your goal is to use a list comprehension to generate a list of dicts, where the keys are the header names and the values are the 
    row entries.

    Instructions

    Inspect the contents of row_lists by printing the first two lists in row_lists.
    Create a list comprehension that generates a dictionary using lists2dict() for each sublist in row_lists. The keys are from the 
    feature_names list and the values are the row entries in row_lists. Use sublist as your iterator variable and assign the resulting 
    list of dictionaries to list_of_dicts.
    Look at the first two dictionaries in list_of_dicts by printing them out.

    # Print the first two lists in row_lists
    print(row_lists[0])
    print(row_lists[1])

    # Turn list of lists into list of dicts: list_of_dicts
    list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]

    # Print the first two dictionaries in list_of_dicts
    print(list_of_dicts[0])
    print(list_of_dicts[1])

    ----------------------------------------------------------------------------------------------
    Turning this all into a DataFrame
    You've zipped lists together, created a function to house your code, and even used the function in a list comprehension to generate 
    a list of dictionaries. That was a lot of work and you did a great job!

    You will now use all of these to convert the list of dictionaries into a pandas DataFrame. You will see how convenient it is to 
    generate a DataFrame from dictionaries with the DataFrame() function from the pandas package.

    The lists2dict() function, feature_names list, and row_lists list have been preloaded for this exercise.

    Go for it!

    Instructions

    To use the DataFrame() function you need, first import the pandas package with the alias pd.
    Create a DataFrame from the list of dictionaries in list_of_dicts by calling pd.DataFrame(). Assign the resulting DataFrame to df.
    Inspect the contents of df printing the head of the DataFrame. Head of the DataFrame df can be accessed by calling df.head().

    # Import the pandas package
    import pandas as pd

    # Turn list of lists into list of dicts: list_of_dicts
    list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]

    # Turn list of dicts into a DataFrame: df
    df = pd.DataFrame(list_of_dicts)

    # Print the head of the DataFrame
    print(df.head())

#####################################################################################################################################
Using Python generators for streaming data:

    Example:
    Processing data in chunks (1)
    Sometimes, data sources can be so large in size that storing the entire dataset in memory becomes too resource-intensive. In this exercise, you will process the first 1000 rows of a file line by line, to create a dictionary of the counts of how many times each country appears in a column in the dataset.

    The csv file 'world_dev_ind.csv' is in your current directory for your use. To begin, you need to open a connection to this file using what is known as a context manager. For example, the command with open('datacamp.csv') as datacamp binds the csv file 'datacamp.csv' as datacamp in the context manager. Here, the with statement is the context manager, and its purpose is to ensure that resources are efficiently allocated when opening a connection to a file.

    If you'd like to learn more about context managers, refer to the DataCamp course on Importing Data in Python.

    Instructions

    Use open() to bind the csv file 'world_dev_ind.csv' as file in the context manager.
    Complete the for loop so that it iterates 1000 times to perform the loop body and process only the first 1000 rows of data of the file.

    # Open a connection to the file
    with open('world_dev_ind.csv') as file:

        # Skip the column names
        file.readline()

        # Initialize an empty dictionary: counts_dict
        counts_dict = {}

        # Process only the first 1000 rows
        for j in range(1000):

            # Split the current line into a list: line
            line = file.readline().split(',')

            # Get the value for the first column: first_col
            first_col = line[0]

            # If the column value is in the dict, increment its value
            if first_col in counts_dict.keys():
                counts_dict[first_col] += 1

            # Else, add to the dict and set value to 1
            else:
                counts_dict[first_col] = 1

    # Print the resulting dictionary
    print(counts_dict)

    ---------------------------------------------------------------------------------------------
    Writing a generator to load data in chunks (2)
    In the previous exercise, you processed a file line by line for a given number of lines. What if, however, you want to do this for 
    the entire file?

    In this case, it would be useful to use generators. Generators allow users to lazily evaluate data. This concept of lazy evaluation 
    is useful when you have to deal with very large datasets because it lets you generate values in an efficient manner by yielding 
    only chunks of data at a time instead of the whole thing at once.

    In this exercise, you will define a generator function read_large_file() that produces a generator object which yields a single 
    line from a file each time next() is called on it. The csv file 'world_dev_ind.csv' is in your current directory for your use.

    Note that when you open a connection to a file, the resulting file object is already a generator! So out in the wild, you won't 
    have to explicitly create generator objects in cases such as this. However, for pedagogical reasons, we are having you practice 
    how to do this here with the read_large_file() function. Go for it!

    Instructions

    In the function read_large_file(), read a line from file_object by using the method readline(). Assign the result to data.
    In the function read_large_file(), yield the line read from the file data.
    In the context manager, create a generator object gen_file by calling your generator function read_large_file() and passing file 
    to it.
    Print the first three lines produced by the generator object gen_file using next().

    # Define read_large_file()
    def read_large_file(file_object):
        """A generator function to read a large file lazily."""

        # Loop indefinitely until the end of the file
        while True:

            # Read a line from the file: data
            data = file_object.readline()

            # Break if this is the end of the file
            if not data:
                break

            # Yield the line of data
            yield data
            
    # Open a connection to the file
    with open('world_dev_ind.csv') as file:

        # Create a generator object for the file: gen_file
        gen_file = read_large_file(file)

        # Print the first three lines of the file
        print(next(gen_file))
        print(next(gen_file))
        print(next(gen_file))

    --------------------------------------------------------------------------------------
    Writing a generator to load data in chunks (3)
    Great! You've just created a generator function that you can use to help you process large files.

    Now let's use your generator function to process the World Bank dataset like you did previously. You will process the file line by 
    line, to create a dictionary of the counts of how many times each country appears in a column in the dataset. For this exercise, 
    however, you won't process just 1000 rows of data, you'll process the entire dataset!

    The generator function read_large_file() and the csv file 'world_dev_ind.csv' are preloaded and ready for your use. Go for it!

    Instructions

    Bind the file 'world_dev_ind.csv' to file in the context manager with open().
    Complete the for loop so that it iterates over the generator from the call to read_large_file() to process all the rows of the file.

    # Initialize an empty dictionary: counts_dict
    counts_dict = {}

    # Open a connection to the file
    with open('world_dev_ind.csv') as file:

        # Iterate over the generator from read_large_file()
        for line in read_large_file(file):

            row = line.split(',')
            first_col = row[0]

            if first_col in counts_dict.keys():
                counts_dict[first_col] += 1
            else:
                counts_dict[first_col] = 1

    # Print            
    print(counts_dict)

####################################################################################################################################
Using pandas' read_csv iterator for streaming data:

    Example:
    Writing an iterator to load data in chunks (1)
    Another way to read data too large to store in memory in chunks is to read the file in as DataFrames of a certain length, say, 100. 
    For example, with the pandas package (imported as pd), you can do pd.read_csv(filename, chunksize=100). This creates an iterable 
    reader object, which means that you can use next() on it.

    In this exercise, you will read a file in small DataFrame chunks with read_csv(). You're going to use the World Bank Indicators 
    data 'ind_pop.csv', available in your current directory, to look at the urban population indicator for numerous countries and 
    years.

    Instructions

    Use pd.read_csv() to read in 'ind_pop.csv' in chunks of size 10. Assign the result to df_reader.
    Print the first two chunks from df_reader.

    # Import the pandas package
    import pandas as pd

    # Initialize reader object: df_reader
    df_reader = pd.read_csv('ind_pop.csv', chunksize=10)

    # Print two chunks
    print(next(df_reader))
    print(next(df_reader))

    ------------------------------------------------------------------------------------
    Writing an iterator to load data in chunks (2)
    In the previous exercise, you used read_csv() to read in DataFrame chunks from a large dataset. In this exercise, you will read in 
    a file using a bigger DataFrame chunk size and then process the data from the first chunk.

    To process the data, you will create another DataFrame composed of only the rows from a specific country. You will then zip 
    together two of the columns from the new DataFrame, 'Total Population' and 'Urban population (% of total)'. Finally, you will 
    create a list of tuples from the zip object, where each tuple is composed of a value from each of the two columns mentioned.

    You're going to use the data from 'ind_pop_data.csv', available in your current directory. pandas has been imported as pd.

    Instructions

    Use pd.read_csv() to read in the file in 'ind_pop_data.csv' in chunks of size 1000. Assign the result to urb_pop_reader.
    Get the first DataFrame chunk from the iterable urb_pop_reader and assign this to df_urb_pop.
    Select only the rows of df_urb_pop that have a 'CountryCode' of 'CEB'. To do this, compare whether df_urb_pop['CountryCode'] is 
    equal to 'CEB' within the square brackets in df_urb_pop[____].
    Using zip(), zip together the 'Total Population' and 'Urban population (% of total)' columns of df_pop_ceb. Assign the resulting 
    zip object to pops.

    # Initialize reader object: urb_pop_reader
    urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)

    # Get the first DataFrame chunk: df_urb_pop
    df_urb_pop = next(urb_pop_reader)

    # Check out the head of the DataFrame
    print(df_urb_pop.head())

    # Check out specific country: df_pop_ceb
    df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']

    # Zip DataFrame columns of interest: pops
    pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])

    # Turn zip object into list: pops_list
    pops_list = list(pops)

    # Print pops_list
    print(pops_list)

    ----------------------------------------------------------------------------------------------
    Writing an iterator to load data in chunks (3)
    You're getting used to reading and processing data in chunks by now. Let's push your skills a little further by adding a column to 
    a DataFrame.

    Starting from the code of the previous exercise, you will be using a list comprehension to create the values for a new column 
    'Total Urban Population' from the list of tuples that you generated earlier. Recall from the previous exercise that the first and 
    second elements of each tuple consist of, respectively, values from the columns 'Total Population' and 'Urban population 
    (% of total)'. The values in this new column 'Total Urban Population', therefore, are the product of the first and second element 
    in each tuple. Furthermore, because the 2nd element is a percentage, you need to divide the entire result by 100, or alternatively, 
    multiply it by 0.01.

    You will also plot the data from this new column to create a visualization of the urban population data.

    The packages pandas and matplotlib.pyplot have been imported as pd and plt respectively for your use.

    Instructions

    Write a list comprehension to generate a list of values from pops_list for the new column 'Total Urban Population'. The output 
    expression should be the product of the first and second element in each tuple in pops_list. Because the 2nd element is a 
    percentage, you also need to either multiply the result by 0.01 or divide it by 100. In addition, note that the column 
    'Total Urban Population' should only be able to take on integer values. To ensure this, make sure you cast the output expression 
    to an integer with int().
    Create a scatter plot where the x-axis are values from the 'Year' column and the y-axis are values from the 
    'Total Urban Population' column.

    # Code from previous exercise
    urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)
    df_urb_pop = next(urb_pop_reader)
    df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']
    pops = zip(df_pop_ceb['Total Population'], 
            df_pop_ceb['Urban population (% of total)'])
    pops_list = list(pops)

    # Use list comprehension to create new DataFrame column 'Total Urban Population'
    df_pop_ceb['Total Urban Population'] = [int(tup[0] * (tup[1] * 0.01)) for tup in pops_list]

    # Plot urban population data
    df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')
    plt.show()

    -----------------------------------------------------------------------------------------------
    Writing an iterator to load data in chunks (4)
    In the previous exercises, you've only processed the data from the first DataFrame chunk. This time, you will aggregate the results 
    over all the DataFrame chunks in the dataset. This basically means you will be processing the entire dataset now. This is neat 
    because you're going to be able to process the entire large dataset by just working on smaller pieces of it!

    You're going to use the data from 'ind_pop_data.csv', available in your current directory. The packages pandas and 
    matplotlib.pyplot have been imported as pd and plt respectively for your use.

    Instructions

    Initialize an empty DataFrame data using pd.DataFrame().
    In the for loop, iterate over urb_pop_reader to be able to process all the DataFrame chunks in the dataset.
    Concatenate data and df_pop_ceb by passing a list of the DataFrames to pd.concat().

    # Initialize reader object: urb_pop_reader
    urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)

    # Initialize empty DataFrame: data
    data = pd.DataFrame()

    # Iterate over each DataFrame chunk
    for df_urb_pop in urb_pop_reader:

        # Check out specific country: df_pop_ceb
        df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']

        # Zip DataFrame columns of interest: pops
        pops = zip(df_pop_ceb['Total Population'],
                    df_pop_ceb['Urban population (% of total)'])

        # Turn zip object into list: pops_list
        pops_list = list(pops)

        # Use list comprehension to create new DataFrame column 'Total Urban Population'
        df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]
        
        # Concatenate DataFrame chunk to the end of data: data
        data = pd.concat([data, df_pop_ceb])

    # Plot urban population data
    data.plot(kind='scatter', x='Year', y='Total Urban Population')
    plt.show()

    ----------------------------------------------------------------------------------------
    Writing an iterator to load data in chunks (5)
    This is the last leg. You've learned a lot about processing a large dataset in chunks. In this last exercise, you will put all the 
    code for processing the data into a single function so that you can reuse the code without having to rewrite the same things all 
    over again.

    You're going to define the function plot_pop() which takes two arguments: the filename of the file to be processed, and the country 
    code of the rows you want to process in the dataset.

    Because all of the previous code you've written in the previous exercises will be housed in plot_pop(), calling the function 
    already does the following:

    Loading of the file chunk by chunk,
    Creating the new column of urban population values, and
    Plotting the urban population data.
    That's a lot of work, but the function now makes it convenient to repeat the same process for whatever file and country code you 
    want to process and visualize!

    You're going to use the data from 'ind_pop_data.csv', available in your current directory. The packages pandas and 
    matplotlib.pyplot has been imported as pd and plt respectively for your use.

    After you are done, take a moment to look at the plots and reflect on the new skills you have acquired. The journey doesn't end 
    here! If you have enjoyed working with this data, you can continue exploring it using the pre-processed version available on Kaggle.

    Instructions

    Define the function plot_pop() that has two arguments: first is filename for the file to process and second is country_code for the 
    country to be processed in the dataset.
    Call plot_pop() to process the data for country code 'CEB' in the file 'ind_pop_data.csv'.
    Call plot_pop() to process the data for country code 'ARB' in the file 'ind_pop_data.csv'.

    # Define plot_pop()
    def plot_pop(filename, country_code):

        # Initialize reader object: urb_pop_reader
        urb_pop_reader = pd.read_csv(filename, chunksize=1000)

        # Initialize empty DataFrame: data
        data = pd.DataFrame()
        
        # Iterate over each DataFrame chunk
        for df_urb_pop in urb_pop_reader:
            # Check out specific country: df_pop_ceb
            df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == country_code]

            # Zip DataFrame columns of interest: pops
            pops = zip(df_pop_ceb['Total Population'],
                        df_pop_ceb['Urban population (% of total)'])

            # Turn zip object into list: pops_list
            pops_list = list(pops)

            # Use list comprehension to create new DataFrame column 'Total Urban Population'
            df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]
            
            # Concatenate DataFrame chunk to the end of data: data
            data = pd.concat([data, df_pop_ceb])

        # Plot urban population data
        data.plot(kind='scatter', x='Year', y='Total Urban Population')
        plt.show()

    # Set the filename: fn
    fn = 'ind_pop_data.csv'

    # Call plot_pop for country code 'CEB'
    plot_pop(fn, 'CEB')

    # Call plot_pop for country code 'ARB'
    plot_pop(fn, 'ARB')

#######################################################################################################################################
Introduction to importing Data in Python
Welcome to the Course: 

Exploring your working directory
In order to import data into Python, you should first have an idea of what files are in your working directory.

IPython, which is running on DataCamp's servers, has a bunch of cool commands, including its magic commands. For example, starting a 
line with ! gives you complete system shell access. This means that the IPython magic command ! ls will display the contents of your 
current directory. Your task is to use the IPython magic command ! ls to check out the contents of your current directory and answer 
the following question: which of the following files is in your working directory?


    Example:
    Importing entire text files
    In this exercise, you'll be working with the file moby_dick.txt. It is a text file that contains the opening sentences of Moby Dick, one of the great American novels! Here you'll get experience opening a text file, printing its contents to the shell and, finally, closing it.

    Instructions

    Open the file moby_dick.txt as read-only and store it in the variable file. Make sure to pass the filename enclosed in quotation marks ''.
    Print the contents of the file to the shell using the print() function. As Hugo showed in the video, you'll need to apply the method 
    read() to the object file.
    Check whether the file is closed by executing print(file.closed).
    Close the file using the close() method.
    Check again that the file is closed as you did above.

    # Open a file: file
    file = open('moby_dick.txt', mode='r')

    # Print it
    print(file.read())

    # Check whether file is closed
    print(file.closed)

    # Close file
    file.close()

    # Check whether file is closed
    print(file.closed)

    --------------------------------------------------------------------------------------------
    Importing text files line by line
    For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines. Enter 
    the readline() method, which allows you to do this. When a file called file is open, you can print out the first line by executing file.readline(). If you execute the same command again, the second line will print, and so on.

    In the introductory video, Hugo also introduced the concept of a context manager. He showed that you can bind a variable file by 
    using a context manager construct:

    with open('huck_finn.txt') as file:
    While still within this construct, the variable file will be bound to open('huck_finn.txt'); thus, to print the file to the shell, 
    all the code you need to execute is:

    with open('huck_finn.txt') as file:
        print(file.readline())
    You'll now use these tools to print the first few lines of moby_dick.txt!

    Instructions
    Open moby_dick.txt using the with context manager and the variable file.
    Print the first three lines of the file to the shell by using readline() three times within the context manager.

    # Read & print the first 3 lines
    with open('moby_dick.txt') as file:
        print(file.readline())
        print(file.readline())
        print(file.readline())

#######################################################################################################################################
The importance of flat files in data Science:


#######################################################################################################################################
Importing flat files using NumPy:

    Example:
    Using NumPy to import flat files
    In this exercise, you're now going to load the MNIST digit recognition dataset using the numpy function loadtxt() and see just how easy it can be:

    The first argument will be the filename.
    The second will be the delimiter which, in this case, is a comma.
    You can find more information about the MNIST dataset here on the webpage of Yann LeCun, who is currently Director of AI Research at Facebook and Founding Director of the NYU Center for Data Science, among many other things.

    Instructions

    Fill in the arguments of np.loadtxt() by passing file and a comma ',' for the delimiter.
    Fill in the argument of print() to print the type of the object digits. Use the function type().
    Execute the rest of the code to visualize one of the rows of the data.

    # Import package
    import numpy as np

    # Assign filename to variable: file
    file = 'digits.csv'

    # Load file as array: digits
    digits = np.loadtxt(file, delimiter=',')

    # Print datatype of digits
    print(type(digits))

    # Select and reshape a row
    im = digits[21, 1:]
    im_sq = np.reshape(im, (28, 28))

    # Plot reshaped data (matplotlib.pyplot already loaded as plt)
    plt.imshow(im_sq, cmap='Greys', interpolation='nearest')
    plt.show()

    ------------------------------------------------------------------------------------------------
    Customizing your NumPy import
    What if there are rows, such as a header, that you don't want to import? What if your file has a delimiter other than a comma? What 
    if you only wish to import particular columns?

    There are a number of arguments that np.loadtxt() takes that you'll find useful:

    delimiter changes the delimiter that loadtxt() is expecting.
    You can use ',' for comma-delimited.
    You can use '\t' for tab-delimited.
    skiprows allows you to specify how many rows (not indices) you wish to skip
    usecols takes a list of the indices of the columns you wish to keep.
    The file that you'll be importing, digits_header.txt, has a header and is tab-delimited.

    Instructions

    Complete the arguments of np.loadtxt(): the file you're importing is tab-delimited, you want to skip the first row and you only want 
    to import the first and third columns.
    Complete the argument of the print() call in order to print the entire array that you just imported.

    # Import numpy
    import numpy as np

    # Assign the filename: file
    file = 'digits_header.txt'

    # Load the data: data
    data = np.loadtxt(file, delimiter='\t', skiprows=1, usecols=[0,2])

    # Print data
    print(data)

    -------------------------------------------------------------------------------------------------------
    Importing different datatypes
    The file seaslug.txt

    has a text header, consisting of strings
    is tab-delimited.
    These data consists of percentage of sea slug larvae that had metamorphosed in a given time period. Read more here.

    Due to the header, if you tried to import it as-is using np.loadtxt(), Python would throw you a ValueError and tell you that it 
    could not convert string to float. There are two ways to deal with this: firstly, you can set the data type argument dtype equal to 
    str (for string).

    Alternatively, you can skip the first row as we have seen before, using the skiprows argument.

    Instructions

    Complete the first call to np.loadtxt() by passing file as the first argument.
    Execute print(data[0]) to print the first element of data.
    Complete the second call to np.loadtxt(). The file you're importing is tab-delimited, the datatype is float, and you want to skip 
    the first row.
    Print the 10th element of data_float by completing the print() command. Be guided by the previous print() call.
    Execute the rest of the code to visualize the data.

    # Assign filename: file
    file = 'seaslug.txt'

    # Import file: data
    data = np.loadtxt(file, delimiter='\t', dtype=str)

    # Print the first element of data
    print(data[0])

    # Import data as floats and skip the first row: data_float
    data_float = np.loadtxt(file, delimiter='\t', dtype=float, skiprows=1)

    # Print the 10th element of data_float
    print(data_float[9])

    # Plot a scatterplot of the data
    plt.scatter(data_float[:, 0], data_float[:, 1])
    plt.xlabel('time (min.)')
    plt.ylabel('percentage of larvae')
    plt.show()

    ---------------------------------------------------------------------------------------------------
    Working with mixed datatypes (2)
    You have just used np.genfromtxt() to import data containing mixed datatypes. There is also another function np.recfromcsv() that 
    behaves similarly to np.genfromtxt(), except that its default dtype is None. In this exercise, you'll practice using this to achieve 
    the same result.

    Instructions

    Import titanic.csv using the function np.recfromcsv() and assign it to the variable, d. You'll only need to pass file to it because 
    it has the defaults delimiter=',' and names=True in addition to dtype=None!
    Run the remaining code to print the first three entries of the resulting array d.

    # Assign the filename: file
    file = 'titanic.csv'

    # Import file using np.recfromcsv: d
    d = np.recfromcsv(file)

    # Print out first three entries of d
    print(d[:3])

#######################################################################################################################################
Importing Flat Files using pandas:

    Example:
    Using pandas to import flat files as DataFrames (1)
    In the last exercise, you were able to import flat files containing columns with different datatypes as numpy arrays. However, the 
    DataFrame object in pandas is a more appropriate structure in which to store such data and, thankfully, we can easily import files 
    of mixed data types as DataFrames using the pandas functions read_csv() and read_table().

    Instructions

    Import the pandas package using the alias pd.
    Read titanic.csv into a DataFrame called df. The file name is already stored in the file object.
    In a print() call, view the head of the DataFrame.

    # Import pandas as pd
    import pandas as pd

    # Assign the filename: file
    file = 'titanic.csv'

    # Read the file into a DataFrame: df
    df = pd.read_csv(file)

    # View the head of the DataFrame
    print(df.head())

    -------------------------------------------------------------------------------------------
    Using pandas to import flat files as DataFrames (2)
    In the last exercise, you were able to import flat files into a pandas DataFrame. As a bonus, it is then straightforward to retrieve 
    the corresponding numpy array using the attribute values. You'll now have a chance to do this using the MNIST dataset, which is 
    available as digits.csv.

    Instructions

    Import the first 5 rows of the file into a DataFrame using the function pd.read_csv() and assign the result to data. You'll need to 
    use the arguments nrows and header (there is no header in this file).
    Build a numpy array from the resulting DataFrame in data and assign to data_array.
    Execute print(type(data_array)) to print the datatype of data_array.

    # Assign the filename: file
    file = 'digits.csv'

    # Read the first 5 rows of the file into a DataFrame: data
    data = pd.read_csv(file, nrows=5, header=None)

    # Build a numpy array from the DataFrame: data_array
    data_array = data.values

    # Print the datatype of data_array to the shell
    print(type(data_array))

    --------------------------------------------------------------------------------------------
    Customizing your pandas import
    The pandas package is also great at dealing with many of the issues you will encounter when importing data as a data scientist, such as comments occurring in flat files, empty lines and missing values. Note that missing values are also commonly referred to as NA or NaN. To wrap up this chapter, you're now going to import a slightly corrupted copy of the Titanic dataset titanic_corrupt.txt, which

    contains comments after the character '#'
    is tab-delimited.
    Instructions

    Complete the sep (the pandas version of delim), comment and na_values arguments of pd.read_csv(). comment takes characters that comments occur after in the file, which in this case is '#'. na_values takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'.
    Execute the rest of the code to print the head of the resulting DataFrame and plot the histogram of the 'Age' of passengers aboard the Titanic.

    # Import matplotlib.pyplot as plt
    import matplotlib.pyplot as plt

    # Assign filename: file
    file = 'titanic_corrupt.txt'

    # Import file: data
    data = pd.read_csv(file, sep='\t', comment='#', na_values='Nothing')

    # Print the head of the DataFrame
    print(data.head())

    # Plot 'Age' variable in a histogram
    pd.DataFrame.hist(data[['Age']])
    plt.xlabel('Age (years)')
    plt.ylabel('count')
    plt.show()

#########################################################################################################################################
Introduction to other file types:

    Example:
    Loading a pickled file
    There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to 
    be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are 
    appropriate for Python dictionaries.

    However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object 
    into a sequence of bytes, or a bytestream.

    In this exercise, you'll import the pickle package, open a previously pickled data structure from a file and load it.

    Instructions

    Import the pickle package.
    Complete the second argument of open() so that it is read only for a binary file. This argument will be a string of two letters, one 
    signifying 'read only', the other 'binary'.
    Pass the correct argument to pickle.load(); it should use the variable that is bound to open.
    Print the data, d.
    Print the datatype of d; take your mind back to your previous use of the function type().

    # Import pickle package
    import pickle

    # Open pickle file and load data: d
    with open('data.pkl', "rb") as file:
        d = pickle.load(file)

    # Print d
    print(d)

    # Print datatype of d
    print(type(d))

    -------------------------------------------------------------------------------------------
    Listing sheets in Excel files
    Whether you like it or not, any working data scientist will need to deal with Excel spreadsheets at some point in time. You won't always want to do so in Excel, however!

    Here, you'll learn how to use pandas to import Excel spreadsheets and how to list the names of the sheets in any loaded .xlsx file.

    Recall from the video that, given an Excel file imported into a variable spreadsheet, you can retrieve a list of the sheet names using the attribute spreadsheet.sheet_names.

    Specifically, you'll be loading and checking out the spreadsheet 'battledeath.xlsx', modified from the Peace Research Institute Oslo's (PRIO) dataset. This data contains age-adjusted mortality rates due to war in various countries over several years.

    Instructions

    Assign the spreadsheet filename (provided above) to the variable file.
    Pass the correct argument to pd.ExcelFile() to load the file using pandas, assigning the result to the variable xls.
    Print the sheetnames of the Excel spreadsheet by passing the necessary argument to the print() function.

    # Import pandas
    import pandas as pd

    # Assign spreadsheet filename: file
    file = 'battledeath.xlsx'

    # Load spreadsheet: xls
    xls = pd.ExcelFile(file)

    # Print sheet names
    print(xls.sheet_names)

    -----------------------------------------------------------------------------------------------
    Importing sheets from Excel files
    In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.

    In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.

    The spreadsheet 'battledeath.xlsx' is already loaded as xls.

    Instructions

    Load the sheet '2004' into the DataFrame df1 using its name as a string.
    Print the head of df1 to the shell.
    Load the sheet 2002 into the DataFrame df2 using its index (0).
    Print the head of df2 to the shell.

    # Load a sheet into a DataFrame by name: df1
    df1 = xls.parse('2004')

    # Print the head of the DataFrame df1
    print(df1.head())

    # Load a sheet into a DataFrame by index: df2
    df2 = xls.parse(0)

    # Print the head of the DataFrame df2
    print(df2.head())

    -----------------------------------------------------------------------------------------------------
    Customizing your spreadsheet import
    Here, you'll parse your spreadsheets and use additional arguments to skip rows, rename columns and select only particular columns.

    The spreadsheet 'battledeath.xlsx' is already loaded as xls.

    As before, you'll use the method parse(). This time, however, you'll add the additional arguments skiprows, names and usecols. These 
    skip rows, name the columns and designate which columns to parse, respectively. All these arguments can be assigned to lists 
    containing the specific row numbers, strings and column numbers, as appropriate.

    Instructions

    Parse the first sheet by index. In doing so, skip the first row of data and name the columns 'Country' and 'AAM due to War (2002)' 
    using the argument names. The values passed to skiprows and names all need to be of type list.
    Parse the second sheet by index. In doing so, parse only the first column with the usecols parameter, skip the first row and rename 
    the column 'Country'. The argument passed to usecols also needs to be of type list.

    # Parse the first sheet and rename the columns: df1
    df1 = xls.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])

    # Print the head of the DataFrame df1
    print(df1.head())

    # Parse the first column of the second sheet and rename the column: df2
    df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])

    # Print the head of the DataFrame df2
    print(df2.head())

########################################################################################################################################
Importing SAS/Sata Files using pandas:

    Example
    Importing SAS files
    In this exercise, you'll figure out how to import a SAS file as a DataFrame using SAS7BDAT and pandas. The file 'sales.sas7bdat' is 
    already in your working directory and both pandas and matplotlib.pyplot have already been imported as follows:

    import pandas as pd
    import matplotlib.pyplot as plt
    The data are adapted from the website of the undergraduate text book Principles of Econometrics by Hill, Griffiths and Lim.

    Instructions

    Import the module SAS7BDAT from the library sas7bdat.
    In the context of the file 'sales.sas7bdat', load its contents to a DataFrame df_sas, using the method to_data_frame() on the object 
    file.
    Print the head of the DataFrame df_sas.
    Execute your entire script to produce a histogram plot!

    # Import sas7bdat package
    from sas7bdat import SAS7BDAT

    # Save file to a DataFrame: df_sas
    with SAS7BDAT('sales.sas7bdat') as file:
        df_sas = file.to_data_frame()

    # Print head of DataFrame
    print(df_sas.head())

    # Plot histogram of DataFrame features (pandas and pyplot already imported)
    pd.DataFrame.hist(df_sas[['P']])
    plt.ylabel('count')
    plt.show()

    --------------------------------------------------------------------------------------------

    Importing Stata files
    Here, you'll gain expertise in importing Stata files as DataFrames using the pd.read_stata() function from pandas. The last 
    exercise's file, 'disarea.dta', is still in your working directory.

    Instructions

    Use pd.read_stata() to load the file 'disarea.dta' into the DataFrame df.
    Print the head of the DataFrame df.
    Visualize your results by plotting a histogram of the column disa10. We’ve already provided this code for you, so just run it!

    # Import pandas
    import pandas as pd

    # Load Stata file into a pandas DataFrame: df
    df = pd.read_stata('disarea.dta')

    # Print the head of the DataFrame df
    print(df.head())

    # Plot histogram of one column of the DataFrame
    pd.DataFrame.hist(df[['disa10']])
    plt.xlabel('Extent of disease')
    plt.ylabel('Number of countries')
    plt.show()

######################################################################################################################################
Importing HDF5 Files:

    Example:
    Using h5py to import HDF5 files
    The file 'LIGO_data.hdf5' is already in your working directory. In this exercise, you'll import it using the h5py library. You'll 
    also print out its datatype to confirm you have imported it correctly. You'll then study the structure of the file in order to see 
    precisely what HDF groups it contains.

    You can find the LIGO data plus loads of documentation and tutorials here. There is also a great tutorial on Signal Processing with 
    the data here.

    Instructions

    Import the package h5py.
    Assign the name of the file to the variable file.
    Load the file as read only into the variable data.
    Print the datatype of data.
    Print the names of the groups in the HDF5 file 'LIGO_data.hdf5'.

    # Import packages
    import numpy as np
    import h5py

    # Assign filename: file
    file = 'LIGO_data.hdf5'

    # Load file: data
    data = h5py.File(file, 'r')

    # Print the datatype of the loaded file
    print(type(data))

    # Print the keys of the file
    for key in data.keys():
        print(key)

    ----------------------------------------------------------------------------------------------
    Extracting data from your HDF5 file
    In this exercise, you'll extract some of the LIGO experiment's actual data from the HDF5 file and you'll visualize it.

    To do so, you'll need to first explore the HDF5 group 'strain'.

    Instructions

    Assign the HDF5 group data['strain'] to group.
    In the for loop, print out the keys of the HDF5 group in group.
    Assign the time series data data['strain']['Strain'] to a NumPy array called strain.
    Set num_samples equal to 10000, the number of time points we wish to sample.
    Execute the rest of the code to produce a plot of the time series data in LIGO_data.hdf5.

    # Get the HDF5 group: group
    group = data['strain']

    # Check out keys of group
    for key in group.keys():
        print(key)

    # Set variable equal to time series data: strain
    strain = np.array(data['strain']['Strain'])

    # Set number of time points to sample: num_samples
    num_samples = 10000

    # Set time vector
    time = np.arange(0, 1, 1/num_samples)

    # Plot data
    plt.plot(time, strain[:num_samples])
    plt.xlabel('GPS Time (s)')
    plt.ylabel('strain')
    plt.show()

########################################################################################################################################
Importing MATLAB Files:

    Example:
    Loading .mat files
    In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it 
    yields.

    The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at 
    UC Davis.

    Instructions

    Import the package scipy.io.
    Load the file 'albeck_gene_expression.mat' into the variable mat; do so using the function scipy.io.loadmat().
    Use the function type() to print the datatype of mat to the IPython shell.

    # Import package
    import scipy.io

    # Load MATLAB file: mat
    mat = scipy.io.loadmat('albeck_gene_expression.mat')

    # Print the datatype type of mat
    print(type(mat))

    ---------------------------------------------------------------------------------------------
    The structure of .mat in Python
    Here, you'll discover what is in the MATLAB dictionary that you loaded in the previous exercise.

    The file 'albeck_gene_expression.mat' is already loaded into the variable mat. The following libraries have already been imported as follows:

    import scipy.io
    import matplotlib.pyplot as plt
    import numpy as np
    Once again, this file contains gene expression data from the Albeck Lab at UCDavis.

    Instructions

    Use the method .keys() on the dictionary mat to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.
    Print the type of the value corresponding to the key 'CYratioCyt' in mat. Recall that mat['CYratioCyt'] accesses the value.
    Print the shape of the value corresponding to the key 'CYratioCyt' using the numpy function shape().
    Execute the entire script to see some oscillatory gene expression data!

    # Print the keys of the MATLAB dictionary
    print(mat.keys())

    # Print the type of the value corresponding to the key 'CYratioCyt'
    print(type(mat['CYratioCyt']))

    # Print the shape of the value corresponding to the key 'CYratioCyt'
    print(np.shape(mat['CYratioCyt']))

    # Subset the array and plot it
    data = mat['CYratioCyt'][25, 5:]
    fig = plt.figure()
    plt.plot(data)
    plt.xlabel('time (min.)')
    plt.ylabel('normalized fluorescence (measure of expression)')
    plt.show()

##################################################################################################################################
Creating a Database engin in Python:

    Example:
    Creating a database engine
    Here, you're going to fire up your very first SQL engine. You'll create an engine to connect to the SQLite database 'Chinook.sqlite',
     which is in your working directory. Remember that to create an engine to connect to 'Northwind.sqlite', Hugo executed the command

    engine = create_engine('sqlite:///Northwind.sqlite')
    Here, 'sqlite:///Northwind.sqlite' is called the connection string to the SQLite database Northwind.sqlite. A little bit of 
    background on the Chinook database: the Chinook database contains information about a semi-fictional digital media store in which 
    media data is real and customer, employee and sales data has been manually created.

    Why the name Chinook, you ask? According to their website,

    The name of this sample database was based on the Northwind database. Chinooks are winds in the interior West of North America, 
    where the Canadian Prairies and Great Plains meet various mountain ranges. Chinooks are most prevalent over southern Alberta in 
    Canada. Chinook is a good name choice for a database that intends to be an alternative to Northwind.

    Instructions

    Import the function create_engine from the module sqlalchemy.
    Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.

    # Import necessary module
    from sqlalchemy import create_engine

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    --------------------------------------------------------------------------------------------
    What are the tables in the database?
    In this exercise, you'll once again create an engine to connect to 'Chinook.sqlite'. Before you can get any data out of the database,
     however, you'll need to know what tables it contains!

    To this end, you'll save the table names to a list using the method table_names() on the engine and then you will print the list.

    Instructions

    Import the function create_engine from the module sqlalchemy.
    Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.
    Using the method table_names() on the engine engine, assign the table names of 'Chinook.sqlite' to the variable table_names.
    Print the object table_names to the shell.

    # Import necessary module
    from sqlalchemy import create_engine

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    # Save the table names to a list: table_names
    table_names = engine.table_names()

    # Print the table names to the shell
    print(table_names)

########################################################################################################################################
Querying Relational Databases in Python:

    Example:
    The Hello World of SQL Queries!
    Now, it's time for liftoff! In this exercise, you'll perform the Hello World of SQL queries, SELECT, in order to retrieve all 
    columns of the table Album in the Chinook database. Recall that the query SELECT * selects all columns.

    Instructions

    Open the engine connection as con using the method connect() on the engine.
    Execute the query that selects ALL columns from the Album table. Store the results in rs.
    Store all of your query results in the DataFrame df by applying the fetchall() method to the results rs.
    Close the connection!

    # Import packages
    from sqlalchemy import create_engine
    import pandas as pd

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    # Open engine connection: con
    con = engine.connect()

    # Perform query: rs
    rs = con.execute('SELECT * FROM Album')

    # Save results of the query to DataFrame: df
    df = pd.DataFrame(rs.fetchall())

    # Close connection
    con.close()

    # Print head of DataFrame df
    print(df.head())

    --------------------------------------------------------------------------------------------
    Customizing the Hello World of SQL Queries
    Congratulations on executing your first SQL query! Now you're going to figure out how to customize your query in order to:

    Select specified columns from a table;
    Select a specified number of rows;
    Import column names from the database table.
    Recall that Hugo performed a very similar query customization in the video:

    engine = create_engine('sqlite:///Northwind.sqlite')

    with engine.connect() as con:
        rs = con.execute("SELECT OrderID, OrderDate, ShipName FROM Orders")
        df = pd.DataFrame(rs.fetchmany(size=5))
        df.columns = rs.keys()
    Packages have already been imported as follows:

    from sqlalchemy import create_engine
    import pandas as pd
    The engine has also already been created:

    engine = create_engine('sqlite:///Chinook.sqlite')
    The engine connection is already open with the statement

    with engine.connect() as con:
    All the code you need to complete is within this context.

    Instructions

    Execute the SQL query that selects the columns LastName and Title from the Employee table. Store the results in the variable rs.
    Apply the method fetchmany() to rs in order to retrieve 3 of the records. Store them in the DataFrame df.
    Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.

    # Open engine in context manager
    # Perform query and save results to DataFrame: df
    with engine.connect() as con:
        rs = con.execute("SELECT LastName, Title FROM Employee")
        df = pd.DataFrame(rs.fetchmany(size=3))
        df.columns = rs.keys()

    # Print the length of the DataFrame df
    print(len(df))

    # Print the head of the DataFrame df
    print(df.head())

    ---------------------------------------------------------------------------------------------
    Filtering your database records using SQL's WHERE
    You can now execute a basic SQL query to select records from any table in your database and you can also perform simple query 
    customizations to select particular columns and numbers of rows.

    There are a couple more standard SQL query chops that will aid you in your journey to becoming an SQL ninja.

    Let's say, for example that you wanted to get all records from the Customer table of the Chinook database for which the Country is 
    'Canada'. You can do this very easily in SQL using a SELECT statement followed by a WHERE clause as follows:

    SELECT * FROM Customer WHERE Country = 'Canada'
    In fact, you can filter any SELECT statement by any condition using a WHERE clause. This is called filtering your records.

    In this interactive exercise, you'll select all records of the Employee table for which 'EmployeeId' is greater than or equal to 6.

    Packages are already imported as follows:

    import pandas as pd
    from sqlalchemy import create_engine
    Query away!

    Instructions

    Complete the argument of create_engine() so that the engine for the SQLite database 'Chinook.sqlite' is created.
    Execute the query that selects all records from the Employee table where 'EmployeeId' is greater than or equal to 6. Use the >= 
    operator and assign the results to rs.
    Apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.
    Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    # Open engine in context manager
    # Perform query and save results to DataFrame: df
    with engine.connect() as con:
        rs = con.execute("SELECT * FROM Employee WHERE EmployeeID >= 6")
        df = pd.DataFrame(rs.fetchall())
        df.columns = rs.keys()

    # Print the head of the DataFrame df
    print(df.head())

    -----------------------------------------------------------------------------------
    Ordering your SQL records with ORDER BY
    You can also order your SQL query results. For example, if you wanted to get all records from the Customer table of the Chinook 
    database and order them in increasing order by the column SupportRepId, you could do so with the following query:

    "SELECT * FROM Customer ORDER BY SupportRepId"
    In fact, you can order any SELECT statement by any column.

    In this interactive exercise, you'll select all records of the Employee table and order them in increasing order by the column 
    BirthDate.

    Packages are already imported as follows:

    import pandas as pd
    from sqlalchemy import create_engine
    Get querying!

    Instructions

    Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.
    In the context manager, execute the query that selects all records from the Employee table and orders them in increasing order by 
    the column BirthDate. Assign the result to rs.
    In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.
    Set the DataFrame's column names to the corresponding names of the table columns.

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    # Open engine in context manager
    with engine.connect() as con:
        rs = con.execute("SELECT * FROM Employee ORDER BY BirthDate")
        df = pd.DataFrame(rs.fetchall())

        # Set the DataFrame's column names
        df.columns = rs.keys()

    # Print head of DataFrame
    print(df.head())

#######################################################################################################################################
Querying relational databases directly with pandas:

    Examples: Pandas and The Hello World of SQL Queries!
    Here, you'll take advantage of the power of pandas to write the results of your SQL query to a DataFrame in one swift line of Python 
    code!

    You'll first import pandas and create the SQLite 'Chinook.sqlite' engine. Then you'll query the database to select all records from 
    the Album table.

    Recall that to select all records from the Orders table in the Northwind database, Hugo executed the following command:

    df = pd.read_sql_query("SELECT * FROM Orders", engine)
    Instructions

    Import the pandas package using the alias pd.
    Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.
    Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all 
    records from the table Album.
    The remainder of the code is included to confirm that the DataFrame created by this method is equal to that created by the previous 
    method that you learned.

    # Import packages
    from sqlalchemy import create_engine
    import pandas as pd

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    # Execute query and store records in DataFrame: df
    df = pd.read_sql_query("SELECT * FROM Album", engine)

    # Print head of DataFrame
    print(df.head())

    # Open engine in context manager and store query result in df1
    with engine.connect() as con:
        rs = con.execute("SELECT * FROM Album")
        df1 = pd.DataFrame(rs.fetchall())
        df1.columns = rs.keys()

    # Confirm that both methods yield the same result
    print(df.equals(df1))

    -------------------------------------------------------------------------------------------
    Pandas for more complex querying
    Here, you'll become more familiar with the pandas function read_sql_query() by using it to execute a more complex query: a SELECT 
    statement followed by both a WHERE clause AND an ORDER BY clause.

    You'll build a DataFrame that contains the rows of the Employee table for which the EmployeeId is greater than or equal to 6 and 
    you'll order these entries by BirthDate.

    Instructions

    Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.
    Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all 
    records from the Employee table where the EmployeeId is greater than or equal to 6 and ordered by BirthDate (make sure to use WHERE 
    and ORDER BY in this precise order).

    # Import packages
    from sqlalchemy import create_engine
    import pandas as pd

    # Create engine: engine
    engine = create_engine('sqlite:///Chinook.sqlite')

    # Execute query and store records in DataFrame: df
    df=pd.read_sql_query("SELECT * FROM Employee Where EmployeeId >= 6 ORDER BY BirthDate", engine)

    # Print head of DataFrame
    print(df.head())

#########################################################################################################################################
Advanced Querying: Exploiting table relationships

    Example:
    The power of SQL lies in relationships between tables: INNER JOIN
    Here, you'll perform your first INNER JOIN! You'll be working with your favourite SQLite database, Chinook.sqlite. For each record 
    in the Album table, you'll extract the Title along with the Name of the Artist. The latter will come from the Artist table and so 
    you will need to INNER JOIN these two tables on the ArtistID column of both.

    Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:

    "SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID"
    The following code has already been executed to import the necessary packages and to create the engine:

    import pandas as pd
    from sqlalchemy import create_engine
    engine = create_engine('sqlite:///Chinook.sqlite')
    Instructions

    Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist 
    of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column 
    of both.
    In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.
    Set the DataFrame's column names to the corresponding names of the table columns.

    # Open engine in context manager
    # Perform query and save results to DataFrame: df
    with engine.connect() as con:
        rs = con.execute("SELECT Title, Name FROM Album INNER JOIN Artist ON Album.ArtistID = Artist.ArtistID")
        df = pd.DataFrame(rs.fetchall())
        df.columns = rs.keys()

    # Print head of DataFrame df
    print(df.head())

    ---------------------------------------------------------------------------------------
    Congrats on performing your first INNER JOIN! You're now going to finish this chapter with one final exercise in which you perform an INNER JOIN and filter the result using a WHERE clause.

    Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:

    "SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID"
    The following code has already been executed to import the necessary packages and to create the engine:

    import pandas as pd
    from sqlalchemy import create_engine
    engine = create_engine('sqlite:///Chinook.sqlite')
    Instructions

    Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all 
    records from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId that satisfy the condition 
    Milliseconds < 250000.

    # Execute query and store records in DataFrame: df
    df = pd.read_sql_query("SELECT * FROM PlaylistTrack INNER JOIN Track ON PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000", engine)

    # Print head of DataFrame
    print(df.head())

########################################################################################################################################
Intermediate importing data in python:
IMporting flat files from the Web:

    Example:
    Importing flat files from the web: your turn!
    You are about to import your first file from the web! The flat file you will import will be 'winequality-red.csv' from the 
    University of California, Irvine's Machine Learning repository. The flat file contains tabular data of physiochemical properties 
    of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.

    The URL of the file is

    'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'
    After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a pandas 
    DataFrame.

    Instructions

    Import the function urlretrieve from the subpackage urllib.request.
    Assign the URL of the file to the variable url.
    Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.
    Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell.

    # Import package
    from urllib.request import urlretrieve

    # Import pandas
    import pandas as pd

    # Assign url of file: url
    url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

    # Save file locally
    urlretrieve(url, 'winequality-red.csv')

    # Read file into a DataFrame and print its head
    df = pd.read_csv('winequality-red.csv', sep=';')
    print(df.head())

    ----------------------------------------------------------------------------------
    Opening and reading flat files from the web
    You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from 
    the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, you can use the 
    function pd.read_csv() with the URL as the first argument and the separator sep as the second argument.

    The URL of the file, once again, is

    'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'
    Instructions

    Assign the URL of the file to the variable url.
    Read file into a DataFrame df using pd.read_csv(), recalling that the separator in the file is ';'.
    Print the head of the DataFrame df.
    Execute the rest of the code to plot histogram of the first feature in the DataFrame df.

    # Import packages
    import matplotlib.pyplot as plt
    import pandas as pd

    # Assign url of file: url
    url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

    # Read file into a DataFrame: df
    df = pd.read_csv(url, sep=';')

    # Print the head of the DataFrame
    print(df.head())

    # Plot first column of df
    df.iloc[:, 0].hist()
    plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
    plt.ylabel('count')
    plt.show()

    ------------------------------------------------------------------------------------------------
    Importing non-flat files from the web
    Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the pandas function 
    pd.read_csv(). This function is super cool because it has close relatives that allow you to load all types of files, not only flat 
    ones. In this interactive exercise, you'll use pd.read_excel() to import an Excel spreadsheet.

    The URL of the spreadsheet is

    'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'
    Your job is to use pd.read_excel() to read in all of its sheets, print the sheet names and then print the head of the first sheet 
    sing its name, not its index.

    Note that the output of pd.read_excel() is a Python dictionary with sheet names as keys and corresponding DataFrames as 
    corresponding values.

    Instructions

    Assign the URL of the file to the variable url.
    Read the file in url into a dictionary xls using pd.read_excel() recalling that, in order to import all sheets you need to pass 
    None to the argument sheet_name.
    Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls.
    Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is '1700'

    # Import package
    import pandas as pd

    # Assign url of file: url
    url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'

    # Read in all sheets of Excel file: xls
    xls = pd.read_excel(url, sheet_name=None)

    # Print the sheetnames to the shell
    print(xls.keys())

    # Print the head of the first sheet (using its name, NOT its index)
    print(xls['1700'].head())

########################################################################################################################################
HTTP Requests to import files from the web:

    Example:
    Performing HTTP requests in Python using urllib
    Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will 
    ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, 
    "https://campus.datacamp.com/courses/1606/4135?ex=2".

    In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then 
    catch the response.

    Instructions

    Import the functions urlopen and Request from the subpackage urllib.request.
    Package the request to the url "https://campus.datacamp.com/courses/1606/4135?ex=2" using the function Request() and assign it to 
    request.
    Send the request and catch the response in the variable response with the function urlopen().
    Run the rest of the code to see the datatype of response and to close the connection!

    # Import packages
    from urllib.request import urlopen, Request

    # Specify the url
    url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

    # This packages the request: request
    request = Request(url)

    # Sends the request and catches the response: response
    response = urlopen(request)

    # Print the datatype of response
    print(type(response))

    # Be polite and close the response!
    response.close()

    --------------------------------------------------------------------------------------
    Printing HTTP request results in Python using urllib
    You have just packaged and sent a GET request to "https://campus.datacamp.com/courses/1606/4135?ex=2" and then caught the response. 
    You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?

    Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has 
    an associated read() method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.

    Instructions

    Send the request and catch the response in the variable response with the function urlopen(), as in the previous exercise.
    Extract the response using the read() method and store the result in the variable html.
    Print the string html.
    Hit submit to perform all of the above and to close the response: be tidy!

    # Import packages
    from urllib.request import urlopen, Request

    # Specify the url
    url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

    # This packages the request
    request = Request(url)

    # Sends the request and catches the response: response
    response = urlopen(request)

    # Extract the response: html
    html = response.read()

    # Print the html
    print(html)

    # Be polite and close the response!
    response.close()

    -----------------------------------------------------------------------------------------
    Performing HTTP requests in Python using requests
    Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their "http://www.datacamp.com/teach/documentation" page.

    Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests!

    Instructions

    Import the package requests.
    Assign the URL of interest to the variable url.
    Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.
    Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable text.
    Hit submit to print the HTML of the webpage.

    # Import package
    import requests

    # Specify the url: url
    url = "http://www.datacamp.com/teach/documentation"

    # Packages the request, send the request and catch the response: r
    r = requests.get(url)

    # Extract the response: text
    text = r.text

    # Print the html
    print(text)

#########################################################################################################################################
Scraping the web in Python:

    Example:
    Parsing HTML with BeautifulSoup
    In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.

    The URL of interest is url = 'https://www.python.org/~guido/'.

    Instructions

    Import the function BeautifulSoup from the package bs4.
    Assign the URL of interest to the variable url.
    Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.
    Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable html_doc.
    Create a BeautifulSoup object soup from the resulting HTML using the function BeautifulSoup().
    Use the method prettify() on soup and assign the result to pretty_soup.
    Hit submit to print to prettified HTML to your shell!

    # Import packages
    import requests
    from bs4 import BeautifulSoup

    # Specify url: url
    url = 'https://www.python.org/~guido/'

    # Package the request, send the request and catch the response: r
    r= requests.get(url)

    # Extracts the response as html: html_doc
    html_doc = r.text

    # Create a BeautifulSoup object from the HTML: soup
    soup = BeautifulSoup(html_doc, 'html.parser')

    # Prettify the BeautifulSoup object: pretty_soup
    pretty_soup = soup.prettify()

    # Print the response
    print(pretty_soup)

    ------------------------------------------------------------------------------------------------
    Turning a webpage into data using BeautifulSoup: getting the text
    As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll 
    figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.

    Instructions

    In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function 
    BeautifulSoup() and to assign the resulting soup to the variable soup.
    Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.
    Print the title of Guido's webpage to the shell using the print() function.
    Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.
    Hit submit to print the text from Guido's webpage to the shell.

    # Import packages
    import requests
    from bs4 import BeautifulSoup

    # Specify url: url
    url = 'https://www.python.org/~guido/'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Extract the response as html: html_doc
    html_doc = r.text

    # Create a BeautifulSoup object from the HTML: soup
    soup = BeautifulSoup(html_doc, 'html.parser')

    # Get the title of Guido's webpage: guido_title
    guido_title = soup.title

    # Print the title of Guido's webpage to the shell
    print(guido_title)

    # Get Guido's text: guido_text
    guido_text = soup.get_text()

    # Print Guido's text to the shell
    print(guido_text)

    -------------------------------------------------------------------------------------------
    Turning a webpage into data using BeautifulSoup: getting the hyperlinks
    In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll 
    become close friends with the soup method find_all().

    Instructions

    Use the method find_all() to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag <a> but passed to 
    find_all() without angle brackets; store the result in the variable a_tags.
    The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the 
    hyperlinks; to do this, for every element link in a_tags, you want to print() link.get('href').

    # Import packages
    import requests
    from bs4 import BeautifulSoup

    # Specify url
    url = 'https://www.python.org/~guido/'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Extracts the response as html: html_doc
    html_doc = r.text

    # create a BeautifulSoup object from the HTML: soup
    soup = BeautifulSoup(html_doc)

    # Print the title of Guido's webpage
    print(soup.title)

    # Find all 'a' tags (which define hyperlinks): a_tags
    a_tags = soup.find_all('a')

    # Print the URLs to the shell
    for link in a_tags:
        print(link.get('href'))

#######################################################################################################################################
Introduction to APIs and JSONs:

    Example:
    Loading and exploring a JSON
    Now that you know what a JSON is, you'll load one into your Python environment and explore it yourself. Here, you'll load the 
    JSON 'a_movie.json' into the variable json_data, which will be a dictionary. You'll then explore the JSON contents by printing the 
    key-value pairs of json_data to the shell.

    Instructions

    Load the JSON 'a_movie.json' into the variable json_data within the context provided by the with statement. To do so, use the 
    function json.load() within the context manager.
    Use a for loop to print all key-value pairs in the dictionary json_data. Recall that you can access a value in a dictionary using 
    the syntax: dictionary[key].

    # Load JSON: json_data
    with open("a_movie.json") as json_file:
        json_data = json.load(json_file)

    # Print each key-value pair in json_data
    for k in json_data.keys():
        print(k + ': ', json_data[k])

########################################################################################################################################
APIs and interacting with the world wide web:

    Example: 
    API requests
    Now it's your turn to pull some movie data down from the Open Movie Database (OMDB) using their API. The movie you'll query the API about is The Social Network. Recall that, in the video, to query the API about the movie Hackers, Hugo's query string was 'http://www.omdbapi.com/?t=hackers' and had a single argument t=hackers.

    Note: recently, OMDB has changed their API: you now also have to specify an API key. This means you'll have to add another argument to the URL: apikey=72bc447a.

    Instructions

    Import the requests package.
    Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network.
    Print the text of the response object r by using its text attribute and passing the result to the print() function.

    # Import requests package
    import requests

    # Assign URL to variable: url
    url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'


    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Print the text of the response
    print(r.text)

    ---------------------------------------------------------------------------------------------
    JSON–from the web to Python
    Wow, congrats! You've just queried your first API programmatically in Python and printed the text of the response to the shell. 
    However, as you know, your response is actually a JSON, so you can do one step better and decode the JSON. You can then print the 
    key-value pairs of the resulting dictionary. That's what you're going to do now!

    Instructions

    Pass the variable url to the requests.get() function in order to send the relevant request and catch the response, assigning the 
    resultant response message to the variable r.
    Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.
    Hit submit to print the key-value pairs of the dictionary json_data to the shell.

    # Import package
    import requests

    # Assign URL to variable: url
    url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Decode the JSON data into a dictionary: json_data
    json_data = r.json()

    # Print each key-value pair in json_data
    for k in json_data.keys():
        print(k + ': ', json_data[k])

    -------------------------------------------------------------------------------------------------
    Checking out the Wikipedia API
    You're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented here). 
    You'll figure out how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your 
    query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into 
    dictionaries within dictionaries.

    The URL that requests the relevant query from the Wikipedia API is

    https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza
    Instructions

    Assign the relevant URL to the variable url.
    Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.
    The variable pizza_extract holds the HTML of an extract from Wikipedia's Pizza page as a string; use the function print() to print 
    this string to the shell.

    # Import package
    import requests

    # Assign URL to variable: url
    url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Decode the JSON data into a dictionary: json_data
    json_data = r.json()

    # Print the Wikipedia page extract
    pizza_extract = json_data['query']['pages']['24768']['extract']
    print(pizza_extract)

########################################################################################################################################
The Twitter API and Authentication:

    Example:
    Streaming tweets
    It's time to stream some tweets! Your task is to create the Streamobject and to filter tweets according to particular keywords. 
    tweepy has been imported for you.

    Instructions

    Create your Stream object with the credentials given.
    Filter your Stream variable for the keywords "clinton", "trump", "sanders", and "cruz".

    # Store credentials in relevant variables
    consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
    consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"
    access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
    access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"

    # Create your Stream object with credentials
    stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)

    # Filter your Stream variable
    stream.filter(track=["clinton", "trump", "sanders", "cruz"])

    ---------------------------------------------------------------------------------------
    Load and explore your Twitter data
    Now that you've got your Twitter data sitting locally in a text file, it's time to explore it! This is what you'll do in the next 
    few interactive exercises. In this exercise, you'll read the Twitter data into a list: tweets_data.

    Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive 
    content (in this exercise, and any following exercises that also use real Twitter data).

    Instructions

    Assign the filename 'tweets.txt' to the variable tweets_data_path.
    Initialize tweets_data as an empty list to store the tweets in.
    Within the for loop initiated by for line in tweets_file:, load each tweet into a variable, tweet, using json.loads(), then append 
    tweet to tweets_data using the append() method.
    Hit submit and check out the keys of the first tweet dictionary printed to the shell.

    # Import package
    import json

    # String of path to file: tweets_data_path
    tweets_data_path = 'tweets.txt'

    # Initialize empty list to store tweets: tweets_data
    tweets_data = []

    # Open connection to file
    tweets_file = open(tweets_data_path, "r")

    # Read in tweets and store in list: tweets_data
    for line in tweets_file:
        tweet = json.loads(line)
        tweets_data.append(tweet)

    # Close connection to file
    tweets_file.close()

    # Print the keys of the first tweet dict
    print(tweets_data[0].keys())

    --------------------------------------------------------------------------------
    Twitter data to DataFrame
    Now you have the Twitter data in a list of dictionaries, tweets_data, where each dictionary corresponds to a single tweet. Next, 
    you're going to extract the text and language of each tweet. The text in a tweet, t1, is stored as the value t1['text']; similarly, 
    the language is stored in t1['lang']. Your task is to build a DataFrame in which each row is a tweet and the columns are 'text' 
    and 'lang'.

    Instructions

    Use pd.DataFrame() to construct a DataFrame of tweet texts and languages; to do so, the first argument should be tweets_data, a list 
    of dictionaries. The second argument to pd.DataFrame() is a list of the keys you wish to have as columns. Assign the result of the 
    pd.DataFrame() call to df.
    Print the head of the DataFrame.

    # Import package
    import pandas as pd

    # Build DataFrame of tweet texts and languages
    df = pd.DataFrame(tweets_data, columns=['text', 'lang'])

    # Print head of DataFrame
    print(df.head())

    ----------------------------------------------------------------------------------------
    A little bit of Twitter text analysis
    Now that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the 
    words 'clinton', 'trump', 'sanders' and 'cruz'. In the pre-exercise code, we have defined the following function word_in_text(), 
    which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).

    import re

    def word_in_text(word, text):
        word = word.lower()
        text = text.lower()
        match = re.search(word, text)

        if match:
            return True
        return False
    You're going to iterate over the rows of the DataFrame and calculate how many tweets contain each of our keywords! The list of 
    objects for each candidate has been initialized to 0.

    Instructions

    Within the for loop for index, row in df.iterrows():, the code currently increases the value of clinton by 1 each time a tweet 
    (text row) mentioning 'Clinton' is encountered; complete the code so that the same happens for trump, sanders and cruz.

    # Initialize list to store tweet counts
    [clinton, trump, sanders, cruz] = [0, 0, 0, 0]

    # Iterate through df, counting the number of tweets in which
    # each candidate is mentioned
    for index, row in df.iterrows():
        clinton += word_in_text('clinton', row['text'])
        trump += word_in_text('trump', row['text'])
        sanders += word_in_text('sanders', row['text'])
        cruz += word_in_text('cruz', row['text'])

    ------------------------------------------------------------------------------------------
    Plotting your Twitter data
    Now that you have the number of tweets that each candidate was mentioned in, you can plot a bar chart of this data. You'll use the 
    statistical data visualization library seaborn, which you may not have seen before, but we'll guide you through. You'll first import 
    seaborn as sns. You'll then construct a barplot of the data using sns.barplot, passing it two arguments:

    a list of labels and
    a list containing the variables you wish to plot (clinton, trump and so on.)
    Hopefully, you'll see that Trump was unreasonably represented! We have already run the previous exercise solutions in your 
    environment.

    Instructions

    Import both matplotlib.pyplot and seaborn using the aliases plt and sns, respectively.
    Complete the arguments of sns.barplot:
    The first argument should be the list of labels to appear on the x-axis (created in the previous step).
    The second argument should be a list of the variables you wish to plot, as produced in the previous exercise (i.e. a list containing 
    clinton, trump, etc).

    # Import packages
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Set seaborn style
    sns.set(color_codes=True)

    # Create a list of labels:cd
    cd = ['clinton', 'trump', 'sanders', 'cruz']

    # Plot the bar chart
    ax = sns.barplot(cd, [clinton, trump, sanders, cruz])
    ax.set(ylabel="count")
    plt.show()

########################################################################################################################################
Cleaning Data in Python:
Data Type Constraints: 

    Example:
    Numeric data or ... ?
    In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called ride_sharing. It contains information on the start and end stations, 
    the trip duration, and some user information for a bike sharing service.

    The user_type column contains information on whether a user is taking a free ride and takes on the following values:

    1 for free riders.
    2 for pay per ride.
    3 for monthly subscribers.
    In this instance, you will print the information of ride_sharing using .info() and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The pandas 
    package is imported as pd.

    Instructions 1/3

    1
    2
    3
    Print the information of ride_sharing.
    Use .describe() to print the summary statistics of the user_type column from ride_sharing.

    -------------------------------------------------------------------------------------
    # Print the information of ride_sharing
    print(ride_sharing.info())

    # Print summary statistics of user_type column
    print(ride_sharing['user_type'].describe())

    --------------------------------------------------------------------------------------
    # Print the information of ride_sharing
    print(ride_sharing.info())

    # Print summary statistics of user_type column
    print(ride_sharing['user_type'].describe())

    # Convert user_type from integer to category
    ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')

    # Write an assert statement confirming the change
    assert ride_sharing['user_type_cat'].dtype == 'category'

    # Print new summary statistics 
    print(ride_sharing['user_type_cat'].describe())

    ------------------------------------------------------------------------------------------
    Example:
    Summing strings and concatenating numbers
    In the previous exercise, you were able to identify that category is the correct data type for user_type and convert it in order to extract relevant statistical summaries that shed light 
    on the distribution of user_type.

    Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not 
    numerical outputs.

    In this exercise, you'll be converting the string column duration to the type int. Before that however, you will need to make sure to strip "minutes" from the column in order to make sure 
    pandas reads it as numerical. The pandas package has been imported as pd.

    Instructions

    Use the .strip() method to strip duration of "minutes" and store it in the duration_trim column.
    Convert duration_trim to int and store it in the duration_time column.
    Write an assert statement that checks if duration_time's data type is now an int.
    Print the average ride duration.

    # Strip duration of minutes
    ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')

    # Convert duration to integer
    ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(int)

    # Write an assert statement making sure of conversion
    assert ride_sharing['duration_time'].dtype == 'int'

    # Print formed columns and calculate average ride duration 
    print(ride_sharing[['duration','duration_trim','duration_time']])
    print(ride_sharing['duration_time'].mean())

##########################################################################################################################################
Data Range Constraints:

    Example:
    Tire size constraints
    In this lesson, you're going to build on top of the work you've been doing with the ride_sharing DataFrame. You'll be working with the tire_sizes column which contains data on each bike's 
    tire size.

    Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the 
    maximum tire size to be 27″.

    In this exercise, you will make sure the tire_sizes column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes.

    Instructions

    Convert the tire_sizes column from category to 'int'.
    Use .loc[] to set all values of tire_sizes above 27 to 27.
    Reconvert back tire_sizes to 'category' from int.
    Print the description of the tire_sizes.

    # Convert tire_sizes to integer
    ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')

    # Set all values above 27 to 27
    ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27

    # Reconvert tire_sizes back to categorical
    ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')

    # Print tire size description
    print(ride_sharing['tire_sizes'].describe())

    ---------------------------------------------------------------------------------------------
    Back to the future
    A new update to the data pipeline feeding into the ride_sharing DataFrame has been updated to register each ride's date. This information is stored in the ride_date column of the type 
    object, which represents strings in pandas.

    A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the ride_date column that occur anytime in the future, and set 
    the maximum possible value of this column to today's date. Before doing so, you would need to convert ride_date to a datetime object.

    The datetime package has been imported as dt, alongside all the packages you've been using till now.

    Instructions

    Convert ride_date to a datetime object using to_datetime(), then convert the datetime object into a date and store it in ride_dt column.
    Create the variable today, which stores today's date by using the dt.date.today() function.
    For all instances of ride_dt in the future, set them to today's date.
    Print the maximum date in the ride_dt column.

    # Convert ride_date to date
    ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date

    # Save today's date
    today = dt.date.today()

    # Set all in the future to today's date
    ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today

    # Print maximum of ride_dt column
    print(ride_sharing['ride_dt'].max())

###########################################################################################################################################
Uniqueness Constraints:

    Example:
    Finding duplicates
    A new update to the data pipeline feeding into ride_sharing has added the ride_id column, which represents a unique identifier for each ride.

    The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, 
    the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the ride_sharing 
    DataFrame.

    In this exercise, you will confirm this suspicion by finding those duplicates. A sample of ride_sharing is in your environment, as well as all the 
    packages you've been working with thus far.

    Instructions

    Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.
    Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.
    Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order.

    # Find duplicates
    duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)

    # Sort your duplicated rides
    duplicated_rides = ride_sharing[duplicates].sort_values(by='ride_id')

    # Print relevant columns of duplicated_rides
    print(duplicated_rides[['ride_id', 'duration', 'user_birth_year']])

    ---------------------------------------------------------------------------------------------
    Treating duplicates
    In the last exercise, you were able to verify that the new update feeding into ride_sharing contains a bug generating both complete and incomplete duplicated 
    rows for some values of the ride_id column, with occasional discrepant values for the user_birth_year and duration columns.

    In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one 
    while keeping the average duration, and the minimum user_birth_year for each set of incomplete duplicate rows.

    Instructions

    Drop complete duplicates in ride_sharing and store the results in ride_dup.
    Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.
    Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.
    Find duplicates again and run the assert statement to verify de-duplication.

    # Drop complete duplicates from ride_sharing
    ride_dup = ride_sharing.drop_duplicates()

    # Create statistics dictionary for aggregation function
    statistics = {'user_birth_year': 'min', 'duration': 'mean'}

    # Group by ride_id and compute new statistics
    ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()

    # Find duplicated values again
    duplicates = ride_unique.duplicated(subset='ride_id', keep=False)
    duplicated_rides = ride_unique[duplicates]

    # Assert duplicates are processed
    assert duplicated_rides.shape[0] == 0

############################################################################################################################################
Membership Constraints:

    Example:
    Finding consistency
    In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco 
    Airport from airline customers.

    The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, 
    safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns.

    In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer 
    and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the airlines and categories 
    DataFrames are in your environment.

    Instructions 1/4

    1
    2
    3
    4
    Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.
    Print the unique values of the survey columns in airlines using the .unique() method.

    # Print categories DataFrame
    print(categories)

    # Print unique values of survey columns in airlines
    print('Cleanliness: ', airlines['cleanliness'].unique(), "\n")
    print('Safety: ', airlines['safety'].unique(), "\n")
    print('Satisfaction: ', airlines['satisfaction'].unique(), "\n")

    ---------------------------------------------------------------------------------------
    # Find the cleanliness category in airlines not in categories
    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])

    # Find rows with that category
    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)

    # Print rows with inconsistent category
    print(airlines[cat_clean_rows])

    -----------------------------------------------------------------------------------------
    # Find the cleanliness category in airlines not in categories
    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])

    # Find rows with that category
    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)

    # Print rows with inconsistent category
    print(airlines[cat_clean_rows])

    # Print rows with consistent categories only
    print(airlines[~cat_clean_rows])

######################################################################################################################################################
Categorical Variables:

    Example:
    Inconsistent categories
    In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.

    As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding 
    cleanliness, safety, and satisfaction on the San Francisco Airport.

    In this exercise, you will examine two categorical columns from this DataFrame, dest_region and dest_size respectively, assess how to address them and 
    make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the airlines DataFrame is in your environment.

    Instructions 1/4

    1
    2
    3
    4
    Print the unique values in dest_region and dest_size respectively.

    # Print unique values of both columns
    print(airlines['dest_region'].unique())
    print(airlines['dest_size'].unique())

    ==========================================================================================
    # Print unique values of both columns
    print(airlines['dest_region'].unique())
    print(airlines['dest_size'].unique())

    # Lower dest_region column and then replace "eur" with "europe"
    airlines['dest_region'] = airlines['dest_region'].str.lower()
    airlines['dest_region'] = airlines['dest_region'].replace({'eur': 'europe'})

    -------------------------------------------------------------------------------------------
    # Print unique values of both columns before making changes
    print(airlines['dest_region'].unique())
    print(airlines['dest_size'].unique())

    # Lower dest_region column and then replace "eur" with "europe"
    airlines['dest_region'] = airlines['dest_region'].str.lower() 
    airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

    # Remove white spaces from `dest_size`
    airlines['dest_size'] = airlines['dest_size'].str.strip()

    # Verify changes have been effected
    print(airlines['dest_region'].unique())
    print(airlines['dest_size'].unique())

    ------------------------------------------------------------------------------------------
    Remapping categories
    To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and 
    wait time at the gate.

    The airlines DataFrame contains the day and wait_min columns, which are categorical and numerical respectively. The day column contains the exact day a flight 
    took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new 
    categorical variables:

    wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+
    day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.
    The pandas and numpy packages have been imported as pd and np. Let's create some new categorical data!

    Instructions

    Create the ranges and labels for the wait_type column mentioned in the description.
    Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.
    Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.
    Create the day_week column by using .replace().

    # Create ranges for categories
    label_ranges = [0, 60, 180, np.inf]
    label_names = ['short', 'medium', 'long']

    # Create wait_type column
    airlines['wait_type'] = pd.cut(airlines['wait_min'], bins=label_ranges, labels=label_names)

    # Create mappings and replace
    mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
                'Thursday': 'weekday', 'Friday': 'weekday', 
                'Saturday': 'weekend', 'Sunday': 'weekend'}

    airlines['day_week'] = airlines['day'].replace(mappings)

#########################################################################################################################################################################
Cleaning Text Data:

    Example:
    Removing titles and taking names
    While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the full_name column. However 
    upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as "Dr.", "Mr.", "Ms." and "Miss".

    Your ultimate objective is to create two new columns named first_name and last_name, containing the first and last names of respondents 
    respectively. Before doing so however, you need to remove honorifics.

    The airlines DataFrame is in your environment, alongside pandas as pd.

    Instructions

    Remove "Dr.", "Mr.", "Miss" and "Ms." from full_name by replacing them with an empty string "" in that order.
    Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics.

    # Replace "Dr." with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Dr.", "")

    # Replace "Mr." with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Mr.", "")

    # Replace "Miss" with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Miss", "")

    # Replace "Ms." with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Ms.", "")

    # Assert that full_name has no honorifics
    assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False

    --------------------------------------------------------------------------------------------------
    Keeping it descriptive
    To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire 
    to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common 
    patterns in what travelers are saying about the airport.

    Their response is stored in the survey_response column. Upon a closer look, you realized a few of the answers gave the shortest possible character 
    amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40 , and make sure your new 
    DataFrame contains responses with 40 characters or more using an assert statement.

    The airlines DataFrame is in your environment, and pandas is imported as pd.

    Instructions

    Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().
    Isolate the rows of airlines with resp_length higher than 40.
    Assert that the smallest survey_response length in airlines_survey is now bigger than 40.

    # Store length of each row in survey_response column
    resp_length = airlines['survey_response'].str.len()

    # Find rows in airlines where resp_length > 40
    airlines_survey = airlines[resp_length > 40]

    # Assert minimum survey_response length is > 40
    assert airlines_survey['survey_response'].str.len().min() > 40

    # Print new survey_response column
    print(airlines_survey['survey_response'])

##########################################################################################################################################################
Uniformity:

    Example:
    Uniform currencies
    In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the banking DataFrame. The dataset contains 
    data on the amount of money stored in accounts (acct_amount), their currency (acct_cur), amount invested (inv_amount), account opening date 
    (account_opened), and last transaction date (last_transaction) that were consolidated from American and European branches.

    You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis 
    accurately, you first need to unify the currency amount into dollars. The pandas package has been imported as pd, and the banking DataFrame is in 
    your environment.

    Instructions

    Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu.
    Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.
    Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'.

    # Find values of acct_cur that are equal to 'euro'
    acct_eu = banking['acct_cur'] == 'euro'

    # Convert acct_amount where it is in euro to dollars
    banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1

    # Unify acct_cur column by changing 'euro' values to 'dollar'
    banking.loc[acct_eu, 'acct_cur'] = 'dollar'

    # Assert that only dollar currency remains
    assert banking['acct_cur'].unique() == 'dollar'

    ---------------------------------------------------------------------------------------------------
    Uniform dates
    After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have 
    been investing their money given the size of their account over each year. The account_opened column represents when customers opened their accounts and 
    is a good proxy for segmenting customer activity and investment over time.

    However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting 
    this column into a datetime object, while making sure that the format is inferred and potentially incorrect formats are set to missing. The banking 
    DataFrame is in your environment and pandas was imported as pd.

    Instructions 1/4

    1
    2
    3
    4
    Print the header of account_opened from the banking DataFrame and take a look at the different results.

    # Print the header of account_opened
    print(banking['account_opened'].head())

    ------------------------------------------------------------------------------------------------------
    # Print the header of account_opened
    print(banking['account_opened'].head())

    # Convert account_opened to datetime
    banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                            # Infer datetime format
                                            infer_datetime_format = True,
                                            # Return missing value for error
                                            errors = 'coerce')

    -------------------------------------------------------------------------------------------------------
    # Print the header of account_opened
    print(banking['account_opened'].head())

    # Convert account_opened to datetime
    banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                            # Infer datetime format
                                            infer_datetime_format = True,
                                            # Return missing value for error
                                            errors = 'coerce') 

    # Get year of account opened
    banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

    # Print acct_year
    print(banking['acct_year'])

################################################################################################################################################
Cross Field Validation:

    Example:
    How's our data integrity?
    New data has been merged into the banking DataFrame that contains details on how investments in the inv_amount column are allocated across 
    four different funds A, B, C and D.

    Furthermore, the age and birthdays of customers are now stored in the age and birth_date columns respectively.

    You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is 
    correct. You will do so by cross field checking values of inv_amount and age against the amount invested in different funds and customers' 
    birthdays. Both pandas and datetime have been imported as pd and dt respectively.

    Instructions 1/2

    1
    Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.
    Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.

    # Store fund columns to sum against
    fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

    # Find rows where fund_columns row sum == inv_amount
    inv_equ = (banking[fund_columns].sum(axis=1) == banking['inv_amount'])

    # Store consistent and inconsistent data
    consistent_inv = banking[inv_equ]
    inconsistent_inv = banking[~inv_equ]

    # Store consistent and inconsistent data
    print("Number of inconsistent investments: ", inconsistent_inv.shape[0])

    ------------------------------------------------------------------------------------------------------
    Store today's date into today, and manually calculate customers' ages and store them in ages_manual.
    Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages.

    # Store today's date and find ages
    today = dt.date.today()
    ages_manual = today.year - banking['birth_date'].dt.year

    # Find rows where age column == ages_manual
    age_equ = banking['age'] == ages_manual

    # Store consistent and inconsistent data
    consistent_ages = banking[age_equ]
    inconsistent_ages = banking[~age_equ]

    # Store consistent and inconsistent data
    print("Number of inconsistent ages: ", inconsistent_ages.shape[0])

####################################################################################################################################################
Completeness:

    Example:
    Missing investors
    Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.

    You just received a new version of the banking DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing inv_amount values.

    You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The pandas, missingno and matplotlib.pyplot packages have been imported as pd, msno and plt respectively. The banking DataFrame is in your environment.

    Instructions 1/4

    1
    2
    3
    4
    Print the number of missing values by column in the banking DataFrame.
    Plot and show the missingness matrix of banking with the msno.matrix() function.

    # Print number of missing values in banking
    print(banking.isna().sum())

    # Visualize missingness matrix
    msno.matrix(banking)
    plt.show()

    --------------------------------------------------------------------------------------------------
    Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors.

    # Print number of missing values in banking
    print(banking.isna().sum())

    # Visualize missingness matrix
    msno.matrix(banking)
    plt.show()

    # Isolate missing and non-missing values of inv_amount
    missing_investors = banking[banking['inv_amount'].isnull()]
    investors = banking[~banking['inv_amount'].isnull()]

    ------------------------------------------------------------------------------------------------------
    Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted.

    # Print number of missing values in banking
    print(banking.isna().sum())

    # Visualize missingness matrix
    msno.matrix(banking)
    plt.show()

    # Isolate missing and non missing values of inv_amount
    missing_investors = banking[banking['inv_amount'].isna()]
    investors = banking[~banking['inv_amount'].isna()]

    # Sort banking by age and visualize
    banking_sorted = banking.sort_values(by='age')

    # Visualize missingness matrix of sorted DataFrame
    msno.matrix(banking_sorted)
    plt.show()

    -----------------------------------------------------------------------------------------------------------
    Follow the money
    In this exercise, you're working with another version of the banking DataFrame that contains missing values for both the cust_id column 
    and the acct_amount column.

    You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows 
    with missing cust_id don't really help you, and that on average acct_amount is usually 5 times the amount of inv_amount.

    In this exercise, you will drop rows of banking with missing cust_ids, and impute missing values of acct_amount with some domain knowledge.

    Instructions

    Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid.
    Use inv_amount to compute the estimated account amounts for banking_fullid by setting the amounts equal to inv_amount * 5, and assign the
     results to acct_imp.
    Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna().

    # Drop missing values of cust_id
    banking_fullid = banking.dropna(subset=['cust_id'])

    # Compute estimated acct_amount
    acct_imp = banking_fullid['inv_amount'] * 5

    # Impute missing acct_amount with corresponding acct_imp
    banking_imputed = banking_fullid.fillna({'acct_amount': acct_imp})

    # Print number of missing values
    print(banking_imputed.isna().sum())

###################################################################################################################################################
Comparing Strings: 

    Example:
    The cutoff point
    In this exercise, and throughout this chapter, you'll be working with the restaurants DataFrame which has data on various restaurants. Your 
    ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.

    This version of restaurants has been collected from many sources, where the cuisine_type column is riddled with typos, and should contain only 
    italian, american and asian cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to 
    use string similarity instead.

    Before doing so, you want to establish the cutoff point for the similarity score using the thefuzz's process.extract() function by finding the 
    similarity score of the most distant typo of each category.

    Instructions 1/2

    1
    2
    Import process from thefuzz.
    Store the unique cuisine_types into unique_types.
    Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all 
    possible matches.

    # Import process from thefuzz
    from thefuzz import process

    # Store the unique values of cuisine_type in unique_types
    unique_types = restaurants['cuisine_type'].unique()

    # Calculate similarity of 'asian' to all values of unique_types
    print(process.extract('asian', unique_types, limit=len(unique_types)))

    # Calculate similarity of 'american' to all values of unique_types
    print(process.extract('american', unique_types, limit=len(unique_types)))

    # Calculate similarity of 'italian' to all values of unique_types
    print(process.extract('italian', unique_types, limit=len(unique_types)))

    -------------------------------------------------------------------------------------------------
    Remapping categories II
    In the last exercise, you determined that the distance cutoff point for remapping typos of 'american', 'asian', and 'italian' cuisine types 
    stored in the cuisine_type column should be 80.

    In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using 
    fuzywuzzy.process's extract() function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a 
    string with an array of strings using process.extract(), the output is a list of tuples where each is formatted like:

    (closest match, similarity score, index of match)
    The restaurants DataFrame is in your environment, and you have access to a categories list containing the correct cuisine types ('italian', 
    'asian', and 'american').

    Instructions 1/4

    1
    2
    3
    4
    Return all of the unique values in the cuisine_type column of restaurants.

    # Inspect the unique values of the cuisine_type column
    print(restaurants['cuisine_type'].unique())

    ---------------------------------------------------------------------------------------------------
    Okay! Looks like you will need to use some string matching to correct these misspellings!

    As a first step, create a list of all possible matches, comparing 'italian' with the restaurant types listed in the cuisine_type column.

    # Create a list of matches, comparing 'italian' with the cuisine_type column
    matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))

    # Inspect the first 5 matches
    print(matches[0:5])

    ------------------------------------------------------------------------------------------------------
    Now you're getting somewhere! Now you can iterate through matches to reassign similar entries.

    Within the for loop, use an if statement to check whether the similarity score in each match is greater than or equal to 80.
    If it is, use .loc to select rows where cuisine_type in restaurants is equal to the current match (which is the first element of match), 
    and reassign them to be 'italian'.

    # Create a list of matches, comparing 'italian' with the cuisine_type column
    matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants['cuisine_type']))

    # Iterate through the list of matches to italian
    for match in matches:
        # Check whether the similarity score is greater than or equal to 80
        if match[1] >= 80:
            # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine
            restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'

    ---------------------------------------------------------------------------------------------------------
    Finally, you'll adapt your code to work with every restaurant type in categories.

    Using the variable cuisine to iterate through categories, embed your code from the previous step in an outer for loop.
    Inspect the final result. This has been done for you.

    # Iterate through categories
    for cuisine in categories:  
        # Create a list of matches, comparing cuisine with the cuisine_type column
        matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants['cuisine_type']))

        # Iterate through the list of matches
        for match in matches:
            # Check whether the similarity score is greater than or equal to 80
            if match[1] >= 80:
                # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine
                restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = cuisine
        
    # Inspect the final result
    print(restaurants['cuisine_type'].unique())

######################################################################################################################################################
Generating pairs:

    Example:
    Pairs of restaurants
    In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. You have a new 
    DataFrame named restaurants_new with new restaurants to train your model on, that's been scraped from a new data source.

    You've already cleaned the cuisine_type and city columns using the techniques learned throughout the course. However you saw duplicates 
    with typos in restaurants names that require record linkage instead of joins with restaurants.

    In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between restaurants and restaurants_new. 
    Both DataFrames, pandas and recordlinkage are in your environment.

    Instructions 1/2

    1
    2
    Instantiate an indexing object by using the Index() function from recordlinkage.
    Block your pairing on cuisine_type by using indexer's' .block() method.
    Generate pairs by indexing restaurants and restaurants_new in that order.

    # Instantiate an indexing object by using the Index() function from recordlinkage
    indexer = recordlinkage.Index()

    # Block your pairing on cuisine_type by using indexer's .block() method
    indexer.block('cuisine_type')

    # Generate pairs by indexing restaurants and restaurants_new in that order
    pairs = indexer.index(restaurants, restaurants_new)

    -----------------------------------------------------------------------------------------------------------
    Similar restaurants
    In the last exercise, you generated pairs between restaurants and restaurants_new in an effort to cleanly merge both DataFrames using record linkage.

    When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact 
    matches, string similarities, and more.

    Now that your pairs have been generated and stored in pairs, you will find exact matches in the city and cuisine_type columns between each pair, 
    and similar strings for each pair in the rest_name column. Both DataFrames, pandas and recordlinkage are in your environment.

    Instructions 1/4

    1
    2
    3
    4
    Instantiate a comparison object using the recordlinkage.Compare() function.

    # Create a comparison object
    comp_cl = recordlinkage.Compare()

    -----------------------------------------------------------------------------------------------------------------
    Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames.
    Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.

    # Create a comparison object
    comp_cl = recordlinkage.Compare()

    # Find exact matches on city, cuisine_types 
    comp_cl.exact('city', 'city', label='city')
    comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')

    # Find similar matches of rest_name
    comp_cl.string('rest_name', 'rest_name', label='name', threshold=0.8)

    --------------------------------------------------------------------------------------------------------------------
    Compute the comparison of the pairs by using the .compute() method of comp_cl.

    # Create a comparison object
    comp_cl = recordlinkage.Compare()

    # Find exact matches on city, cuisine_types
    comp_cl.exact('city', 'city', label='city')
    comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')

    # Find similar matches of rest_name
    comp_cl.string('rest_name', 'rest_name', label='name', threshold=0.8)

    # Get potential matches and print
    potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)
    print(potential_matches)

##################################################################################################################################################
Linking DataFrames:

    Example:
    Linking them together!
    In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new. You've generated the 
    different pairs of potentially matching rows, searched for exact matches between the cuisine_type and city columns, but compared for 
    similar strings in the rest_name column. You stored the DataFrame containing the scores in potential_matches.

    Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of restaurants_new that are matching 
    across the columns mentioned above from potential_matches. Then you will subset restaurants_new on these indices, then append the 
    non-duplicate values to restaurants. All DataFrames are in your environment, alongside pandas imported as pd.

    Instructions

    Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.
    Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using 
    the .get_level_values() method.
    Subset restaurants_new for rows that are not in matching_indices.
    Append non_dup to restaurants.

    # Isolate potential matches with row sum >= 3
    matches = potential_matches[potential_matches.sum(axis=1) >= 3]

    # Get values of second column index of matches
    matching_indices = matches.index.get_level_values(1)

    # Subset restaurants_new based on non-duplicate values
    non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]

    # Append non_dup to restaurants
    full_restaurants = restaurants.append(non_dup)
    print(full_restaurants)

######################################################################################################################################################

